#! /usr/bin/python3
#
# Copyright (c) 2017-20 Intel Corporation
#
# SPDX-License-Identifier: Apache-2.0
#

"""TCF's backbone test case finder and runner
------------------------------------------

This implements a high level test case finder and runner to find and
execute test cases.

The execution of each test case consists of the following phases:

 - configure
 - build
 - target acquisition
 - deploy
 - one or more evaluation sequences
   (each consisting of setup, start, evaluation per se, teardown)
 - clean [not executed by default]

the configuration, build and deployment phases happen in the local
host; the evaluation phases can happen in the local host (for static
tests) or in remote targets (for dynamic tests).

The backbone is designed so multiple drivers (test case drivers) can
be implemented that find test cases and specify to the backbone how to
build, run and test for success or failure. This is done by
subclassing :class:`tcfl.tc.tc_c` and extending or redefining
:meth:`tc_c.is_testcase`.

Testcases are defined also by subclassing :class:`tcfl.tc.tc_c` and
implementing the different methods the meta runner will call to
evaluate. Testcases can manipulate targets (if they need any) by using
the APIs defined by :class:`tcfl.tc.target_c`.

The runner will collect the list of available targets and determine
whih testcases have to be run on which targets, and then create an
instance of each testcase for those groups of targets where it has to
run. All the instances will then be run in parallel throgh a
multiprocessing pool.

To testcases report results via the report API, which are handled by
drivers defined following :class:`tcfl.tc.report_driver_c`, which can
be subclassed and extended to report to different destinations
according to need. Default drivers report to console and logfiles.

.. _tc_id:

Testcase run identification
^^^^^^^^^^^^^^^^^^^^^^^^^^^

A message identification mechanism for which all the messages are
prefixed with a code:

[RUNID:]HASH{CBDEL}[XX][.N]

- RUNID is a constant string to identify a run of a set of test cases
  (defaults to nothing, can be autogenerated with `-i` or an specific
  string given with `-i RUNID`).

- HASH is a base32 encoded hash of the testcase name, targets where it
  is to be run and their :term:`BSP model`.

- CBDEL is one capital letter representing the phase being ran
  (Configure, Build, Deploy, Evaluation, cLean)

- [XX]: base32 encoded hash of the BSP name, applies only to dynamic
  test case builds per BSP.

This helps locating anything specific to a testcase by grepping in the
logfile for a given string and adding more components restricts the
output.

This means that the message IDs are stable across runs, save RUNIDs
being specified.

We also use the RUNID:TC combination for a ticket when requesting a
target lock--note this does not conflict with other users, as the
tickets are namespaced per-user. This allows the server log to be used
to crossreference what was being ran, to sort out issues.

The *hash* length (number of characters used) is controlled by
:data:`tcfl.tc.tc_c.hashid_len`.

"""
import ast
import atexit
import collections
import contextlib
import concurrent.futures
import copy
import datetime
import errno
import functools
import imp
import inspect
import json
import logging
import numbers
import os
import platform
import pprint
import random
import re
import shutil
import signal
import socket
import string
import subprocess
import sys

import tempfile
import threading
import time
import traceback
import types
import typing

import tcfl

# We multithread to run testcases in parallel
#
# When massively running threads in production environments, we end up
# with hundreds/thousands of threads based on the setup which are just
# launching a build and waiting. However, sometimes something dies
# inside Python and leaves the thing hanging with the GIL taken and
# everythin deadlocks.
#
# For those situations, using the PathOS/pools library works better,
# as it can multithread as processes (because of better pickling
# abilities) and doesn't die.
#
# So there, the PATHOS.multiprocess library if available and in said
# case, use process pools for the testcases.

_multiprocessing_method_pool_c = None
_multiprocessing_sync_manager = None
_multiprocessing = None

def import_mp_pathos():
    import pathos.pools
    import pathos.multiprocessing
    import multiprocess.managers
    global _multiprocessing_method_pool_c
    global _multiprocessing_tc_pool_c
    global _multiprocessing_sync_manager
    global _multiprocessing
    _multiprocessing_method_pool_c = pathos.pools._ThreadPool
    def _multiprocessing_tc_pool_c(**kwargs):
        return pathos.pools._ProcessPool(maxtasksperchild = 2, **kwargs)
    _multiprocessing_sync_manager = multiprocess.managers.SyncManager
    _multiprocessing = pathos.multiprocessing

def import_mp_std():
    import multiprocessing.pool
    import multiprocessing
    import multiprocessing.managers
    global _multiprocessing_method_pool_c
    global _multiprocessing_tc_pool_c
    global _multiprocessing_sync_manager
    global _multiprocessing
    _multiprocessing_method_pool_c = multiprocessing.pool.ThreadPool
    def _multiprocessing_tc_pool_c(**kwargs):
        return multiprocessing.pool.ThreadPool(**kwargs)
    _multiprocessing_sync_manager = multiprocessing.managers.SyncManager
    _multiprocessing = multiprocessing

mp = os.environ.get('TCF_USE_MP', None)
if mp == None:
    try:
        import_mp_pathos()
    except ImportError as e:
        import_mp_std()
elif mp.lower() == 'std':
    import_mp_std()
elif mp.lower() == 'pathos':
    import_mp_pathos()
else:
    raise RuntimeError('Invalid value to TCF_USE_MP (%s)' % mp)


import requests.exceptions

from . import app
import commonl
import commonl.expr_parser
from . import ttb_client
from . import msgid_c

# discovered by the importer
version = None


#
# FIXME: COMPAT moved to tcfl to start cleaning up import hell
#
# - deprecate by putting in class-as-namespace in tcfl, add a warning
#   message
#
exception = tcfl.exception
pass_e = tcfl.pass_e
error_e = tcfl.error_e
blocked_e = tcfl.blocked_e	# deprecate
block_e = tcfl.blocked_e
timeout_error_e = tcfl.timeout_error_e
failed_e = tcfl.fail_e	# deprecate
fail_e = tcfl.fail_e
timeout_failed_e = tcfl.timeout_failed_e
skip_e = tcfl.skip_e

valid_results = tcfl.valid_results

logger = logging.getLogger("tcfl.tc")
# classes inheriting tc_c can have methos called any of these NAME*
# which will be run as test steps

_method_phase_prefixes = frozenset({
    'configure',
    'build',
    'deploy',
    'setup',
    'start',
    'eval',
    'teardown',
    'class_teardown',
    'clean',
})

class _simple_namespace:
    def __init__(self, kw):
        self.__dict__.update(kw)

#
# I dislike globals, but sometimes they are needed
#

log_dir = None
report_console_impl = None
report_file_impl = None
# FIXME: no like, this is a hack and shall use tc_c.ticket
ticket = None

def _globals_init():
    # This function is used by test routines to cleanup the global state

    global log_dir
    global report_console_impl
    global report_file_impl
    # Should be possible to move most of these to tc_c
    log_dir = None
    tc_c._dry_run = False
    tc_c.runid = None
    if report_console_impl:
        report_driver_c.remove(report_console_impl)
        report_console_impl = None
    if report_file_impl:
        report_driver_c.remove(report_file_impl)
        report_file_impl = None

class target_extension_c(object):
    r"""Implement API extensions to targets

    An API extension allows you to extend the API for the
    :py:class:`tcfl.tc.target_c` class so that more functionality can
    be added to the `target` objects passed to testcase methods (like
    build\*(), eval\*(), etc) and used as::

    >>> class extension_a(target_extension_c):
    >>>   def function(self):
    >>>     self.target.report_info("Hello world from extension_a")
    >>>   variable = 34
    >>> ...
    >>> target_c.extension_register(etension_a)
    >>> ...

    Now, in an (e.g) evaluation function in a testcase:

    >>> @tcfl.tc.target()
    >>> @tcfl.tc.target()
    >>> class _test(tcfl.tc.tc_c):
    >>>
    >>>   def eval_something(self, target, target1):
    >>>      target1.extension_a.function()
    >>>      if target1.extension_a.variable > 3:
    >>>        do_something()
    >>> ...

    Extensions have to be registered with
    :py:func:`tcfl.tc.target_c.extension_register`. Unregister with
    :py:func:`tcfl.tc.target_c.extension_unregister`

    The extension can be anything, but are commonly used to provide
    the code to access APIs that are not distributed as part of the
    core TCF distribution (like for example, an API to access an
    special sensor).

    A package might add support on the server side for an interface to
    access the target and on the client side to access said
    interfaces.

    The `__init__()` method will typically first check if the target
    meets the criteria needed for the extension to work or be used. If
    not it can raise :py:class:`target_extension_c.unneeded` to avoid
    the extension to be created.

    Then it proceeds to create an instance that will be attached to
    the target for later use.

    """

    class unneeded(Exception):
        """
        Raise this from __init__() if this extension is not needed for
        this target.
        """
        pass

    def __init__(self, _target):
        assert isinstance(_target, target_c)
        #: Target this extension applies to
        self.target = _target

    @classmethod
    def __check_name(cls):
        # verify this is a valid extension name
        pass


#: Character used to separate RUNID/HASHID in reports
#:
#: This is the consequence of a very bad past design decisison which
#: called for a filename *report-RUNID:HASHID.txt* and of course,
#: colons are bad because they are used to mean a lot of things.
#:
#: Trying to move to hashes, but there is a lot of legacy, so these
#: variables allow to quickly to new behaviour via configuration.
#:
#: Defaults to existing *:*
report_runid_hashid_separator = ":"

#: Character used to separate RUNID/HASHID in filenames
#:
#: Defaults to existing *:* (see comments for
#: data:`report_runid_hashid_separator`)
report_runid_hashid_file_separator = ":"


class report_driver_c(object):
    """Reporting driver interface

    To create a reporting driver, subclass this class, implement
    :meth:`report` and then create an instance, adding it calling
    :meth:`add`.

    A testcase reports information by calling the `report_*()` APIs in
    :class:`reporter_c`, which multiplexes into each reporting driver
    registered with :meth:`add`, calling each drivers :meth:`report`
    function which will direct it to the appropiate place.

    Drivers can be created to dump the information in any format and
    to whichever location, as needed.

    For examples, look at:

    - :mod:`tcfl.report_console`
    - :mod:`tcfl.report_jinja2`
    - :mod:`tcfl.report_taps`
    - :mod:`tcfl.report_mongodb`

    """

    #: Name for the driver
    #
    #: This is optional and can be set with the
    #: :meth:`report_driver_c.add() <add>` call.
    name = None

    def report(self, testcase, target, tag, ts, delta,
               level, message, alevel, attachments):
        """Low level report from testcases

        The reporting API calls this function for the final recording
        of a reported message. In here basically anything can be
        done--but they being called frequently, it has to be efficient
        or will slow testcase execution considerably. Actions done in
        this function can be:

        - filtering (to only run for certain testcases, log levels, tags or
          messages)
        - dump data to a database
        - record to separate files based on whichever logic
        - etc

        When a testcase is completed, it will issue a message
        *COMPLETION <result>*, which marks the end of the testcase.

        When all the testcases are run, the global testcase reporter
        (:data:`tcfl.tc.tc_global`) will issue a *COMPLETION <result>*
        message. The global testcase reporter can be identified because
        it has an attribute *skip_reports* set to *True* and thus can
        be identified with:

        .. code-block:: python

           if getattr(_tc, "skip_reports", False) == True:
               do_somethin_for_the_global_reporter

        Important points:

        - **do not rely on globals**; this call is not lock protected
          for concurrency; will be called for *every* single report the
          internals of the test runner and the testcases do from
          multiple threads at the same time. Expect a *lot* of calls.

          Must be ready to accept multiple threads calling from
          different contexts. It is a good idea to use thread local
          storage (TLS) to store state if needed. See an example in
          :class:`tcfl.report_console.driver`).

        :param tcfl.tc_c testcase: testcase tho is reporting
          this. Note this might be a top level or a subcase.

        :param tcfl.target_c target: target who is reporting this;
          might be *None* if the report is not associated to a target.

        :param str tag: type of report (PASS, ERRR, FAIL, BLCK, INFO,
          DATA); note they are all same length and described in
          :data:`valid_results`.

        :param float ts: timestamp for when the message got generated
          (in seconds)

        :param float delta: time lapse from when the testcase started
          execution until this message was generated.

        :param int level: report level for the *message* (versus for
          the attachments); note report levels greater or equal to
          1000 are used to pass control messages, so they might not be
          subject to normal verbosity control (for example, for a log
          file you might want to always include them).

        :param str message: single line string describing the message
          to report.

          If the message starts with  *"COMPLETION "*, this is the
          final message issued to mark the result of executing a
          single testcase. At this point, you can use fields such
          as :data:`tc_c.result` and :data:`tc_c.result_eval` and it
          can be used as a synchronization point to for example, flush
          a file to disk or upload a complete record to a database.

          Python2: note this has been converted to unicode UTF-8

        :param int alevel: report level for the attachments

        :param dict attachments: extra information to add to the
          message being reported; shall be reported as *KEY: VALUE*;
          VALUE shall be recursively reported:

          - lists/tuples/sets shall be reported indicating the index
            of the member (such as *KEYNAME[3]: VALUE*

          - dictionaries shall be recursively reported

          - strings and integers shall be reported as themselves

          - any other data type can be reported as what it's *repr*
            function returns when converting it to unicode ro whatever
            representation the driver can do.

          You can use functions such :func:`commonl.data_dump_recursive`
          to convert a dictionary to a unicode representation.

          This might contain strings that are not valid UTF-8, so you
          need to convert them using :func:`commonl.mkutf8` or
          similar. :func:`commonl.data_dump_recursive` do that for you.

        """
        raise NotImplementedError

    _drivers = []

    @classmethod
    def add(cls, obj, name = None, origin = None):
        """
        Add a driver to handle other report mechanisms

        A report driver is used by *tcf run*, the meta test runner, to
        report information about the execution of testcases.

        A driver implements the reporting in whichever way it decides
        it needs to suit the application, uploading information to a
        server, writing it to files, printing it to screen, etc.

        >>> class my_report_driver(tcfl.tc.report_driver_c)
        >>>     ...
        >>> tcfl.tc.report_driver_c.add(my_report_driver())

        :param tcfl.tc.report_driver_c obj: object subclasss of
          :class:`tcfl.tc.report_driver_c` that implements the
          reporting.

        :param str name: (optional) driver name; useful so
          :data:`reporter_c.level_driver_max` can be used.

          The name *console* refers to a driver used to print stuff to
          the console/command line from which *tcf run* (for example)
          is exectuted.

          If from a configuration file a report driver is added as
          *console*, then core will not add the default one
          (:class:`tcfl.report_console.driver`).

        :param str origin: (optional) where is this being registered;
          defaults to the caller of this function.
        """
        assert isinstance(obj, cls)
        if origin == None:
            o = inspect.stack()[1]
            origin = commonl.origin_get(2)
        setattr(obj, "origin", origin)

        signature = inspect.signature(cls.report)
        if len(signature.parameters) != 10:
            # old style, bail out
            raise RuntimeError(
                f"WARNING! Driver {cls} (@{origin}) is old style,"
                f" please update to new report_driver_c.report() [{len(signature.parameters)}]")

        obj.name = name
        cls._drivers.append(obj)

    @classmethod
    def get_by_name(cls, name):
        for driver in cls._drivers:
            if driver.name == name:
                return driver
        raise ValueError("%s: report driver does not exist" % name)

    @classmethod
    def remove(cls, obj):
        """
        Remove a report driver previously added with :meth:`add`

        :param tcfl.tc.report_driver_c obj: object subclasss of
          :class:`tcfl.tc.report_driver_c` that implements the
          reporting.

        """
        assert isinstance(obj, cls)
        cls._drivers.remove(obj)


class reporting_logging_handler_c(logging.Handler):
    """
    logging.Handler interface: allows using this as a logging handler
    that can be used to interface Python's logging mechanism

    Given a :class:`tcfl.tc.tc_c` object, create a logger:

    >>> log = logging.getLogger("somename")

    then add a handler to it from the testcase or a target:

    >>> log.addHandler(testcase.logging_handler)
    >>> log.addHandler(target.testcase.logging_handler)

    tadah!
    """

    def __init__(self, reporter):
        self.reporter = reporter
        logging.Handler.__init__(self, level = 1)


    def emit(self, record):
        # logging levels: maximum (CRITICAL) is 50, minimum (DEBUG is
        # 10), verbosity goes from 0 (least to N) most, so reverse
        # moves from 50, 40, 30 ... 10 to 5, 4, 3, 2,

        attachments = {
            "origin": record.pathname + ":" + record.funcName + ":" + str(record.lineno),
            "level": record.levelno,
            "logger_name": record.name,
            "process": record.process,
            "processName": record.processName,
            "thread": record.thread,
            "threadName": record.threadName,
        }
        if record.exc_info:
            attachments['exc_info.type'] = record.exc_info[0]
            attachments['exc_info.message'] = record.exc_info[1]
            attachments['exc_info.traceback'] = traceback.format_tb(record.exc_info[2])
        if record.exc_text:
            attachments['exc_text'] = record.exc_text
        if record.levelno >= logging.ERROR:  # includes logging.CRITICAL
            self.reporter.report_error(record.getMessage(),
                                       attachments = attachments,
                                       level = 0, alevel = 3)
        elif record.levelno >= logging.WARNING:
            level = int((record.levelno - logging.DEBUG) / 10)
            self.reporter.report_info(record.getMessage(),
                                      attachments = attachments,
                                      level = 1 + level, alevel = level + 2)
        elif record.levelno >= logging.DEBUG and record.levelno < logging.WARNING:
            # incs INFO
            level = int((record.levelno - logging.DEBUG) / 10)
            self.reporter.report_info(record.getMessage(),
                                      attachments = attachments,
                                      level = 2 + level, alevel = level + 2)
        if record.levelno < logging.DEBUG:
            level = record.levelno + 3
            self.reporter.report_info(record.getMessage(),
                                      attachments = attachments,
                                      level = level, alevel = level + 2)


class reporter_c:
    """
    High level reporting API

    Embedded as part of a target or testcase, allows them to report in
    a unified way

    This class accesses members that are undefined in here but defined
    by the class that inherits it (tc_c and target_c):

    - self.kws

    """
    def __init__(self, testcase = None):
        # this is to be set by whoever inherits this
        self._report_prefix = "reporter_c._report_prefix/uninitialized"
        #: time when this testcase or target was created (and thus all
        #: references to it's inception are done); note if this is for
        #: a testcase, in __init_shallow__() we update this for when
        #: we assign it to a target group to run.
        if testcase:
            self.ts_start = testcase.ts_start
            # FIXME: HORRIBLE HACK
            #
            # Can't use isinstance(tclf.tc_info_c) because we still
            # have import hell (tcfl.config gets imported by
            # tcfl.__init__) -- once that gets solved, this can be
            # done properly; it causes:
            #
            # Can't use AttributeError: partially initialized module 'tcfl' has no attribute 'tc_info_c' (most likely due to a circular import)

            horrible_hack_type = str(type(testcase))
            assert isinstance(testcase, tc_c) or \
                "tc_info_c" in horrible_hack_type
            self.testcase = testcase	# record who our testcase is
        else:
            self.ts_start = time.time()
            assert isinstance(self, tc_c)
            self.testcase = self
        self.logging_handler = reporting_logging_handler_c(self)


    #: Ignore messages with verbosity about this level
    #:
    #: >>> self.level_max = 4
    report_level_max = None

    #: Ignore messages with verbosity about this level (per driver)
    #:
    #: >>> class _test(tcf.tc.tc_c):
    #: >>>     ...
    #: >>>     report_level_driver_max = {
    #: >>>         "DRIVERNAME": 4,
    #: >>>         "DRIVERNAME2": 7
    #: >>>     }
    #:
    report_level_driver_max = {}

    @staticmethod
    def _argcheck(message, attachments, level, dlevel, alevel):
        assert isinstance(message, str), \
            f"message: expected str, got {type(message)}"
        if attachments:
            assert isinstance(attachments, dict)
            # FIXME: valid values?
        assert level >= 0
        # just check is some kind of integer (positive, negative or zero)
        assert dlevel >= 0 or dlevel < 0
        assert alevel >= 0 or alevel < 0

    def _report(self, level, alevel, tag, message,
                attachments, subcase = None, subcase_base = None,
                soft = False):
        assert subcase == None or isinstance(subcase, str), \
            f"subcase: expected short string; got {type(subcase)}"
        assert isinstance(soft, bool), \
            f"soft: expected bool; got {type(soft)}"
        if self.report_level_max != None and level >= self.report_level_max:
            return
        ts = time.time()
        delta = ts - self.ts_start

        testcase = self.testcase
        subl = []
        # If there is no subcase_base, then we take it from the TLS
        # stack; this is normally used when we raise an exception that
        # has to be reported in a different subcase path, not in the
        # current one in the stack. That is done by tcfl.msgid_c.__exit__
        if subcase_base == None:
            subcase_base = msgid_c.subcase()
        if subcase_base:
            subl.append(subcase_base)
        if subcase:
            subl.append(subcase)
        subcase = "##".join(subl)

        if subcase:
            subtc = testcase._subcase_get(subcase)
            if not soft:
                if tag == "PASS":
                    subtc.result.passed += 1
                elif tag == "FAIL":
                    subtc.result.failed += 1
                elif tag == "ERRR":
                    subtc.result.errors += 1
                elif tag == "BLCK":
                    subtc.result.blocked += 1
                elif tag == "SKIP":
                    subtc.result.skipped += 1
            report_on = subtc
        else:
            # FIXME: this should also set testcase.result.WHATEVER += 1
            report_on = testcase

        for driver in report_driver_c._drivers:
            if driver.name:
                level_driver_max = self.report_level_driver_max.get(driver.name, None)
                if level_driver_max != None and level >= level_driver_max:
                    continue
            if isinstance(self, target_c):
                target = self
            else:
                target = None
            driver.report(
                report_on, target, tag, ts, delta, level,
                commonl.mkutf8(message), alevel, attachments)


    def report_pass(self, message, attachments = None,
                    level = None, dlevel = 0, alevel = 2, subcase = None,
                    soft = False):
        """Report a check has passed (a positive condition we were
        looking for was found).

        >>> report_pass("this thing worked ok",
        >>>             dict(
        >>>                 measurement1 = 34,
        >>>                 log = commonl.generator_factory_c(
        >>>                     commonl.file_iterator, "LOGILENAME")
        >>>             ),
        >>>             subcase = "subtest1")

        A check, described by *message* has passed

        :param str message: message describing the check or condition
          that has passed

        :param dict attachments: (optional) a dictionary of extra data
          to append to the report, keyed by string. Stick to simple
          values (bool, int, float, string, nested dict of the same )
          for all report drivers to be able to handle it.

          Additionally, use class:`commonl.generator_factory_c` for
          generators (since the report drivers will have to spin the
          generator once each).

        :param str subcase: (optional, default *None*) report this
          message as coming from a subcase

        :param int level: (optional, default set by
          :class:`tcfl.msgid_c` context depth) verbosity level of this
          message. Must be a zero or positive integer. 0 is most
          important. Usually you want to set *dlevel*.

        :param int dlevel: (optional, default 0) verbosity level of
          this message relative to level (normally to the default
          level).

          Eg: if given -2 and level resolves to 4, the verbosity level
          would be 2.

        :param int alevel: (optional, default 2) verbosity level of
          the attachments to this message relative to level (normally
          to the default level).

          The attachments might contain a lot of extra data that in
          some cases is not necessary unless more verbosity is
          declared.

        :param bool soft: (optional; default *False*) consider this a
          *soft* condition (which will not update the testcase's
          pass/failure/error/block counts)

          When reporting a condition that can be recovered, it is
          customary to set this field to *True* and when the condition
          is confirmed to call it with *soft* set to *False*.

          Eg: when retrying soft failures

        """
        if level == None:		# default args are computed upon def'on
            level = msgid_c.depth()
        self._argcheck(message, attachments, level, dlevel, alevel)
        level += dlevel
        self._report(level, level + alevel, "PASS", message,
                     attachments, subcase = subcase, soft = soft)

    def report_fail(self, message, attachments = None,
                    level = None, dlevel = 0, alevel = 2, subcase = None,
                    soft = False):
        """
        Report a check that has failed (an negative condition we were
        looking for was found).

        Same parameters as :meth:`report_pass`.
        """
        if level == None:		# default args are computed upon def'on
            level = msgid_c.depth()
        self._argcheck(message, attachments, level, dlevel, alevel)
        level += dlevel
        self._report(level, level + alevel, "FAIL", message,
                     attachments, subcase = subcase, soft = soft)

    def report_error(self, message, attachments = None,
                     level = None, dlevel = 0, alevel = 2, subcase = None,
                     soft = False):
        """
        Report a check that has errored (a negative condition we were
        not looking for was found).

        Same parameters as :meth:`report_pass`.
        """
        if level == None:		# default args are computed upon def'on
            level = msgid_c.depth()
        self._argcheck(message, attachments, level, dlevel, alevel)
        level += dlevel
        self._report(level, level + alevel, "ERRR", message,
                     attachments, subcase = subcase, soft = soft)

    def report_blck(self, message, attachments = None,
                    level = None, dlevel = 0, alevel = 2, subcase = None,
                    soft = False):
        """
        Report a check that has blocked (something has happened most
        likely in infrastructure that disallows us for checking
        conditions to determine pass/fail/error/skip).

        Same parameters as :meth:`report_pass`.
        """
        if level == None:		# default args are computed upon def'on
            level = msgid_c.depth()
        self._argcheck(message, attachments, level, dlevel, alevel)
        level += dlevel
        self._report(level, level + alevel, "BLCK", message,
                     attachments, subcase = subcase, soft = soft)

    def report_skip(self,  message, attachments = None,
                    level = None, dlevel = 0, alevel = 2, subcase = None,
                    soft = False):
        """
        Report a check that has skipped (the conditions needed to test
        are not met).

        Same parameters as :meth:`report_pass`.
        """
        if level == None:		# default args are computed upon def'on
            level = msgid_c.depth()
        self._argcheck(message, attachments, level, dlevel, alevel)
        level += dlevel
        self._report(level, level + alevel, "SKIP", message,
                     attachments, subcase = subcase, soft = soft)

    def report_info(self, message, attachments = None,
                    level = None, dlevel = 0, alevel = 2, subcase = None):
        """
        Report an informational progress message.

        Same parameters as :meth:`report_pass`.
        """
        if level == None:		# default args are computed upon def'on
            level = msgid_c.depth()
        self._argcheck(message, attachments, level, dlevel, alevel)
        level += dlevel
        self._report(level, level + alevel, "INFO", message,
                     attachments, subcase = subcase)

    def report_data(self, domain, name, value, expand = True,
                    level = 1, dlevel = 0, subcase = None):
        """Report measurable data

        When running a testcase, if data is collected that has to be
        reported for later analysis, use this function to report
        it. This will be reported by the report driver in a way that
        makes it easy to collect later on.

        Measured data is identified by a *domain* and a *name*, plus
        then the actual value.

        A way to picture how this data can look once aggregated is as
        a table per domain, on which each invocation is a row and each
        column will be the values for each name.

        :param str domain: to which domain this measurement applies
          (eg: "Latency Benchmark %(type)s");

          Well known domains:

           - *Warnings [%(type)s]*: values would be accumulated over
             how many times it has been reported

             >>> # self is a tcfl.tc.tc_c
             >>> condition = "SOMENAME"
             >>> with self.lock:
             >>>     self.buffers.setdefault(condition, 0)
             >>>     self.buffers[condition] += 1
             >>> self.report_data("Warnings [%(type)s]", condition,
             >>>     self.buffers[condition])

           - *Recovered conditions [%(type)s]*: values would be
             accumulated over how many times it has been reported

             Same reporting example as for *Warnings* above.

        :param str name: name of the value  (eg: "context switch
          (microseconds)"); it is recommended to always add the unit
          the measurement represents.

        :param value: value to report for the given domain and name;
           any type can be reported.

        :param bool expand: (optional) by default, the *domain* and
          *name* fields will be %(FIELD)s expanded with the keywords
          of the testcase or target. If *False*, it will not be
          expanded.

          This enables to, for example, specify a domain of "Latency
          measurements for target %(type)s" which will automatically
          create a different domain for each type of target.
        """
        assert isinstance(domain, str)
        assert isinstance(name, str)
        assert level >= 0
        assert dlevel >= 0
        assert isinstance(expand, bool)

        if expand:
            domain = domain % self.kws
            name = name % self.kws
        level += dlevel

        self._report(
            level, 1000, "DATA",
            domain + "::" + name + "::" + str(value), subcase = subcase,
            attachments = dict(domain = domain, name = name, value = value))

    def report_tweet(self, what, result, extra_report = "",
                     ignore_nothing = False, attachments = None,
                     level = None, dlevel = 0, alevel = 2,
                     dlevel_failed = 0, dlevel_blocked = 0,
                     dlevel_passed = 0, dlevel_skipped = 0, dlevel_error = 0,
                     subcase = None):
        if level == None:		# default args are computed upon def'on
            level = msgid_c.depth()
        self._argcheck(what, attachments, level, dlevel, alevel)
        assert dlevel_failed >= 0
        assert dlevel_blocked >= 0
        assert dlevel_passed >= 0
        assert dlevel_skipped >= 0
        assert dlevel_error >= 0
        level += dlevel
        r = False
        if result.failed > 0:
            tag = "FAIL"
            msg = valid_results[tag][1]
            level += dlevel_failed
        elif result.errors > 0:
            tag = "ERRR"
            msg = valid_results[tag][1]
            level += dlevel_error
        elif result.blocked > 0:
            tag = "BLCK"
            msg = valid_results[tag][1]
            level += dlevel_blocked
        elif result.passed > 0:
            tag = "PASS"
            msg = valid_results[tag][1]
            r = True
            level += dlevel_passed
        elif result.skipped > 0:
            tag = "SKIP"
            msg = valid_results[tag][1]
            r = True
            level += dlevel_skipped
        else:            # When here, nothing was run, all the counts are zero
            if ignore_nothing == True:
                return True
            self._report(level, level + alevel, "BLCK",
                         what + " / nothing ran " + extra_report,
                         attachments, subcase = subcase)
            return False
        self._report(level, level + alevel,
                     tag, what + " " + msg + " " + extra_report,
                     attachments, subcase = subcase)
        return r

    # logging.Handler interface
    #
    # Adaptor that allows using this as a logging handler that can be
    # used to interface Python's logging mechanism
    def emit(self, record):
        """

        >>> tc = XYZ
        >>> log = logging.getLogger("somename")
        >>> log.addHandler(target)
        >>> log.addHandler(target.testcase)

        """
        # logging levels: maximum (CRITICAL) is 50, minimum (DEBUG is
        # 10), verbosity goes from 0 (least to N) most, so reverse
        # moves from 50, 40, 30 ... 10 to 5, 4, 3, 2,

        attachments = {
            "origin": record.pathname + ":" + report.funcName + ":" + str(report.lineno),
            "level": record.level,
            "logger_name": record.name,
            "process": record.process,
            "processName": record.processName,
            "thread": record.thread,
            "threadName": record.threadName,
        }
        if record.exc_info:
            attachments['exc_info'] = record.exc_info
        if record.exc_text:
            attachments['exc_text'] = record.exc_text
        if record.level >= logging.ERROR:  # includes logging.CRITICAL
            self.error(record.getMessage(), attachments, level = 0)
        elif record.level >= logging.WARNING:
            level = (record.level - logging.DEBUG) / 10
            self.info(record.getMessage(), attachments, level = 1 + level)
        elif record.level >= logging.DEBUG and record.level < logging.WARNING:
            # incs INFO
            level = (record.level - logging.DEBUG) / 10
            self.info(record.getMessage(), attachments, level = 2 + level)
        if record.level < logging.DEBUG:
            self.info(record.getMessage(), attachments, level = 3 + level)

class target_c(reporter_c):
    """A remote target that can be manipulated

    :param dict rt: remote target descriptor (dictionary) as returned
        by :py:mod:`tcfl.targets`.
    :param tc_c tescase: testcase descriptor to which this target
        instance will be uniquely assigned.

    A target always operate in a given :term:`BSP model`, as decided
    by the testcase runner. If a remote target A has two BSP models (1
    and 2) and a testcase T shall be run on both, it will create two
    testcase instances, T1 and T2. Each will be assigned an instance
    of :class:`target_c`, A1 and A2 respectively, representing the
    same target A, but each set to a different :term:`BSP model`.

    Note these object expose the basic target API, but then different
    extensions provide APIs to access other interfaces, depending on
    if the target exposes it or not; these is the current list of
    implemented interfaces:

    - :py:class:`console <tcfl.target_ext_console.extension>`
    - :py:class:`capture <tcfl.target_ext_capture.extension>` for
      stream and snapshot captures of audio, video, network traffic,
      etc
    - :py:class:`debug <tcfl.target_ext_debug.extension>`
    - :py:class:`fastboot <tcfl.target_ext_fastboot.extension>`
    - :py:class:`images <tcfl.target_ext_images.extension>`
    - :py:class:`power <tcfl.target_ext_power.extension>`
    - :py:class:`shell <tcfl.target_ext_shell.shell>`
    - :py:class:`ssh <tcfl.target_ext_ssh.ssh>`
    - :py:class:`tunnel <tcfl.target_ext_tunnel.tunnel>`
    - :py:class:`zephyr <tcfl.app_zephyr.zephyr>`

    """

    #
    # Public API
    #
    def __init__(self, rt, testcase, bsp_model, target_want_name,
                 extensions_only = None):
        assert isinstance(rt, dict)
        assert isinstance(testcase, tc_c)
        reporter_c.__init__(self, testcase = testcase)
        #: Name this target is known to by the testcase (as it was
        #: claimed with the :func:`tcfl.tc.target` decorator)
        self.want_name = target_want_name
        #: Remote tags of this target
        self.rt = rt
        # Mind those static TCs, local target has no rtb
        self.server_url = rt.get('server', None)
        # transition path to new core client code in tcfl,
        # tcfl.servers + tcfl.targets; if there is a 'server'
        # keyword, it means this has been discovered with the new
        # core code so we use it
        self.server = tcfl.server_c.servers[self.server_url]
        # FIXME: this is here until we transition away all of the
        # ttb_client sublibrary
        self.rtb = tcfl.ttb_client.rest_target_brokers[self.server_url]
        #: (short) id of this target
        self.id = rt['id']
        #: Full id of this target
        self.fullid = rt['fullid']
        #: Type name of this target
        self.type = rt['type']
        #: ticket used to acquire this target
        self.ticket = testcase.ticket
        #: Testcase that this target is currently executing
        self.testcase = testcase
        #: All of BSPs supported by this target
        self._bsps_all = list(rt.get('bsps', {}).keys())
        #: :term:`BSP model` this target is currently configured for
        self.bsp_model = None
        #: List of BSPs supported by this target's currently selected
        #: :term:`BSP model`
        self.bsps = []
        #: Make sure the testcase indicates the daemon that this
        #: target is to be marked as active during the testcase
        #: execution of expectation loops.
        self.keep_active = True
        #: Dict of BSPs that have to be stubbed for the board to work
        #: correctly in the current :term:`BSP model` (if the board
        #: has two BSPs but BSP1 needs to have an image of something
        #: so BSP2 can start). The App builder can manipulate this to
        #: remove BSPs that can be ignored. The value is a tuple (app,
        #: srcinfo) that indicates the App builder that will build the
        #: stub and with which source information (path to the
        #: source).
        self.bsps_stub = {}
        #: BSP (inside the :term:`BSP model`) this target is currently
        #: configured for
        self.bsp = None
        # KW handling for targets is a little bit messed up, as we
        # want to include the keywords root testcase kws, the ones for
        # the testcase instance, the ones for the target (incuding
        # anything the user might want to set), the ones for the
        # target's :term:`BSP model` and the ones for the target's
        # BSP.  So we keep the "originals" for modification with
        # kw_set in self._kw and then the update KWS modifies the
        # self.kws set of keywords merging everthing together.
        # note self.kws comes from reporter_c
        #: Target specific keywords
        self._kws = {}
        #: Target specific keywords for each BSP
        #:
        #: This is where BSP specific information can be set
        self._kws_bsp = {}
        #: Origin of keys defined in self._kws
        self._kws_origin = {}

        #: Temporary directory where to store files -- this is the same
        #: as the testcase's -- it is needed for the report driver to
        #: find where to put stuff.
        self.tmpdir = os.path.join(testcase.tmpdir, "targets", target_want_name)
        commonl.makedirs_p(self.tmpdir)

        #: Keywords for ``%(KEY)[sd]`` substitution specific to the
        #: testcase or target and its current active :term:`BSP model`
        #: and BSP as set with :meth:`bsp_set`.
        #:
        #: FIXME: elaborate on testcase keywords, target keyworkds
        #:
        #: These are obtained from the remote target descriptor
        #: (`self.rt`) as obtained from the remote *ttbd* server.
        #:
        #: These can be used to generate strings based on information,
        #: as:
        #:
        #:   >>>  print "Something %(id)s" % target.kws
        #:   >>>  target.shcmd_local("cp %(id)s.config final.config")
        #:
        #: To find which fields are available for a target::
        #:
        #:   $ tcf list -vv TARGETNAME
        #:
        #: The testcase will provide also other fields, in
        #: :data:`tcfl.tc.tc_c.kws`, which are rolled in in this variable
        #: too. See how to more available keywords :ref:`here
        #: <finding_testcase_metadata>`
        #:
        #: Note that testcases might be setting more keywords in the
        #: target or the testcase with:
        #:
        #: >>> target.kw_set("keywordname", "somevalue")
        #: >>> self.kw_set("keywordname", "somevalue")
        #:
        #: as well, any of the target's properties set with
        #: :meth:`TARGET.property_set<tcfl.tc.target_c.property_set>`
        #: (or it's command line equivalent ``tcf property-set TARGET
        #: PROPERTY VALUE``) will show up as keywords.
        self.kws = {}		# Updated when we update the kws
        #: Origin of keys defined in self.kws
        self.kws_origin = {}	# in tc_c.__init_shallow__()
        # Generate KWS in case the extensions have to use it
        if bsp_model:
            self._bsp_model_set(bsp_model)
        else:
            self._report_mk_prefix()
        # Fire up the extensions
        for name, extension in self.__extensions.items():
            if extensions_only != None and name not in extensions_only:
                continue
            try:
                self.report_info("%s: %s: initializing target extension"
                                 % (self.want_name, name),
                                 dlevel = 6)
                e = extension(self)
                setattr(self, name, e)
                e.target = self
                self.report_info("%s: %s: initialized target extension"
                                 % (self.want_name, name),
                                 dlevel = 5)
            except target_extension_c.unneeded:
                self.report_info("%s: %s: unneeded target extension"
                                 % (self.want_name, name),
                                 dlevel = 5)
                continue
            except Exception as e:
                self.report_blck("%s: %s: exception initializing extension: %s"
                                 % (self.want_name, name, e))
                raise
        # Re-generate KWS in case the extensions have changed anything
        if bsp_model:
            self._bsp_model_set(bsp_model)
        else:
            self._report_mk_prefix()
        # And finally, set the default BSP to the default (which is
        # the first one declared by the target, if any)
        if self.bsps:
            self.bsp_set()
        #: Shall we acquire this target? By default the testcases get
        #: the targets they request acquired for exclusive use, but in
        #: some cases, it might not be needed (default: True)
        self.do_acquire = True

        #: Lock to manipulate the target--when doing state
        #: modification operations from multiple threads in the same
        #: testcase, this can be taken to avoid race conditions.
        #
        #: Note this only applies mainly to _*set()* operations and
        #: remember other testcases can't use the target.
        self.lock = threading.Lock()


    @classmethod
    def extension_register(cls, ext_cls, name = None):
        """
        Register an extension to the :py:class:`tcfl.tc.target_c` class.

        This is usually called from a config file to register an
        extension provided by a package.

        See :py:class:`target_extension_c` for details

        :param target_extension_c ext_cls: a class that provides an extension
        :param str name: (optional) name of the extension (defaults to
          the class name)
        """
        assert issubclass(ext_cls, target_extension_c)
        assert name == None or isinstance(name, str)
        if name == None:
            name = ext_cls.__name__
        if name in list(target_c.__dict__.keys()):
            raise RuntimeError("%s: can't register extension, there is "
                               "already a field in class target_c with "
                               "said name that would be overriden"
                               % name)
        if name in cls.__extensions:
            raise RuntimeError("%s: can't register extension, there is "
                               "already one registered with said name"
                               % name)
        cls.__extensions[name] = ext_cls

    @classmethod
    def extension_unregister(cls, ext_cls, name = None):
        """
        Unregister an extension to the :py:class:`tcfl.tc.target_c` class.

        This is usually used by unit tests. There usually is no need
        to unregister extensions.

        See :py:class:`target_extension_c` for details

        :param target_extension_c ext_cls: a class that provides an extension
        :param str name: (optional) name of the extension (defaults to
          the class name)
        """
        assert name == None or isinstance(name, str)
        if name == None:
            name = ext_cls.__name__
        del cls.__extensions[name]



    #
    # Actions that don't affect the target
    #
    @property
    def bsps_all(self):
        """
        Return a list of all BSPs in the target (note this might be
        more than the ones available in the currentlt selected BSP
        model).
        """
        return self._bsps_all

    def _bsp_model_set(self, bsp_model):
        """\
        Set which :term:`BSP model` this target shall operate on.

        Note this does not physically alter anything on the hardware,
        but it will affect how the build configuration is done so that
        only the right BSPs are used.

        :param str bsp_model: name of the :term:`BSP model` to use, which must
          be one of the ones listed in target.bsp_models.

        """
        if 'bsp_models' not in list(self.rt.keys()) \
           or not bsp_model in self.rt['bsp_models']:
            raise blocked_e("target %s: BSP model '%s' not supported"
                            % (self.fullid, bsp_model))
        bsps = self.rt['bsp_models'][bsp_model]
        if bsps == None:
            bsps = [bsp_model]
        self.bsps = bsps
        self.bsps_stub = {
            bsp: (None, None, None) for bsp in set(self.bsps_all) - set(bsps)
        }
        self.bsp_model = bsp_model
        self._kws_update()
        self._report_mk_prefix()

    def bsp_set(self, bsp = None):
        """Set the active BSP

        If the BSP is omitted, this will select the first BSP in the
        current BSP model. This means that if there is a preference in
        multiple BSPs, they have to be listed as such in the target's
        configuration.

        If there are no BSPs, this will raise an exception

        :param str bsp: (optional) The name of any BSP supported by
          the board (not necessarily in the BSP-Model list of active
          BSPs. These are always in :attr:`bsps_all`.
          If this argument is False, then the active BSP is reset to none.

        """
        # This function assumes there is always an active BSP-Model
        # that has been set, somehow...or that there are no BSPs, so
        # then it makes no sense to call it.
        if bsp == None:
            if not self.bsps:
                raise blocked_e(
                    "%s: can't set any BSP; target's BSP model "
                    "%s has no BSPs" % (self.id, self.bsp_model))
            bsp = self.bsps[0]
        if bsp == False:
            self.bsp = None
        elif isinstance(bsp, str):
            if not bsp in self.bsps_all:
                raise blocked_e(
                    "%s: can't set BSP '%s'; target's BSP model "
                    "%s has only BSPs %s"
                    % (self.id, bsp, self.bsp_model, ",".join(self.bsps_all)))
            self.bsp = bsp
        else:
            raise AssertionError("Bad arguments to target_c.bsp_set()")
        self._kws_update(bsp)
        self._report_mk_prefix()

    def _kws_update_interconnect_addrs(self, interconnects):
        # Some interconnect specific work
        # If we are doing a testcase with interconnects and the target
        # belongs to any of the interconnects, pull out the addresses
        # of the targets in such interconnect to top level, so it can
        # easily do linux.kws['ipv4_addr'] (for example)
        #
        # So we are going to go over all the interconnects this
        # testcase is using and if the target has addresses in any,
        # we'll put those addresses at the top level
        for ic_name in sorted(interconnects):
            for tg_ic_name, tg_ic_dict \
                in self.rt.get('interconnects', {}).items():
                if tg_ic_name != ic_name:
                    continue
                for key, value in tg_ic_dict.items():
                    if not key.endswith("_addr"):
                        continue
                    self._kws[key] = value

    def kws_set(self, d, bsp = None):
        """
        Set a bunch of target's keywords and values

        :param dict d: A dictionary of keywords and values
        :param str bsp: (optional) BSP for which we want to set the
          keywords; if omitted, we are setting the keywords for the whole
          target
        """
        assert isinstance(d, dict)
        for kw in list(d.keys()):
            assert isinstance(kw, str)
        if bsp == None:
            self._kws.update(d)
        else:
            self._kws_bsp.setdefault(bsp, {}).update(d)
        self._kws_update(self.bsp)
        self._report_mk_prefix()

    def kw_set(self, kw, val, bsp = None):
        """
        Set a target's keyword and value

        :param str kw: A string describing naming the keyword/value pair
        :param val: the value (can be anything)
        :param str bsp: (optional) BSP for which we want to set the
          keyword; if omitted, we are setting the keyword for the whole
          target
        """
        assert isinstance(kw, str)
        if bsp == None:
            self._kws[kw] = val
        else:
            self._kws_bsp.setdefault(bsp, {})[kw] = val
        self._kws_update(self.bsp)
        self._report_mk_prefix()

    def kw_unset(self, kw, bsp = None):
        """
        Unset a target's string keyword

        :param str kw: keyword name
        :param str bsp: (optional) BSP for which we want to unset the
          keyword; if omitted, we are setting the keyword for the whole
          target
        """
        assert isinstance(kw, str)
        if bsp == None:
            if kw in self._kws:
                del self._kws[kw]
        else:
            if kw in self._kws_bsp[bsp]:
                del self._kws_bsp[bsp][kw]
        self._kws_update(self.bsp)
        self._report_mk_prefix()

    def kws_required_verify(self, kws):
        """
        Verify if a target exports required keywords, raise blocked
        exception if any is missing.
        """
        assert isinstance(kws, list)
        missing = []
        for kw in kws:
            if not kw in list(self.kws.keys()):
                missing.append(kw)
        if missing:
            raise blocked_e(
                'target does not export needed keywords: %s'
                % ', '.join(missing))


    def ic_key_get(self, ic, key, default = None):
        """
        Obtain the value of an interconnect key

        An *interconnect key* is a key that is specific to the
        interconnect we are talking about; for example, for a network
        X, the proxy might be address A1 but for network Y the proxy
        might be A2.

        A target might be a member of one or more interconnects, as
        described by its tags (*interconnects* section).

        These keys live in the *interconnects.INTERCONNECTNAME*
        hiearchy of the target's inventory. If they are not there,
        they are seeked in the interconnect's inventory (top level),
        thus the resolution order for key *KEY* is:

         - TARGET:interconnects.INTERCONNECTNAME.KEY
         - INTERCONNECTNAME:KEY
         - TARGET:KEY
         - <default>

        **Note:** this order is such since the main goal of this is to
        obtain values of things that are specific to an
        interconnect. A value off TARGET's inventory will be
        general to all the interconnects, so more specific values for
        that interconnect are sure to be found in
        *TARGET:interconnects.INTERCONNECT.KEY* or *INTERCONNECT:KEY*,
        hence they are tried first.

        :param tcfl.tc.target_c ic: target describing the interconnect
          of which this target is a member

        :param str key: name of the inventory key we need

        :param default: (optional, default *None*) value to return if
          not found

        :returns: value of key or the default.

        Note this is different to :meth:`ic_field_get` in that:

        - :meth:`ic_field_get` only gets fields from
          *interconnects.INTERCONNECTNAME*

        - :meth:`ic_field_get` raises exceptions when the key is not
          found


        Notable keys which are accessed with this mechanism:

        - *server.url*: URL of the server when seen inside the
          interconnect (if the network is isolated, so that the target
          can access the server)

        - *http_proxy* / *https_proxy* / *ftp_proxy*: URLs of the
          HTTP/S/FTP proxy
        """
        assert ic == None or isinstance(ic, target_c)
        assert isinstance(key, str)

        interconnect_data = self.kws.get('interconnects', {})
        if ic != None and not ic.id in interconnect_data:
            raise tcfl.error_e(
                f"tried to request interconnect key '{key}'"
                f" from interconnect '{ic.id}';"
                f" but target is not connected to it",
                dict(target = target))
        if ic != None and key in interconnect_data[ic.id]:
            return interconnect_data[ic.id][key]
        if ic != None and key in ic.kws:
            return ic.kws[key]
        if key in self.kws:
            return self.kws[key]
        return default


    def ic_field_get(self, ic, field, field_description = ""):
        """Obtain the value of a field for a target in an interconnect

        A target might be a member of one or more interconnects, as
        described by its tags (*interconnects* section).

        See similar :meth:`ic_key_get`

        :param tcfl.tc.target_c ic: target describing the interconnect
          of which this target is a member (as defined in a
          @ :func:`tcfl.tc.interconnect` decorator to the testcase
          class)

        :param str field: name of the filed whose value we want.

        >>> def eval_somestep(self, ic, target1, target2):
        >>>    target1.shell.run("ifconfig eth0 %s/%s"
        >>>                      % (target2.addr_get(ic, 'ipv4'),
        >>>                         target2.ic_field_get(ic, 'ipv4_addr_len'))
        """
        assert isinstance(ic, target_c)
        assert isinstance(field, str)

        if not 'interconnects' in self.rt:
            raise RuntimeError('target provides no interconnect information')
        if not ic.id in self.rt['interconnects']:
            raise RuntimeError("target is not in interconnect '%s'" % ic.id)
        if not field in self.rt['interconnects'][ic.id]:
            raise RuntimeError(
                "target does not provide field '%s' "
                "%s in interconnect '%s'"
                % (field, field_description, ic.id))
        return self.rt['interconnects'][ic.id][field]

    def addr_get(self, ic, tech, instance = None):
        """Obtain the address for a target in an interconnect

        A target might be a member of one or more interconnects, as
        described by its tags (*interconnects* section).

        :param tcfl.tc.target_c ic: target describing the interconnect
          of which this target is a member (as defined in a
          @ :func:`tcfl.tc.interconnect` decorator to the testcase
          class)

        :param str tech: name of the technology on which address we
          are interested.

          As part of said membership, one or more key/value pairs can be
          specified. Assigned addresses are always called
          *TECHNOLOGY_addr*, were *TECHNOLOGY* can be things like
          *ipv4*, *ipv6*, *bt*, *mac*, etc...

          If *tech* fits a whole key name, it will be used instead.

        :param str instance: (optional) when this target has multiple
          connections to the same interconnect (via multiple physical
          or virtual network interfaces), you can select which
          instance of those it is wanted.

          By default this will return the default instance (eg, the
          one corresponding to the interconnect ``ICNAME``), but if an
          instance is added, it will return the IP address for
          ``ICNAME#INSTANCE`` as declared in the target's
          configuration with functions such as
          :func:`ttbl.test_target.add_to_interconnect`.

        When the target, for the current testcase is member of a
        single interconnect, any *TECHNOLOGY_addr* for the
        interconnect key/value will be available in the :attr:`kws`
        member as for example.

        >>> target.kws['ipv4_addr']

        However, when member of multiple interconnects, which members
        are promoted to top level is undertermined if both
        interconnects provide address information for the same
        technology. Use this function to obtain the
        interconnect-specific information.

        >>> def eval_somestep(self, ic, target1, target2):
        >>>    target1.shell.run("scp /etc/passwd %s:/etc/passwd"
        >>>                      % target2.addr_get(ic, 'ipv4'))

        """
        if instance:
            assert isinstance(instance, str)
            ic = ic + "#" + instance
        return self.ic_field_get(ic, tech + "_addr", "(address)")

    def app_get(self, bsp = None, noraise = True):
        """
        Return the App builder that is assigned to a particular BSP in
        the target.

        :param str bsp: Name of the BSP we are querying the App
          builder for. Can be omitted if the current :term:`BSP model`
          for the target contains a single BSP, and thus it'll be
          auto-guessed.

        :param bool noraise: (optional) if True, do not raise an
          exception if we cannot find an App Builder and just return
          Nones.
        """
        if bsp == None:
            assert len(self.bsps) == 1, "Auto-guessing of BSP can only " \
                "be done if the current BSP model carries only one BSP"
            bsp = self.bsps[0]
        return self._app_get_for_bsp(None, bsp = bsp, noraise = noraise)[0]

    def shcmd_local(self, cmd, origin = None, reporter = None,
                    logfile = None, env = None):
        """
        Run a shell command in the local machine, substituting
        *%(KEYWORD)[sd]* with keywords defined by the target and testcase.
        """
        if origin == None:
            origin = commonl.origin_get(2)
        return self.testcase._shcmd_local(cmd % self.kws, origin = origin,
                                          reporter = self, logfile = logfile,
                                          env = env)
    #
    # Actions that operate on the target
    #
    def release(self, force: bool = False):
        """
        Release a target from its allocation

        :param bool force: (optional; default *False*) force releasing
          the target even if not the owner

        See :meth:`tcfl.server_c.release`
        """
        self.report_info("releasing", dlevel = 3)
        self.server.release(self.rt['id'], force = force)
        self.report_info("released", dlevel = 2)



    def active(self):
        """
        Tell the server a target is being used.

        This way the server doesn't consider it idle and release it.

        You can do this in a more general way sending a keepalive
        request to the server (FIXME:link).

        :param str targetid: name of the target in the server
        """
        # This one is annoying, so debuglevel it up
        self.report_info("marking as active", dlevel = 3)
        self.server.send_request(
            "PUT", f"targets/{self.id}/active")
        self.report_info("marked as active", dlevel = 2)



    def property_get(self, property_name, default = None):
        """
        Read a property from the target

        :param str property_name: Name of the property to read
        :returns str: value of the property (if set) or None

        Note that getting a property named *a.b.c* expects the target
        to have a property *a* that contains a field *b* that is also
        a dictionary and this will return its value *c*.
        """
        self.report_info("reading property '%s'" % property_name, dlevel = 3)
        data = { "projection": json.dumps([ property_name ]) }
        r = self.server.send_request("GET", "targets/" + self.id, data = data)
        # unfold a.b.c.d which returns { a: { b: { c: { d: value } } } }
        propertyl = property_name.split(".")
        for prop_name in propertyl:
            r = r.get(prop_name, None)
            if r == None:
                val = None
                break
        else:
            val = r
        self.report_info("read property '%s': '%s' [%s]"
                         % (property_name, val, default), dlevel = 4)
        if val == None and default != None:
            return default
        return val

    def property_set(self, property_name, value = None):
        """
        Set a property on the target

        :param str property_name: Name of the property to read
        :param value: (optional) Value to set; *None* to unset it
        """
        if value:
            assert isinstance(value, (str, numbers.Integral, numbers.Real, bool))
        self.report_info("setting property '%s' to '%s'"
                         % (property_name, value), dlevel = 4)
        data = { property_name: value }
        self.rtb.send_request("PATCH", "targets/" + self.id,
                              json = data, timeout_extra = None)
        self.report_info("set property '%s' to '%s'" % (property_name, value),
                         dlevel = 2)

    def properties_set(self, d):
        """
        Set a recursive dictionary tree of properties

        :param dict d: Dictionary of properties and values
        """
        assert isinstance(d, dict)
        self.report_info("setting %d properties" % (len(d)), dlevel = 3)
        self.rtb.send_request("PATCH", "targets/" + self.id,
                              json = d, timeout_extra = None)
        self.report_info("set %d properties" % (len(d)), dlevel = 2)

    def properties_get(self, *projections):
        """
        Get a dictionary of properties from the server

        :param str projections: (optional: default all) zero or more
          name of fields to ask for

          Field names can use periods to dig into dictionaries.

          Field names can match :mod:`fnmatch` regular
          expressions.

          >>> target.properties_get("interfaces.tunnel", "instrumentation")

          would return the tree:

          >>> {
          >>>     interfaces: {
          >>>         tunnel: { ... },
          >>>         instrumentation: { ... }
          >>> }

        """
        commonl.assert_none_or_list_of_strings(projections, "projections", "projection")
        if projections:
            data = { 'projections': json.dumps(projections) }
        else:
            data = None
        return self.rtb.send_request("GET", "targets/" + self.id, data = data)


    def disable(self, reason = 'disabled by the administrator'):
        """
        Disable a target, setting an optional reason

        :param str reason: (optional) string describing the reason
          [default: none]

        This sets a field *disabled* in the inventory with
        the messages; convention is this means it is disabled.
        """
        self.property_set('disabled', reason)

    def enable(self):
        """
        Enable a (maybe disabled) target

        This removes the *disabled* field from the inventory.
        """
        self.property_set('disabled', None)

    def thing_plug(self, thing):
        """
        Connect a thing described in the target's :attr:`tags`
        *things* dictionary to the target.

        :param str thing: thing to connect
        """
        if isinstance(thing, target_c):
            thing = thing.id
        assert isinstance(thing, str)
        self.report_info("plugging thing '%s'" % thing, dlevel = 3)
        self.rtb.rest_tb_thing_plug(self.rt, thing)
        self.report_info("plugged thing '%s'" % thing, dlevel = 2)

    def thing_unplug(self, thing):
        """
        Disconnect a thing described in the target's :attr:`tags`
        *things* dictionary from the target.

        :param str thing: thing to disconnect
        """
        if isinstance(thing, target_c):
            thing = thing.id
        assert isinstance(thing, str)
        self.report_info("unplugging thing '%s'" % thing, dlevel = 3)
        self.rtb.rest_tb_thing_unplug(self.rt, thing)
        self.report_info("unplugged thing '%s'" % thing, dlevel = 2)

    def thing_list(self):
        """
        Return a list of connected things
        """
        self.report_info("listing things", dlevel = 3)
        r = self.rtb.rest_tb_thing_list(self.rt)
        self.report_info("listed things", dlevel = 2)
        return r


    def console_tx(self, data, console = None, detect_context = ""):
        """Transmits the data over the given console doing a
        synchronization point with the expect sequences engine.

        This function does not append a newline not translates newline
        characters, see :meth:send for that.

        Synchronization point with the expect sequences engine means
        that after calling this function to send anything to a
        console, :meth:`target.expect <tcfl.tc.target_c.expect>` or
        :meth:`testcase.expect <tcfl.tc.tc_c.expect>` can be used to
        expect only anything received after sending:

        >>> target.console_tx("echo hello1")
        >>> target.console_tx("echo hello2")
        >>> target.console_tx("echo hello3")
        >>> target.expect("hello")		# will match only hello3

        but then:

        >>> target.console_tx("echo hello1")
        >>> target.console_tx("echo hello2")
        >>> target.console_tx("echo hello3")
        >>> target.expect("hello1")		# will not match

        :param data: data to be sent; data can be anything that can be
           transformed into a sequence of bytes

        :param str console: (optional) name of console over which to
           send the data (otherwise use the default one).

        :param str detect_context: (optional) the detection context is
          a string which is used to help identify how to keep track of
          where we are looking for things (detecting) in a console's
          output. Further info :ref:`here
          <console_expectation_detect_context>`

        Note this function is equivalent to
        :meth:`target.console.write
        <tcfl.target_ext_console.extension.write>`, which is the raw
        version of this function.

        See :meth:`send` for a version that also translates newline
        conventions and appends a newline
        """
        # the console member is only present if the member extension has
        # been loaded for the target (determined at runtime), hence
        # pylint gets confused
        self.console.send_expect_sync(console, detect_context)
        self.console.write(data, console)	# pylint: disable = no-member

    def send(self, data, crlf = None, crlf_replace = "\n",
             console = None, detect_context = ""):
        """Like :py:meth:`console_tx`, transmits the string of data
        over the given console.

        This function, however, differs in takes only strings and that
        it will append a CRLF sequence at the end of the given
        string. As well, it will *flush* the receive pipe so that next
        time we :meth:`expect` something, it will be only for anything
        received after we called this function (so we'll expect to see
        even the sending of the command).

        Arguments are the same as :meth:`console_tx` plus:

        :param str crlf: (optional) CRLF end-of-line convention to
          use:

          - *None*: [**default**] use whatever is in or
            :attr:`target.console.crlf
            <tcfl.target_ext_console.extension.crlf>` for the current
            console; if it all resolves to *None*, no replacement will
            be done.

          - ``""``: (empty string) do not append anything

          - ``\\r``: use carriage return

          - ``\\r\\n``: use carriage return and line feed

          - ``\\n``: use line feed

          - ``ANYSTRING``: append *ANYSTRING*

        :param str crlf_replace: (optional; default *\\n*)
          string/character that denotes and end of line convention to
          replace with *crlf*

        """
        assert isinstance(data, (str, bytes))

        if console == None:
            console = self.console.default
        if crlf == None:
            # note that target_ext_console.extension.__init__ might
            # have initialized this from server info
            crlf = self.console.crlf.get(console, None)

        if isinstance(data, bytes):
            # data has to be astring to be JSON encoded anyway...
            data = data.decode('utf-8')

        self.console.send_expect_sync(console, detect_context)
        if crlf:
            self.console.write(data.replace(crlf_replace, crlf) + crlf, console)
        else:
            self.console.write(data, console)

    def on_console_rx(self, regex_or_str, timeout = None, console = None,
                      result = "pass"):
        """Set up an action to perform (pass, fail, block or skip)
        when a string or regular expresison is received on a given
        console in this target.

        Note this does not wait for said string; you need to run the
        testcase's *expecter* loop with::

        >>> self.expect()

        As well, those actions will be performed when running
        :meth:`expect` or :meth:`wait` for blocking versions.

        This allows you to specify many different things you are
        waiting for from one or *more* targets and wait for all of
        them at the same time and block until all of them are received
        (or timeout).

        :param regex_or_str: string or regular expression (compiled
          with :py:func:`re.compile`.

        :param int timeout: Seconds to wait for regex_or_str to be
          received, raise :py:exc:`tcfl.tc.failed_e`
          otherwise. If *False*, no timeout check is done; if *None*,
          it is taken from the default timeout set by the testcase.

        :param str console: (optional) name of console from which to
           receive the data

        :param result: what to do when that regex_or_str is found on
          the given console:

          - *pass*, (default) raise :py:exc:`tcfl.tc.pass_e`
          - *block*, raise :py:exc:`tcfl.tc.blocked_e`
          - *error*, raise :py:exc:`tcfl.tc.error_e`,
          - *failed*, raise :py:exc:`tcfl.tc.failed_e`,
          - *blocked*, raise :py:exc:`tcfl.tc.blocked_e`

          Note that when running an expecter loop, if seven different
          actions are added indicating they are expected to pass, the
          seven of them must have raised a pass exception (or
          indicated passage somehow) before the loop will consider it
          a full pass. See :py:meth:`tcfl.tc.tc_c.expect`.

        :raises: :py:exc:`tcfl.tc.pass_e`,
          :py:exc:`tcfl.tc.blocked_e`, :py:exc:`tcfl.tc.failed_e`,
          :py:exc:`tcfl.tc.error_e`, :py:exc:`tcfl.tc.skip_e`,
          any other exception from runtimes.

        :returns: *True* if a poller for the console was added to the
          testcase's expecter loop, *False* otherwise.

        """
        self.report_info("on_console_rx is deprecated, use expect_global_append")
        if timeout == False:
            timeout = 0
        elif timeout == None:
            timeout = self.testcase.tls.expect_timeout

        if result == "pass":
            raise_on_found = pass_e
        elif result == "block":
            raise_on_found = blocked_e
        elif result == "error":
            raise_on_found = error_e
        elif result == "failed":
            raise_on_found = failed_e
        elif result == "skip":
            raise_on_found = skip_e
        else:
            raise AssertionError("unknown result %s" % result)

        if hasattr(regex_or_str, "pattern"):
            msg = "found (in console '%s') %s" % (console, regex_or_str.pattern)
        else:
            msg = "found (in console '%s') %s" % (console, regex_or_str)

        self.testcase.expect_global_append(
            self.console.text(
                regex_or_str,
                timeout = timeout,
                console = console,
                raise_on_found = raise_on_found(msg, dict(target = self))
            )
        )

    def wait(self, regex_or_str, timeout = None, console = None):
        """
        Wait for a particular regex/string to be received on a given
        console of this target before a given timeout.

        See :py:meth:`expect` for a version that just raises
        exceptions when the output is not received.

        :param int timeout: Seconds to wait for regex_or_str to be
          received, raise :py:exc:`tcfl.tc.error_e`
          otherwise. If *False*, no timeout check is done; if *None*,
          it is taken from the default timeout set by the testcase.

        :param str console: (optional) name of console from which to
           receive the data

        :returns: *True* if the output was received before the
          timeout, *False* otherwise.
        """

        if timeout:
            assert isinstance(timeout, int)
        if console:
            assert isinstance(console, str)

        try:
            self.expect(regex_or_str, timeout, console,
                        raise_on_timeout = _timeout_e)
            return True
        except _timeout_e:
            return False

    def expect(self, regex_or_str, timeout = None, console = None,
               name = None, raise_on_timeout = failed_e,
               previous_max = 4096,
               origin = None, detect_context = "", report = None):
        """
        Wait for a particular regex/string to be received on a given
        console of this target before a given timeout.

        Similar to :py:func:`wait`, it will raise an exception if
        @regex_or_str is not received before @timeout on @console.

        :param int timeout: Seconds to wait for regex_or_str to be
          received, raise :py:exc:`tcfl.tc.error_e`
          otherwise. If *False*, no timeout check is done; if *None*,
          it is taken from the default timeout set by the testcase.

        :param str console: (optional) name of console from which to
           receive the data

        :param str origin: (optional) when reporting information about
          this expectation, what origin shall it list, eg:

          - *None* (default) to get the current caller
          - *commonl.origin_get(2)* also to get the current caller
          - *commonl.origin_get(1)* also to get the current function

          or something as:

          >>> "somefilename:43"

        :param str detect_context: (optional) the detection context is
          a string which is used to help identify how to keep track of
          where we are looking for things (detecting) in a console's
          output. Further info :ref:`here
          <console_expectation_detect_context>`

        :param str name: (optional) name for this match; defaults to a
          sanitized version of the regular expression to match and
          will be in the return value's key.

        :param int report: (optional; default all) number of bytes to
          report of console output found before the match; this allows
          to cut reports when the previous output was very large and
          gets in the way. The output still will be captured in the
          report files.

        :returns: dictionary with the match information:

          >>> target.send("hello")
          >>> target.expect("hello")
          >>> {
          >>>     'hello': {
          >>>         'console': u'serial0',
          >>>         'console output': <commonl.generator_factory_c object at 0x7fc899511f10>,
          >>>         'groupdict': {},
          >>>         'offset': 0,
          >>>         'offset_match_end': 710,
          >>>         'offset_match_start': 709,
          >>>         'origin': 'FILENAME:LINE',
          >>>         'pattern': 'hello',
          >>>         'target': <tcfl.tc.target_c object at 0x7fc891959f90>
          >>>     }
          >>> }

          Given the regular expression to match in :meth:`expect`,
          when matched returns information about said match; see
          :meth:`tcfl.target_ext_console.expect_text_on_console_c.detect`
          for full details and field description.

        :raises: :py:exc:`tcfl.tc.blocked_e` on error,
          :py:exc:`tcfl.tc.error_e` if not received,
          any other runtime exception.
        """
        if timeout == False:
            timeout = 0
        elif timeout == None:
            timeout = self.testcase.tls.expect_timeout

        if origin == None:
            origin = commonl.origin_get(2)
        return self.testcase.expect(
            self.console.text(
                regex_or_str,
                console = console, timeout = timeout, name = name,
                raise_on_timeout = raise_on_timeout,
                previous_max = previous_max,
                detect_context = detect_context, report = report),
            origin = origin, timeout = timeout,
        )

    def stub_app_add(self, bsp, _app, app_src, app_src_options = ""):
        """\
        Add App builder information for a BSP that has to be stubbed.

        When running on a target that has multiple BSPs but some of
        then will not be used by the current :term:`BSP model`, stubs
        might have to be added to those BSPs to make sure their CPUs
        are not going wild. Use this function to specify which app
        builder is going to be used, the path to the stub source and
        build options. The App building mechanism will take it from
        there.

        An app builder might determine that a given BSP needs no stub;
        in said case it can remove it from the dict
        :py:meth:`bsps_stub` with:

        >>> del target.bsps_stub[BSPNAME]

        This is like the app information added by _target_app_add(),
        but it is stored in the target_c instance, not to the testcase
        class.

        This is because the stubbing that has to be done is specific
        to each target (as the BSPs to stub each target have might be
        different depending on the target and BSP-model).

        Note this information is only added if there is nothing
        existing about said BSP. To override, you need to delete and
        add:

        >>> del target.bsps_stub[BSPNAME]
        >>> target.stub_app_add(BSPNAME, etc etc)
        """
        assert isinstance(bsp, str)
        assert issubclass(_app, app.app_c)
        assert isinstance(app_src, str)
        assert isinstance(app_src_options, str)
        if not bsp in self.bsps_stub \
           or self.bsps_stub[bsp] == None \
           or self.bsps_stub[bsp][0] == None:
            self.bsps_stub[bsp] = (_app, app_src, app_src_options)


    @classmethod
    def create_from_cmdline_args(
            cls, args, target_name = None, iface = None,
            ifaces: list = None,     # COMPAT: list[str] for < python3.8
            extensions_only = None,
            # FIXME: add tcfl.targets.discovery_agent_c once import hell is fixed
            target_discovery_agent = None):
        """
        Create a :class:`tcfl.tc.target_c` object from command line
          arguments

        :param argparse.Namespace args: arguments from argparse
        :param str target_name: (optional) name of the target, by
          default is taken from *args.target*.

        Other parameters as of :meth:`create`.

        :returns: instance of :class:`tcfl.tc.target_c` representing
          said target, if it is available.
        """
        if target_name == None:
            if not hasattr(args, 'target'):
                raise RuntimeError("missing 'target' argument")
            target_name = getattr(args, 'target', None)
        target = cls.create(target_name,
                            iface = iface, ifaces = ifaces,
                            extensions_only = extensions_only,
                            target_discovery_agent = target_discovery_agent)
        if args.ticket:
            target.ticket = args.ticket
            target.testcase.ticket = args.ticket
        return target


    @staticmethod
    def create(target_name, iface = None,
               ifaces: list = None, # COMPAT: list[str] for < python3.8
               extensions_only = None,
               # FIXME: add tcfl.targets.discovery_agent_c once import hell is fixed
               target_discovery_agent = None):
        """Create a :class:`tcfl.tc.target_c` object for a direct test

        This can be used as:

        >>> import tcfl.targets
        >>> tcfl.targets.subsystem_setup()
        >>> tcfl.tc.target_c.create("NAME")

        :param str target_name: name of the target; this can be just
          an ID or a fullid (SERVER/ID).

        :param str iface: (optional) target must support the given
          interface, otherwise an exception is raised.

        :param list[str] ifaces: (optional) target must support the given
          interfaces, otherwise an exception is raised.

        :param list extensions_only: (optional) list of extensions to
          load; if *[]*, load no extensions, if *None* load all
          extensions available/needed; otherwise, load only the
          extensions listed by name.

        :param tcfl.targets.discovery_agent_c target_discovery_agent:
          (optional; default *None*, the global discovery agent)
          use an specifc target discovery agent:

          >>> import tcfl.servers
          >>> import tcfl.targets
          >>> tcfl.servers.subsystem_setup()
          >>> discovery_agent = discovery_agent_c(*args, projections = projections, **kwargs)
          >>> discovery_agent.update_start()
          >>> discovery_agent.update_complete(update_globals = True)
          >>> tcfl.tc.target_c.create("NAME", target_discovery_agent = discovery_agent)

        :returns: instance of :class:`tcfl.tc.target_c` representing
          said target, if it is available.

        """
        if target_discovery_agent == None:
            target_discovery_agent = tcfl.targets.discovery_agent

        rt = target_discovery_agent.rts_flat[target_name]
        rt = dict(rt)	# clone, don't modify original
        rt['rtb'] = tcfl.ttb_client.rest_target_brokers[rt['rtb']]
        target = target_c(rt, tc_global, None, "target",
                          extensions_only = extensions_only)

        if iface != None and not iface in target.rt.get('interfaces', []):
            raise RuntimeError("%s: target does not support the %s interface"
                               % (target_name, iface))
        if ifaces:
            for iface in ifaces:
                if iface != None and not iface in target.rt.get('interfaces', []):
                    raise RuntimeError("%s: target does not support the %s interface"
                                       % (target_name, iface))
        target.testcase.ticket = "TICKET"
        target.ticket = target.testcase.ticket
        target.testcase.__init_shallow__(target.testcase)
        target.testcase.__thread_init__(None)
        target._kws_update()
        return target


    @staticmethod
    def get_by_name(testcase, target_name):
        """
        Return a :class:`tcfl.tc.tc_c` instance of a target the
        testcase is running on given its remote name.

        :param tcfl.tc.tc_c testcase: testcase on which we are looking
          for the target

        :param str target_name: is the name of a remote target that
          *testcase* has allocated to execute (i.e.: is part of the
          testcase's target group).

        >>> t = tcfl.tc.target_c.get_by_name(self, "machine16")

        """
        for _role_name, target in testcase.target_group.targets.items():
            if target.id == target_name:
                return target
        raise LookupError(
            f"cannot find target '{target_name}' in the list of "
            f"targets {testcase.name}/{testcase.runid_hashid} is running on")


    def mkticket_for_call(self):
        """
        Generate a ticket useful for debugging interactions with the server

        **internal**

        This usually contains the testcase's ticket and the subcase, if any
        """
        # information that might help debugging who did what when
        s = (self.ticket or '') + (self.testcase.ident() or '')
        subcase_s = msgid_c.subcase()
        if subcase_s:
            s += "-" + subcase_s
        return s


    def ttbd_iface_call(self, interface, call, method = "PUT",
                        component = None, stream = False, raw = False,
                        files = None, timeout = 160,
                        retry_timeout = 0, retry_backoff = 0.5,
                        **kwargs):
        """
        Execute a general interface call to TTBD, the TCF remoting server

        This allows to call any interface on the server that provides
        this target. It is used to implement higher level calls.

        :param str interface: interface name (eg: "power", "console",
          etc); normally any new style interface listed in the
          target's *interfaces* tag.
        :param str call: name of the call implemented by such
          interface (eg: for power, "on", "off"); these are described
          on the interface's implementation.
        :param str method: (optional, defaults to *PUT*); HTTP method
          to use to call; one of *PUT*, *GET*, *DELETE*,
          *POST*. The interface dictates the method.
        :param str component: (optional, default *None*) for
          interfaces that implement multiple components (a common
          pattern), specify which component the call applies to.
        :param dict files: (optional) dictionary of keys pointing to
          file names that have to be streamed to the server. Keys are
          strings with names of the files, values opened file
          descriptors (or iterables). FIXME: needs more clarification
          on how this works.

        :param float retry_timeout: (optional, default 0--disabled)
          how long (in seconds) to retry connections in case of failure
          (:class:`requests.exceptions.ConnectionError`,
          :class:`requests.exceptions.ReadTimeout`)

          Note a retry can have side effects if the request is not
          idempotent (eg: writing to a console); retries shall only be
          enabled for GET calls. Support for non-idempotent calls has
          to be added to the protocol.

        :param float retry_backofff: (optional) how long to wait in
          between retries. This number is increased each time by 20%
          until it is at most *retry_timeout / 10*.

        Rest of the arguments are a dictionary keyed by string with
        values that will be serialized to pass the remote call
        as arguments, and thus are interface specific.

        Anything that is an interable or dictionary will be
        serialized as JSON. The rest are kept as body arguments so the
        daemon can decode it properly.
        """
        assert isinstance(interface, str)
        assert isinstance(call, str)
        assert component == None or isinstance(component, str)
        assert isinstance(stream, bool)
        assert isinstance(raw, bool)
        assert method.upper() in ( "PUT", "GET", "DELETE", "POST" ), \
            "method must be PUT|GET|DELETE|POST; got %s" % method
        assert isinstance(retry_timeout, (int, float)) and retry_timeout >= 0, \
            f"retry_timeout: {retry_timeout} has to be an int/float >= 0"
        assert isinstance(retry_backoff, (int, float)) and retry_backoff > 0
        if retry_timeout > 0:
            assert retry_backoff < retry_timeout, \
                f"retry_backoff {retry_backoff} has to be" \
                f" smaller than retry_timeout {retry_timeout}"
        if self.rtb.server_json_capable:
            all_json = True
        else:
            all_json = False

        for k, v in kwargs.items():
            if all_json:
                # We need None  passed verbatim so it is not really
                # passed, so we don't encode it as JSON here--this needs
                # some cleanup, is quite confusing, to be fair.
                if v == None: # or isinstance(v, str):
                    continue
                kwargs[k] = json.dumps(v)
            else:
                if isinstance(v, str):
                    continue
                if isinstance(v, (collections.abc.Sequence, collections.abc.Mapping)):
                    kwargs[k] = json.dumps(v)

        if component:
            if all_json == True:
                kwargs['component'] = json.dumps(component)
            else:
                kwargs['component'] = component

        kwargs['ticket'] = self.mkticket_for_call()

        # This looks really complex, but it is just the
        # rtb.send_request() below call
        #
        # the rest is just a retry loop that keeps trying if
        # retry_timeout > 0 until the timeout expires, backing off an
        # incremental amount of time.
        retry_count = -1
        retry_ts = None
        while True:
            retry_count += 1
            try:
                return self.rtb.send_request(
                    method,
                    f"targets/{self.id}/{interface}/{call}",
                    stream = stream, raw = raw, files = files,
                    timeout = timeout, timeout_extra = None,
                    data = kwargs if kwargs else None)

            except (
                    requests.exceptions.ConnectionError,
                    requests.exceptions.ReadTimeout,
            ) as e:
                # Failure, shall we retry?
                # Report how many retries we have done, wait a backoff
                # and then loop it.
                if retry_timeout == 0:
                    raise
                name = f"HTTP {method} {interface}/{call}"
                testcase = self.testcase
                ts = time.time()
                if retry_ts == None:
                    retry_ts = ts	# first one
                else:
                    if ts - retry_ts > retry_timeout:
                        raise RuntimeError(
                            f"{name}: giving up after {retry_timeout}s"
                            f" retrying {retry_count} connection errors") from e
                with testcase.lock:
                    testcase.buffers.setdefault(name, 0)
                    testcase.buffers[name] += 1
                    count = testcase.buffers[name]
                self.report_data("Warnings [%(type)s]", name, count)
                self.testcase.log.warning(
                    f"{name}: retrying for {retry_timeout - (ts - retry_ts):.0f}s"
                    f" more after connection error {type(e)}: {e}")
                time.sleep(retry_backoff)
                # increase the backoff to avoid pestering too much,
                # but make sure it doesn't get too big so we at least
                # get 10 tries in the timeout period
                if retry_backoff < retry_timeout / 10:
                    retry_backoff *= 1.2
                continue

            except requests.RequestException as e:
                raise error_e("%s: %s/%s: remote call failed: %s"
                              % (self.id, interface, call, e),
                              dict(
                                  # FIXME: we need to stop doing this,
                                  # since it makes the exceptions
                                  # unpickable; replace with a
                                  # targetid, it's all we usually need
                                  target = self,
                                  targetid = self.fullid,
                                  server = str(self.rtb),
                                  error = str(e)
                              )) from e


    def instrument_name_get(self, interface_name, component = None):
        """
        Given a component in an interface, return the name of the
        instrument that implements it

        In the inventory metadata, every interface exposes components
        and each component declares the unique instrument ID of the HW
        piece that implements it in
        *interfaces.INTERFACENAME.COMPONENT.INSTRUMENTID*.

        This function then goes to the instrumentation inventory in
        *instrumentation.INSTRUMENTID* and returns the value of thef
        first one found:

         - *instrumentation.INSTRUMENTID.name_long*

         - *instrumentation.INSTRUMENTID.name*

        :returns str:* name of the instrument that implements the
          function or *instrument:INSTRUMENTID* if not found.

        """
        if component:
            instrument_hash = self.kws.get(
                'interfaces.' + interface_name + '.' + component + '.instrument',
                None)
        else:
            instrument_hash = self.kws.get(
                'interfaces.' + interface_name + '.instrument',
                None)
        if instrument_hash == None:
            self.report_info(
                "WARNING! cannot find instrument name for instrument %s" % (
                    instrument_hash),
                dict(target = self, interface_name = interface_name,
                     component = component))
            instrument = "instrument:" + instrument_hash
        else:
            # get the long name (more human friendly, otherwise the short one)
            instrument = self.kws.get(
                "instrumentation." + instrument_hash + ".name_long",
                self.kws.get(
                    "instrumentation." + instrument_hash + ".name",
                None))
        return instrument


    def instrument_data_get(self, iface_name, component = None, do_raise = True):
        """
        Return the name of data for an instrument associated to a component

        :param tcfl.tc.target_c: target from which to get the instrument data

        :param str iface_name: name of the interface where the component
          is located

        :param str component: name of the component

        :param do_raise: (optional, default *True*) if not found, raise an
          exception

        :returns tuple(str,dict): if no instrument found
        """
        if component:
            entry = self.rt['interfaces'][iface_name].get(component, None)
        else:
            entry = self.rt['interfaces'][iface_name]
            instrument_hash = self.kws.get(
                f'interfaces.{iface_name}.instrument',
                None)
        if not isinstance(entry, dict):
            # some interfaces drop entries in the top level that are
            # not components and can be skipped
            return None, None
        instrument_hash = entry.get('instrument', None)

        if instrument_hash == None:
            if do_raise:
                raise tcfl.tc.error_e(
                    f"{iface_name}/{component}: declares no instrument;"
                    f" configuration bug?",
                    dict(entry = entry))
            else:
                self.report_fail(
                    f"{iface_name}/{component}: declares no instrument;"
                    f" configuration bug?")
                return None, None
            return None, None

        instrument_data = self.rt['instrumentation'].get(instrument_hash, None)
        if instrument_data == None:
            if do_raise:
                raise tcfl.tc.error_e(
                    f"{iface_name}/{component}: missing instrument data '{instrument_hash}';"
                    f" configuration bug?",
                    dict(entry = entry))
            else:
                self.report_fail(
                    f"{iface_name}/{component}: missing instrument data '{instrument_hash}';"
                    f" configuration bug?")
                return None, None
        return instrument_hash, instrument_data



    #
    # Private API
    #

    __extensions = {}



    def _kws_update(self, bsp = None):
        self.kws.clear()
        # FIXME: all these updates should be checking for overwrites
        self.kws.update(self.testcase.kws)
        self.kws_origin.update(self.testcase.kws_origin)
        self.kws.update(self._kws)
        self.kws_origin.update(self._kws_origin)
        # we publish both the flattened rt and the nested one
        commonl.kws_update_from_rt(self.kws, self.rt)
        for key, val in commonl.dict_to_flat(self.rt,
                                             sort = False, empty_dict = True):
            if key == 'rtb':
                # ok, this is truly a hack -- but we will get ONE value
                # from target.rt that is the daemon representation, and we
                # just want its url, which str() does.
                val = str(val)
            self.kws[key] = val

        kws_bsp = dict()
        if 'bsps' in list(self.rt.keys()) and bsp and bsp in self.rt['bsps']:
            commonl.kws_update_type_string(kws_bsp, self.rt['bsps'][bsp])
            kws_bsp.update(self._kws_bsp.get(bsp, {}))
            kws_bsp['bsp'] = bsp
        self.kws.update(kws_bsp)


    def _kw_set(self, kw, value, origin = None):
        """
        Set a string keyword for later substitution in commands

        Note these are the target specific keywords (see doc for
        :py:data:`_kws`).

        :param str kw: keyword name
        :param (str, int, float, bool) value: value for the keyword
        :param str origin: origin of this setting; if none, it will be
          taken from the stack

        """
        assert isinstance(kw, str)
        if isinstance(value, ttb_client.rest_target_broker):
            # ok, this is truly a hack -- but we will get ONE value
            # from target.rt that is the daemon representation, and we
            # just want its url, which str() does.
            value = str(value)
        assert value in ({}, None) \
            or isinstance(value, (str, int, float, bool)), \
            "keyword %s: value type %s not allowed (str, int, float, bool): %s" % (
                kw, type(value), value)
        if origin == None:
            o = inspect.stack()[1]
            origin = "%s:%s" % (o[1], o[2])
        else:
            assert isinstance(origin, str)
        self._kws[kw] = value
        self._kws_origin.setdefault(kw, []).append(origin)

    @staticmethod
    def _report_argcheck(message, attachments, level, dlevel, alevel, ulevel):
        assert isinstance(message, str)
        if attachments:
            assert isinstance(attachments, dict)
        assert isinstance(level, int)
        assert isinstance(dlevel, int)
        assert isinstance(alevel, int)
        assert isinstance(ulevel, int)

    # Semi private API, linkage with tc_c API
    def bsp_model_suffix(self):
        if self.bsp_model:
            return ":%s" % self.bsp_model	# No BSP selected
        else:
            return ""

    def bsp_suffix(self):
        if self.bsp_model and self.bsp:
            if self.bsp_model == self.bsp:	# BSP-Model w/ one BSP
                return ""
            else:				# BSP-Model w/ many BSPs
                return "/%s" % self.bsp
        else:
            return ""

    def _report_mk_prefix(self):
        """
        Update the prefix we use for the logging/reports when some
        parameter changes.
        """
        # FIXME: this needs to be revamped to consider as inputs only: tcname hash_salt target-group AXES
        # Some considerations:
        # - if the target group we are running the testcase on has a
        #   single target, then we don't add anything else, as the target
        #   group will name itself as the lone target--we justneed to add
        #   BSP-model/BSP information as pertinent
        # - if the target group has more than one target, then we add our
        #   name after a | symbol
        tc = self.testcase
        tg = tc.target_group
        if tg:
            if tg.len() == 1:
                # If there is only one target in the group, the name is
                # already enough
                tgname = ""
            else:
                tgname = "|" + self.fullid
        else:
            tgname = ""
        self._report_prefix = self.testcase.report_mk_prefix() \
            + tgname + self.bsp_suffix()

    def report_mk_prefix(self):
        return self._report_prefix



    # Internal target_c API

    def _app_get_for_bsp(self, what = None, bsp = None, noraise = False):
        """
        Return which app driver and source has to be used for
        the given what action in the current BSP.
        """
        if what == None:
            what = ""
        else:
            what = what + ": "
        if bsp == None:
            assert self.bsp
            bsp = self.bsp
        target_want = self.testcase._targets[self.want_name]
        twapp = target_want['app']
        if not twapp:
            if noraise:
                return (None, None)
            raise blocked_e("%sNo App builders defined for "
                                    "target '%s', can't figure out "
                                    "what to do" % (what, self.want_name))
        elif bsp in list(twapp.keys()):
            _app = twapp[bsp][0]
            app_src = twapp[bsp][1]
        elif "*" in list(twapp.keys()):
            _app = twapp["*"][0]
            app_src = twapp["*"][1]
        else:
            raise blocked_e("%sCan't figure out which "
                                    "app to use" % what)
        return _app, app_src


class target_group_c(object):
    """\
    A unique group of targets (each set to an specific :term:`BSP
    model`) assigned to to a testcase for execution.

    A testcase can query a :class:`tcfl.tc.target_c` instance of the
    remote target to manipualte it by declaring it as an argument to a
    testcase method, querying the :attr:`targets` dictionary or
    calling :meth:`target`:

    >>> @tcfl.tc.target(name = "mytarget")
    >>> class mytest(tcfl.tc.tc_c):
    >>>     ...
    >>>
    >>>     def eval_1(self, mytarget):
    >>>         mytarget.power.cycle()
    >>>
    >>>     def eval_2(self):
    >>>         mytarget = self.target_group.target("mytarget")
    >>>         mytarget.power.cycle()
    >>>
    >>>     def eval_3(self):
    >>>         mytarget = self.targets["mytarget"]
    >>>         mytarget.power.cycle()
    >>>
    """
    def __init__(self, descr):
        assert isinstance(descr, str)
        # This is needed so that things can be initialized in the same
        # order (by default) as declared in the testcase
        self._targets = collections.OrderedDict()
        self.descr = descr
        self._name = "FIXME0"

    @property
    def name(self):
        return self._name

    def name_set(self, tgid):
        self._name = tgid

    def len(self):
        """Return number of targets in the group"""
        return len(self._targets)

    def target(self, target_name):
        """
        Return the instance of :class:`tcfl.tc.target_c` that represents a
        remote target that met the specification requested

        :func:`tcfl.tc.target` decorator with name *target_name*

        """
        return self._targets[target_name]

    def target_add(self, target_name, _target):
        assert isinstance(target_name, str)
        assert isinstance(_target, target_c)
        self._targets[target_name] = _target

    @property
    def targets(self):
        """
        Dictionary of :class:`tcfl.tc.target_c` descriptor for a remote target
        keyed by the name they were requested with the
        :func:`tcfl.tc.target` decorator.
        """
        return self._targets

    @targets.setter
    def targets(self, targets):
        assert isinstance(targets, dict)
        self._targets = targets
        return targets

assign_period = 5
poll_period = 0.25

result_c = tcfl.result_c


class tc_logadapter_c(logging.LoggerAdapter):
    """
    Logging adapter to prefix test case's current :term:`BSP model`,
    bsp and target name.
    """
    id = 0
    prefix = ""
    def process(self, msg, kwargs):
        return '[%08x] %s: %s ' % (self.id, self.prefix, msg), kwargs

    def isEnabledFor(self, level):
        return True


#
# Testcase Driver API
#


def tags(*args, **kwargs):
    """
    Add tags to a testcase

    :param str args: every `arg` is converted into a boolean tag
      (present, thus True)
    :param dict kwargs: every argument `name = value` is added as tag
      `name` with value `value`.
    """
    origin = commonl.origin_get(2)
    # It is fine to pass no tags at all; this allows us to
    # calculate them dynamically (for example seeing if some env
    # vars are available and filing a skip if not available) but
    # then leaving it empty as needed.
    def decorate_class(cls):
        """
        Wrap function
        """
        assert isinstance(cls, type)
        assert issubclass(cls, tc_c)
        # Ugly way of doing it; we want to build upon the tags of the
        # base class -- but not modify them; so when we add, we COPY the
        # _tags dictionary from our base class to modify it specific
        # to this class
        origin = commonl.origin_get(2)
        if id(super(cls, cls)._tags) == id(cls._tags):
            cls._tags = dict(super(cls, cls)._tags)
        for name in args:
            cls._tags[name] = (True, origin)
        if kwargs:
            for key, val in kwargs.items():
                cls._tags[key] = (val, origin)
        return cls
    return decorate_class


def serially():
    """
    Force a testcase method to run serially (vs :func:`concurrently`).

    Remember methods that are ran serially are run first and by
    default are those that

    - take more than one target as arguments

    - are evaluation methods
    """
    def decorate_fn(fn):
        setattr(fn, "execution_mode", 'serial')
        return fn
    return decorate_fn

def concurrently():
    """
    Force a testcase method to run concurrently after all the serial
    methods (vs decorator :func:`serially`).

    Remember methods that are ran concurrently are run after the
    serial methods and by default those that:

    - are not evaluation methods

    - take only one target as argument (if you force two methods that
      share a target to run in parallel, it is your responsiblity to
      ensure proper synchronization
    """
    def decorate_fn(fn):
        setattr(fn, "execution_mode", 'parallel')
        return fn
    return decorate_fn

def subcase(subcase = None, break_on_non_pass = False):
    """
    Decorate a testcase method as a subcase

    A subcase will be reported as a separate testcase, under the
    current testcase.

    >>> class _test(tcfl.tc.tc_c):
    >>>     ...
    >>>     def eval_20(self):
    >>>         # test something
    >>>
    >>>     @tcfl.tc.subcase("subcase1")
    >>>     def eval_30(self):
    >>>         # test subcase 1
    >>>

    This will yield two testcases, *_test* and *_test##subcase1*, but
    it is all a reporting trick, the execution happens the same, they
    are just reported to a different name.

    :param str subcase: (optional) name of the subcase; defaults to
      the name of the method with the phase name removed (eg,
      *eval_20_subcase1* becomes *20_subcase1*).

    :param bool break_on_non_pass: (optional; default *False*) stop all
      execution if anything inside the subcase fails, otherwise,
      continue execution to the next method.
    """
    assert subcase == None or isinstance(subcase, str), \
        f"subcase: expected string; got {type(subcase)}"
    assert isinstance(break_on_non_pass, bool), \
        f"break_on_non_pass: expected bool; got {type(break_on_non_pass)}"

    def wrapper(method):
        assert callable(method), \
            "tcfl.subcase can only decorate methods of classes" \
            f" derived from tcfl.tc._c; got {type(method)}"
        if subcase == None:
            method_name = method.__name__
            for prefix in _method_phase_prefixes:
                prefix += "_"
                if method_name != prefix and method_name.startswith(prefix):
                    method_name = method_name[len(prefix):]
            _subcase = method_name
        else:
            _subcase = subcase
        # this signals the execution system to keep going if someting
        # fails, as we consider it an isolated subcase that shan't
        # interrupt the flow
        setattr(method, "break_on_non_pass", break_on_non_pass)
        @functools.wraps(method)
        def wrapped(*args, **kwargs):
            with msgid_c(subcase = _subcase, depth_relative = 0):
                testcase = args[0] # The `self`  argument to the test case
                r = result_c.call_fn_handle_exception(method, *args, **kwargs)
                # ensure we register the result of this testcase,
                # otherwise we'll miss reporting about it
                subtc = testcase._subcase_get(_subcase)
                subtc.result = result_c.from_retval(r)
                return r
        return wrapped
    return wrapper


def _target_app_setup(obj, cls_name, target_want_name):
    """
    Setup the phase hooks so that the App builder is called in the
    different phases of testcase execution.
    """
    if hasattr(obj, "configure_for_%s" % target_want_name):
        # already done
        return
    # Note we don't add eval* or test* hooks here, because we expect
    # the creator of testcases to do that
    for fnname in [
            'configure',
            'build',
            'setup',
            'start',
            'deploy',
            'teardown',
            'clean']:
        method_name = "%s_50_%s" % (fnname, target_want_name)
        dest_method_name = "__%s_%s_50_for_%s" % (cls_name, fnname,
                                                  target_want_name)
        # Do not override existing methods, so the user can override
        # defaults given by app_X.
        if hasattr(obj, method_name):
            method_name = "overriden_" + method_name
        # Monkey Patching for the win; this is basically the dirty way
        # of adding a method to an existing class
        object_code = compile(
            "def __%s_%s_50_for_%s(tc, %s):\n"
            "    return tc._%s_50_for_target(%s)\n"
            % (cls_name, fnname, target_want_name,
               target_want_name, fnname, target_want_name),
            commonl.origin_get(2) + "|" + commonl.origin_get(), 'exec')
        eval(object_code)
        if type(obj) == type:
            # Bind to the class, no need to bind to an instance, will
            # self itself in __method_trampoline_call()
            # Note how we cast with the instance set to None, so in
            # __method_trampoline_call() we can tell we need to bind it.
            value = eval(dest_method_name)
        else:
            # Bind to an object, force not binding to an instance,
            # setting it to None, so in __method_trampoline_call() we
            # can tell we need to bind it.
            if isinstance(obj, type):
                # We are binding to a class
                value = types.MethodType(eval(dest_method_name), None, obj)
            else:
                # We are binding to a object instance
                value = types.MethodType(eval(dest_method_name), None,
                                         type(obj))
        setattr(obj, method_name, value)

def _target_app_add(obj, target_want_name, app_name, app_src):
    """
    """
    # note there can be only "*" is it is the ONLY one
    app_src = app.args_app_src_check(app_name, app_src)
    if app.driver_valid(app_name):
        twapp = obj._targets[target_want_name]['app']
        _app = app._driver_get(app_name)
        app_src = app._app_src_validate(app_name, app_src)
        if isinstance(app_src, dict):
            for bsp, src in app_src.items():
                twapp[bsp] = (_app, src)
        else:
            twapp["*"] = (_app, app_src)
        # Consistency check
        if "*" in twapp and len(twapp) > 1:
            raise blocked_e(
                "%s: Cannot have a wildcard BSP '*' and other "
                "BSPs specified in the app_* sections @%s"
                % (target_want_name, commonl.origin_get_object(app_name)))
    elif app_name.endswith("_options") and app.driver_valid(app_name[:-8]):
    # Note 8 is the lenght of "_options"
        pass
    else:
        raise blocked_e("%s: unknown App builder '%s' (or option to "
                        "existing) @%s"
                        % (target_want_name, app_name, commonl.origin_get(3)))

def _target_want_decorate_class(obj, cls_name,
                                name, type_name, short_type_name,
                                next_index, **kwargs):
    """
    Wrap function
    """
    # next_index: index number to use if we have to generate a default
    # name; passed so we can use different indexes for ICs or for
    # Targets

    # FIXME: the order makes it counterintuitive, because we are # defining:
    #
    # @tcfl.tc.interconnect()
    # @tcfl.tc.interconnect()
    # @tcfl.tc.target()
    # @tcfl.tc.target()
    # @tcfl.tc.target()
    #
    # Would be listed as ic0 ic target2 target1 target. Change so that
    # all the decorator does is write the list of wants and keywords
    # and we'll have init sequence them in the right order, so the
    # (automatic) naming  is more intuitive and results in:
    #
    # ic0 ic1 target target1 target2
    #
    # Will need to remove hack `FIXME: reversed for decorator workaround`_

    if name != None:
        assert isinstance(name, str)
        if '%d' in name:
            name = name % next_index
        if name in obj._targets:
            raise blocked_e("%s name '%s' @%s already defined, "
                            "choose another"
                            % (type_name, name, commonl.origin_get(2)))
        target_want_name = name
    else:
        if next_index == 0:
            target_want_name = short_type_name
        else:
            target_want_name = short_type_name + "%d" % next_index
    origin = commonl.origin_get(3)
    obj._targets[target_want_name] = dict(
        app = {},
        kws = kwargs,
        origin = origin)
    return target_want_name

def _target_want_add_check_key(obj, cls_name, target_want_name,
                               key, val):
    valid = False
    if key.startswith('app_'):
        _target_app_setup(obj, cls_name, target_want_name)
        _target_app_add(obj, target_want_name, key, val)
        valid = True
    elif key == "mode":
        valid_modes = [ 'one-per-type', 'any', 'all' ]
        if val not in valid_modes:
            raise blocked_e(
                "%s: unknown value (accepts %s"
                % (key, ", ".join(valid_modes)))
        valid = True
    return valid

def _target_want_add(obj, cls_name, name, spec, origin, count = 1, **kwargs):
    if count > 1 and name != None:
        # later a default will be given
        name = name + "%d"
    for _cnt in range(count):
        target_want_name = _target_want_decorate_class(
            obj, cls_name, name, "target", "target",
            obj._target_count, **kwargs)
        obj._targets[target_want_name]['spec'] = spec
        obj._targets[target_want_name]['origin'] = origin
        for key, val in kwargs.items():
            valid = _target_want_add_check_key(
                obj, cls_name, target_want_name, key, val)
            if not valid:
                raise blocked_e("%s: unknown key @%s" % (key, origin))
        obj._target_count += 1

def target_want_add(_tc, target_want_name, spec, origin,
                    count = 1, **kwargs):
    """\
    Add a requirement for a target to a testcase instance

    Given a testcase instance, add a requirement for it to need a
    target, filtered with the given specification (*spec*, which
    defaults to any), a name and optional arguments in the form of
    keywords.

    This is equivalent to the :func:`tcfl.tc.target` decorator, which
    adds the requirement to the class, not to the instance. Please
    refer to it for the arguments.
    """
    assert isinstance(_tc, tc_c)
    cls = type(_tc)
    if id(_tc._targets) == id(cls._targets):
        # this means that this testcase instance does not have a list
        # of targets specific to it. Because now we are adding one, we
        # make a private copy of all that information *for* the
        # testcase, separated from the type
        _tc._targets = copy.deepcopy(cls._targets)
        _tc._target_count = cls._target_count
        _tc._interconnects = copy.deepcopy(cls._interconnects)
    _target_want_add(_tc, cls.__name__, target_want_name,
                     spec, origin, count = count, **kwargs)

def target(spec = None, name = None, count = 1, **kwargs):
    """\
    Add a requirement for a target to a testcase instance

    For each target this testcase will need, a :ref:`filtering
    specification <target_specification>` can be given (*spec*), a
    name (or it will default to *targetN* except for the first one,
    which is just *target*) and optional arguments in the form of
    keywords.

    Of those optional arguments, the most important are the app_*
    arguments. An app_* argument is supplying a source path for an
    application that has to be (maybe) configured, built and deployed
    to the target. The decorator will add phase methods to the
    testcase to configure, build and deploy the application. Now,
    depending on the application drivers installed, the application
    can be built or not. FIXME make this explanation better.

    :param str spec: :ref:`specification <target_specification>` to
      filter against the tags the remote target exposes.

    :param str name: name for the target (must not exist already). If
      none, first declared target is called *target*, the next
      *target1*, then *target2* and so on.

    :param int count: (optional, default 1) number of targets to
      allocate; if more than one, targets will be called *target*,
      *target1*, *target2*, following the count of targets assigned to
      the testcase.

      Note in case of doubt, these can be discovered with (eg for an
      example of an interconnect with one server and two clients):

      >>> import tcfl.tc
      >>> @tcfl.tc.interconnect()
      >>> @tcfl.tc.target(name = "server")
      >>> @tcfl.tc.target(name = "client", count = 2)
      >>> @tcfl.tc.tags(build_only = True)
      >>> class _test(tcfl.tc.tc_c):
      >>>     def configure(self):
      >>>         self.report_info("target names: "
      >>>                          + " ".join(self.target_group.targets.keys()),
      >>>                          level = 0)

    :param dict kwargs: extra keyword arguments are allowed, which
      might be used in different ways that are still TBD. Main ones
      recognized:

       - *app_NAME = dict(BSP1: PATH1, BSP1: PATH2)*: specify a list
         of paths to apps that shall be built and deployed to the
         given BSPs by App builder *app_NAME*; App builders exist for
         Zephyr, Arduino Sketch and other setups, so you don't
         manually have to build your apps. You can create your own
         too.

         When a board is being run in a multiple BSP mode, each BSP
         has to be added to an App builder if using the App builder
         support, otherwise is an error condition.

       - *app_NAME = PATH*: the same, but one for when one BSP is
         used; it applies to `any` BSP in a single :term:`BSP model`
         target.

         FIXME: add link to app builders

       - *app_NAME_options = STRING*: Extra options to pass to the APP
          builder FIXME: support also BSP_options?

       .. _tcf_target_modes:

       - *mode*: how to consider this target at the time of generating
          multiple permutations of targets to run a testcase:

          - *any*: run the testcase on any target that can
            be found to match the specification

          - *one-per-type*: run the testcase on one target of each
            type that meets the specification (so if five targets
            match the specification but they are all of the same type,
            only one will run it; however, if there are two different
            types in the set of five, one of each type will run it)

          - *all*: run on every single target that matches the
            specification

         Specially on testcases that require multiple targets, there
         can be a huge number of permutations on how to run the
         testcase to ensure maximum coverage of different combinations
         of targets; some experimentation is needed to decide how to
         tell TCF to run the testcase and balance how many resources
         are used.

    """

    def decorate_class(cls):
        # This uses quite a dirty hack to add a method. I'd
        # like to know a better way to do it.
        #
        # The constraint is that we need to add the method with
        # SPECIFIC argument names so that later on, the method
        # dispatcher can call it with the right target instance.
        #
        # So we literally compile a chunk of python code that defines
        # a function __CLASSNAME_PHASE_for_TARGETNAME(TARGETNAME) that
        # than can behave as a method, as it takes a testcase for
        # first argument (as if it were `self`). This it calls
        # tc_c._PHASE_50_for_target() to do the actual work. We use
        # setattr() to inject it as a method of the class, calling it
        # PHASE_for_TARGETNAME(TARGETNAME).
        #
        # The method dispatcher in tc_c.methods_call() will call
        # PHASE_for_TARGETNAME(TARGETNAME) with TARGETNAME being an
        # instance to the right target_c for the testcase from
        # tc_c._targets[]
        #
        assert issubclass(cls, tc_c), \
            "cls has to be a subclass of tcfl.tc.tc_c"
        # Ugly way of doing it; we want to build upon the tags of the
        # base class -- but not modify them; so when we add, we COPY the
        # _targets dictionary from our base class to modify it specific
        # to this class
        # FIXME: replace target_want_name with targettarget_want_name
        if id(super(cls, cls)._targets) == id(cls._targets):
            cls._targets = collections.OrderedDict(super(cls, cls)._targets)

        _target_want_add(cls, cls.__name__,
                         name, spec, commonl.origin_get(2),
                         count = count, **kwargs)
        return cls

    return decorate_class


def _interconnect_want_add(obj, cls_name, name, spec, origin, count = 1, **kwargs):
    if count > 1 and name != None:
        # later a default will be given
        name = name + "%d"
    for _cnt in range(count):
        ic_want_name = _target_want_decorate_class(
            obj, cls_name,
            name, "interconnect", "ic", obj._ic_count, **kwargs)
        obj._targets[ic_want_name]['spec'] = spec
        obj._targets[ic_want_name]['origin'] = origin
        obj._interconnects.add(ic_want_name)
        for key, val in kwargs.items():
            valid = _target_want_add_check_key(
                obj, cls_name, ic_want_name,
                key, val)
            if not valid:
                raise blocked_e("%s: unknown key @%s" % (key, origin))
        # By default, interconnects are explored all
        obj._targets[ic_want_name]['kws'].setdefault('unlimited', True)
        obj._ic_count += 1


def interconnect(spec = None, name = None, count = 1, **kwargs):
    """\
    Add a requirement for an interconnect to a testcase instance

    An interconect is a target that binds two or more targets
    together, maybe provides interconnectivity services (networking or
    any other); we declare it's need just like any other
    target--however, we add the name to an special list so it is
    easier to handle later.

    The arguments are the same as to :func:`tcfl.tc.target`.
    """

    def decorate_class(cls):
        # This uses quite a dirty hack to add a method. I'd
        # like to know a better way to do it.
        #
        # The constraint is that we need to add the method with
        # SPECIFIC argument names so that later on, the method
        # dispatcher can call it with the right target instance.
        #
        # So we literally compile a chunk of python code that defines
        # a function __CLASSNAME_PHASE_for_TARGETNAME(TARGETNAME) that
        # than can behave as a method, as it takes a testcase for
        # first argument (as if it were `self`). This it calls
        # tc_c._PHASE_50_for_target() to do the actual work. We use
        # setattr() to inject it as a method of the class, calling it
        # PHASE_for_TARGETNAME(TARGETNAME).
        #
        # The method dispatcher in tc_c.methods_call() will call
        # PHASE_for_TARGETNAME(TARGETNAME) with TARGETNAME being an
        # instance to the right target_c for the testcase from
        # tc_c._targets[]
        #
        assert issubclass(cls, tc_c), \
            "cls has to be a subclass of tcfl.tc.tc_c"
        # Ugly way of doing it; we want to build upon the tags of the
        # base class -- but not modify them; so when we add, we COPY the
        # _targets dictionary from our base class to modify it specific
        # to this class
        # FIXME: replace target_want_name with targettarget_want_name
        if id(super(cls, cls)._interconnects) == id(cls._interconnects):
            cls._interconnects = copy.deepcopy(super(cls, cls)._interconnects)

        _interconnect_want_add(cls, cls.__name__,
                               name, spec, commonl.origin_get(2),
                               count = count,
                               **kwargs)
        return cls

    return decorate_class


def interconnect_want_add(_tc, ic_want_name, spec, origin,
                          count = 1, **kwargs):
    """\
    Add a requirement for an interconnect to a testcase instance

    Given a testcase instance, add a requirement for it to need an
    interconnect, filtered with the given specification (*spec*, which
    defaults to any), a name and optional arguments in the form of
    keywords.

    This is equivalent to the :func:`tcfl.tc.interconnect` decorator, which
    adds the requirement to the class, not to the instance. Please
    refer to it for the arguments.
    """
    assert isinstance(_tc, tc_c)
    cls = type(_tc)
    if id(_tc._interconnects) == id(cls._interconnects):
        # this means that this testcase instance does not have a list
        # of targets specific to it. Because now we are adding one, we
        # make a private copy of all that information *for* the
        # testcase, separated from the type
        _tc._ic_count = cls._ic_count
        _tc._interconnects = copy.deepcopy(cls._interconnects)
    _interconnect_want_add(_tc, cls.__name__, ic_want_name,
                           spec, origin, count = count, **kwargs)


class expectation_c(object):
    '''Expectations are something we expect to find in the data polled
    from a source.

    An object implementing this interface can be given to
    :meth:`tcfl.tc.tc_c.expect` as something to expect, which can be,
    for example:

    - text in a serial console output
    - templates in an image capture
    - audio in an audio capture
    - network data in a network data capture
    - ...

    when what is being expected is found, :meth:`tcfl.tc.tc_c.expect`
    can return data about it (implementation specific) can be
    returned to the caller or exceptions can be raised (eg: if we see
    an error), or if not found, timeout exceptions can be raised.

    See :meth:`tcfl.tc.tc_c.expect` for more details and
    :class:`target.console.text
    <tcfl.target_ext_console.expect_text_on_console_c>` and
    :class:`target.capture.image_on_screenshot
    <tcfl.target_ext_capture.extension.image_on_screenshot>` for
    implementation examples.

    .. note:: the :meth:`poll` and :meth:`detect` methods will be
              called in a loop until all the expectations have been
              detected.

              It is recommended that internal state is only saved in
              the *buffers* and *buffers_poll* storage areas provided
              (vs storing inside the object).

    :param tcfl.tc.target_c target: target on which this expectation
      is operating.

    :param float poll_period: how often this data needs to be polled
      in seconds (default *1s*).

    :param int timeout: maximum time to wait for this expectation;
      raises an exception of type *raise_on_timeout* if the timeout is
      exceeded. If zero (default), no timeout is raised, which
      effectively treats an expectation as optional or along with
      *raise_on_found* above, to raise an exception if an expectation
      is to be associated with an error condition.

    :param tcfl.tc.exception raise_on_timeout: (optional) a *type*
      (**not an instance**) to throw when not found before the
      timeout; a subclass of :class:`tcfl.tc.exception`.

    :param tcfl.tc.exception raise_on_found: an *instance* (**not a
      type**) to throw when found; this is useful to implement errors,
      such as *if I see this image in the screen, bail out*:

      >>> self.expect("wait for boot",
      >>>             crash = target.capture.image_on_screenshot(
      >>>                 'screen', 'icon-crash.png',
      >>>                 raise_on_found = tcfl.tc.error_e("Crash found"),
      >>>                 timeout = 0),
      >>>             login_prompt = target.capture.image_on_screenshot(
      >>>                 'screen', 'canary-login-prompt.png',
      >>>                 timeout = 4),
      >>>             login_prompt_serial = target.console.text(
      >>>                 "login: ",
      >>>                 name = "login prompt",
      >>>             ),
      >>> )

      Note you need to tell it also *zero* timeout, otherwise it will
      complain if it didn't find it.

      The exception's attachments will be updated with the dictionary
      of data returned by the expectation's :meth:`detect`.

    :param str origin: (optional) when reporting information about
      this expectation, what origin shall it list, eg:

      - *None* (default) to get the current caller
      - *commonl.origin_get(2)* also to get the current caller
      - *commonl.origin_get(1)* also to get the current function

      or something as:

      >>> "somefilename:43"

    '''

    def __init__(self, target, poll_period, timeout = 0,
                 raise_on_timeout = error_e,
                 raise_on_found = None, origin = None):
        assert target == None or isinstance(target, target_c)
        assert issubclass(raise_on_timeout, exception), \
            'expected subclass of tcfl.tc.exception, got %s' \
            % type(raise_on_timeout).__name__
        assert raise_on_found == None \
            or isinstance(raise_on_found, exception), \
            'expected instance of tcfl.tc.exception, got %s' \
            % type(raise_on_found).__name__
        assert poll_period > 0
        # FIXME: if too frequent, set some warning
        assert timeout >= 0
        self.target = target
        self.poll_period = poll_period
        self.poll_name = None	# will be set by :meth:tcfl.tc.tc_c.expect()
        self.timeout = timeout
        self.raise_on_timeout = raise_on_timeout
        self.raise_on_found = raise_on_found
        # this is a default, in case it is not overriden by the class
        # that implements this interface
        self.name = commonl.mkid('%s' % id(self), 4)
        self.origin = origin

    def poll_context(self):
        """
        Return a string that uniquely identifies the polling source for
        this expectation so multiple expectations that are polling
        from the same place don't poll repeatedly.

        For example, if we are looking for multiple image templates in
        a screenshot, it does not make sense to take one screenshot
        per image. It can take one screenshot and look for the images
        in the same place.

        Thus:

        - if we are polling from target with role *target.want_name*
          from it's screen capturer called *VGA*, our context becomes:

          >>> return '%s-%s' % (self.target.want_name, "VGA")

          so it follows that for a generic expectation from a
          screenshot capturer stored in *self.capturer*:

          >>> return '%s-%s' % (self.target.want_name, self.capturer)

        - for a serial console, it would become:

          >>> return '%s-%s' % (self.target.want_name, self.console_name)

        """
        raise NotImplementedError

    def poll(self, testcase, run_name, buffers_poll):
        """
        Poll a given expectation for new data from their data source

        The expect engine will call this from
        :meth:`tcfl.tc.tc_c.expect` periodically to get data where to
        detect what we are expecting. This data could be serial
        console output, video output, screenshots, network data,
        anything.

        The implementation of this interface will store the data
        (append, replace, depending on the nature of it) in
        *buffers_poll*.

        For example, a serial console reader might read from the
        serial console and append to a file; a screenshot capturer
        might capture the screenshot and put it in a file and make the
        file name available in *buffers_poll['filename']*.

        Note that when we are working with multiple expectations, if a
        number of them share the same data source (as determined by
        :meth:`poll_context`), only one poll per each will be done and
        they will be expected to share the polled data stored in
        *buffers_poll*.

        :param tcfl.tc.tc_c testcase: testcase for which we are
          polling.

        :param str run_name: name of this run of
          :meth:`tcfl.tc.tc_c.expect`--they are always different.

        :param dict buffers_poll: dictionary where we can store state
          for this poll so it can be shared between calls. Detection
          methods that use the same poling source (as given by
          :meth:`poll_context`) will all be given the same storage
          space.

        Called with a lock held for exclusive access to *buffers_poll*;
        *buffers* is thread specific.
        """
        raise NotImplementedError

    def detect(self, testcase, run_name, buffers_poll, buffers):
        """
        Look for what is being expected in the polled data

        After the :meth:`tcfl.tc.tc_c.expect` has polled data (with
        :meth:`poll` above) and stored it in *buffers_poll*, this
        function is called to detect what we are expecting in that
        data.

        Note the form of the data is completely specific to this
        expectation object. It can be data saved into the
        *buffers_poll* dictionary or that can be referring to a file
        in the filesystem. See FIXME examples.

        For example, a serial console detector might take the data
        polled by :meth:`poll`, load it and look for a string in there.

        :param tcfl.tc.tc_c testcase: testcase for which we are
          detecting.

        :param str run_name: name of this run of
          :meth:`tcfl.tc.tc_c.expect`--they are always different.

        :param dict buffers_poll: dictionary where the polled data has
          is available. Note Detection methods that use the same
          poling source (as given by :meth:`poll_context`) will all be
          given the same storage space. as per :meth:`poll` above.

        :param dict buffers: dictionary available exclusively to this
          expectation object to keep data from run to run.

        :returns: information about the detection; if *None*, this
          means the detection process didn't find what is being looked
          for and the detection process will continue.

          If not *None* is returned, whatever it is, it is considered
          what was expected has been found.

          :meth:`tcfl.tc.tc_c.expect` will save this in a dictionary
          of results specific to each expectation object that
          will be returned to the user as the return value of
          :meth:`tcfl.tc.tc_c.expect`.

          As well, if a *raise_on_found* exception was given, these
          fields are added to the attachments.

        Called with a lock held for exclusive access to *buffers_poll*;
        *buffers* is thread specific.
        """
        raise NotImplementedError


    def flush(self, testcase, run_name, buffers_poll, buffers, results):
        """
        Generate collateral for this expectation

        This is called by :meth:`tcfl.tc.tc_c.expect` when all the
        expectations are completed and can be used to for example, add
        marks to an image indicating where a template or icon was
        detected.

        Note different expectations might be creating collateral from
        the same source, on which case you need to pile on (eg: adding
        multiple detectio marks to the same image)

        Collateral files shall be generated with name
        :data:`tcfl.tc.tc_c.report_file_prefix` such as:

        >>> collateral_filename = testcase.report_file_prefix + "something"

        will generate filename `report-RUNID:HASHID.something`; thus,
        when multiple testcases are executed in parallel, they will
        not override each other's collateral.

        :param tcfl.tc.tc_c testcase: testcase for which we are
          detecting.

        :param str run_name: name of this run of
          :meth:`tcfl.tc.tc_c.expect`--they are always different.

        :param dict buffers_poll: dictionary where the polled data has
          is available. Note Detection methods that use the same
          poling source (as given by :meth:`poll_context`) will all be
          given the same storage space. as per :meth:`poll` above.

        :param dict buffers: dictionary available exclusively to this
          expectation object to keep data from run to run. This was
          used by :meth:`detect` to store data needed during the
          detection process.

        :param dict results: dictionary of results generated by
          :meth:`detect` as a result of the detection process.

        Called with a lock held for exclusive access to *buffers_poll*;
        *buffers* is thread specific.
        """
        raise NotImplementedError

    def on_timeout(self, run_name, poll_context, buffers_poll, buffers,
                   ellapsed, timeout):
        """
        Perform an action when the expectation times out being found

        Called by the innards of the expec engine when the expectation
        times out; by default, raises a generic exception (as
        specified during the expectation's creation); can be overriden
        to offer a more specific message, etc.
        """
        attachments = {
            'origin': self.origin
        }
        raise self.raise_on_timeout(
            "%s/%s: timed out finding expectation in "
            "'%s' @%.1f/%.1fs/%.1fs)"
            % (run_name, self.name, poll_context,
               ellapsed, timeout, self.timeout),
            attachments)


    def on_found(self, run_name, poll_context, buffers_poll, buffers,
                 ellapsed, timeout, match_data):
        """
        Perform an action when the expectation is found

        Called by the innards of the expect engine when the expectation
        is found; by default, raises the exception set in
        :data:raise_on_found, but can be overriden to do any other
        action.

        Parameter description as on :meth:`flush`.

        :param dict match_data: this is whatever the :meth:`detect`
          method returned that made this expectation be considered as
          found.
        """
        if self.raise_on_found == None:
            # compare like this, since some exceptions will bool to False
            return
        self.raise_on_found.attachments = {
            'origin': self.origin,
            'match_data': match_data,
        }
        self.raise_on_found.attachments_update(match_data)
        if self.origin:
            self.raise_on_found.attachments_update({
                'origin': self.origin
            })
        raise self.raise_on_found


#
# Metaclass for tc_c, to initialize class specific fields
#
# Note we cannot define it nested due to some bug in the pickling
# code that fails to pick up nested classes.
#
# https://stackoverflow.com/questions/1947904/how-can-i-pickle-a-nested-class-in-python
#
# Note we only manipulate these in the single-thread path of 'tcf
# run'.
#
# Can't really use a multiprocess[ing].SyncManager here for the locks,
# as in places where we run this the paths are too deep and it goes
# into failing to create the Unix socket because the path is too long.
# In any case, it doesn't really matter, because this part we only
# touch it in the single-threaded path of TCF run.
#
class _tc_mc(type):
    def __init__(cls, name, bases, d):
        type.__init__(cls, name, bases, d)

        #: What is the current accumulated result of all the
        #: testcases of this class that have ran and finished
        cls.class_result = result_c(0, 0, 0, 0, 0)

#
# The core testcase object
#
# FIXME: make it inherit tcinfo_c when we cleanup
class tc_c(reporter_c, metaclass=_tc_mc):
    r"""
    A testcase, with instructions for configuring, building, deploying,
    setting up, running, evaluating, tearing down and cleaning up.

    Derive this class to create a testcase, implementing the different
    testcase methods to build, deploy and evaluate if it is considered
    a pass or a failure:

    >>> class sometest(tcfl.tc.tc_c):
    >>>
    >>>     def eval_device_present(self):
    >>>          if not os.path.exists("/dev/expected_device"):
    >>>              raise tcfl.tc.error_e("Device not connected")
    >>>
    >>>     def eval_mode_correct(self):
    >>>          s = os.stat("/dev/expected_device"):
    >>>          if s.st_mode & 0x644 == 0:
    >>>              raise tcfl.tc.failed_e("wrong mode")

    .. note:: the class will be ignored as a testcase if its name
              starts with *_base_*; this is useful to create common
              code which will be instantiated in another class without
              it being confused with a testcase.

    :param str name: the name of the testcase
    :param str tc_file_path: the path to the file where the testcase
      was found
    :param str origin: the origin of the testcase (in most cases this
      is a string *FILENAME:LINE*)

    Note that in the most cases, the three arguments will be the same,
    as the name of the testcase will be the same as the path where the
    test case is found and if there is only one testcase per file, the
    origin is either line 1 or no line.

    When a file contains specifies multiple testcases, then they can
    be created such as:

     - name TCFILEPATH#TCCASENAME
     - tc_file_path TCFILEPATH
     - origin TCFILEPATH:LINENUMBER (matching the line number where
       the subcase is specified)

    this allows a well defined namespace in which cases from multiple
    files that are run at the same time don't conflict in name.

    The runner will call the testcase methods to evaluate the test;
    any failure/blockage causes the evaluation to stop and move on to
    the next testcase:

    - *configure\*()* for getting source code, configuring a buid,
      etc ..

    - *build\*()* for building anything that is needed to run the
      testcase

    - *deploy\*()* for deploying the build products or artifacts
      needed to run the testcase to the diffrent targets

    - For evaluating:

      - *setup\*()* to setup the system/fixture for an evaluation run
      - *start\*()* to start/power-on the targets or anything needed
        for the test case evaluation
      - *eval\*()* to actually do evaluation actions
      - *teardown\*()* for powering off

      As well, any *test\*()* methods will be run similarly, but for
      each, the sequence called will be setup/start/test/teardown (in
      contrast to *eval* methods, where they are run in sequence
      without calling setup/start/teardown in between).

    - *clean\*()* for cleaning up (ran only if *-L* is passed on the
      command line)

    - *class_teardown* is mostly used for self-testing and debugging,
       but are functions called whenever every single testcase of the
       same class has completed executing.

    Methods can take no arguments or the names of one or more targets
    they will operate with/on. These targets are declared using the
    :func:`tcfl.tc.target` (for a normal target) and
    :func:`tcfl.tc.interconnect` (for a target that
    interconnects/groups the rest of the targets together).

    The methods that take no targets will be called sequentially in
    **alphabetical order** (not in declaration order!). The methods
    that take different targets will be called in parallel (to
    maximize multiple cores, unless decorated with
    :func:`tcfl.tc.serially`). Evaluation functions are always called
    sequentially, except if decorated with :func:

    The testcase methods use the APIs exported by this class and module:

     - to report information at the appropiate log level:
       :meth:`reporter_c.report_pass`, :meth:`reporter_c.report_fail`,
       :meth:`reporter_c.report_blck` and :meth:`reporter_c.report_info`

     - raise an exception to indicate result of this method:

       - *pass*, raise :py:exc:`tcfl.tc.pass_e` (or simply return)
       - *failed*, raise :py:exc:`tcfl.tc.failed_e`,
       - *error*, raise :py:exc:`tcfl.tc.error_e`,
       - *blocked*, raise :py:exc:`tcfl.tc.blocked_e`; any other
         uncaught Python exception is also converted to this.
       - *skipped*, raise :py:exc:`tcfl.tc.skip_e`

     - run commands in the local machine with :meth:`shcmd_local`; the
       command can be formatted with *%(KEYWORD)[sd]* that will be
       substituted with values found in :attr:`kws`.

     - Interact with the remote targets through instances of
       :class:`target_c` that represent them:

       - via arguments to the method
       - via :attr:`targets`, a dictionary keyed by the names of the
         targets requested with the :func:`target` and
         :func:`interconnect` decorators; for example:

         >>> @tcfl.tc.interconnect()	# named "ic" by default
         >>> @tcfl.tc.target()		# named "target1" by default
         >>> @tcfl.tc.target()		# named "target" by default
         >>> class mytest(tcfl.tc.tc_c):
         >>>     ...
         >>>
         >>>     def start(self, ic, target, target1):
         >>>         ic.power.cycle()
         >>>         target.power.cycle()
         >>>         target1.power.cycle()
         >>>
         >>>     def eval_1(self, target):
         >>>         target.expect("Hello world")
         >>>
         >>>     def eval_2(self):
         >>>         target2 = self.target_group.target("target2")
         >>>         target2.expect("Ready")
         >>>
         >>>     def eval_3(self):
         >>>         mytarget = self.targets["ic"]
         >>>         ic.expect("targets are online")
         >>>
         >>>     def teardown(self):
         >>>         for _n, target in reversed(self.targets.items()):
         >>>            target.power.off()
         >>>

       :class:`target_c` expose APIs to act on the targets, such as
       power control, serial console access, image deployment

    """

    #
    # Public testcase API/interface
    #

    #: Reason why this testcase is being executed
    #:
    #: This is a string that will be sent to the servers when asking
    #: for targets, as information for other users to know what is the
    #: target used for.
    #:
    #: Note there are no size limits, but the server might crop it
    #:
    #: The fields are in the format *%(FIELD)s* and these are the
    #: :ref:`keywords <finding_testcase_metadata>` the testcase
    #: exports.
    reason = None

    #: Priority to use to allocate targets when running testcases
    priority = 500

    #: Allocate targets with preemption rights
    preempt = False

    #: Allocate on behalf of another user
    obo = None

    #: List of places where we declared this testcase is build only
    build_only = []


    #: Allocation ID this testcase is using
    #:
    allocid = None
    # (internally: setting this forces the testcase to use the given
    # allocation ID)

    #: Max runs for each testcase
    #:
    #: When calculating how many permutations of targets are executed,
    #: limit it to this total on each.
    #:
    #: This is currently a hack to tie-us up with current orchestrator
    #: limitations until the improved orchestrator is ready.
    max_runs_per_tc = 0

    #: reporting hooks on exception keyed by callable, value origin
    _report_exception_hooks = {}

    def __init__(self, name, tc_file_path, origin, hashid = None):
        #
        # need this before calling reporter_c.__init__
        #
        #: Time when this testcase was created (and thus all
        #: references to it's inception are done); note in
        #: __init_shallow__() we update this for when we assign it to
        #: a target group to run.
        self.ts_start = time.time()
        self.ts_end = None

        reporter_c.__init__(self, testcase = self)
        for hook_pre in self.hook_pre:
            assert callable(hook_pre), \
                "tcfl.tc.tc_c.hook_pre contains %s, defined as type '%s', " \
                "which  is not callable" % type(hook_pre).__name__

        #: Lock to access :attr:`buffers` safely from multiple threads
        #: at the same time for the same testcase.
        self.lock = None	# initialized by __init_shallow__()
        # see also __init_shallow__()

        #: Keywords for *%(KEY)[sd]* substitution specific to this
        #: testcase.
        #:
        #: Note these do not include values gathered from remote
        #: targets (as they would collide with each other). Look at
        #: data:`target.kws <tcfl.tc.target_c.kws>` for that.
        #:
        #: These can be used to generate strings based on information,
        #: as:
        #:
        #:   >>>  print "Something %(FIELD)s" % target.kws
        #:   >>>  target.shcmd_local("cp %(FIELD)s.config final.config")
        #:
        #: Fields available:
        #:
        #:   - `runid`: string specified by the user that applies to
        #:     all the testcases
        #:
        #:   - `srcdir` and `srcdir_abs`: directory where this
        #:     testcase was found
        #:
        #:   - `thisfile`: file where this testscase as found
        #:
        #:   - `tc_hash`: unique four letter ID assigned to this
        #:     testcase instance. Note that this is the same for all
        #:     the targets it runs on. A unique ID for each target of
        #:     the same testcase instance is the field *tg_hash* in the
        #:     target's keywords :data:`target.kws
        #:     <tcfl.tc.target_c.kws>` (FIXME: generate, currently
        #:     only done by app builders)
        #:
        #: (this will actually be fully initialzied in *__init_shallow__()*)
        self.kws = {}
        self.kws_origin = {}

        self.__init_shallow__(None)
        self.name = name

        self.kw_set('pid', str(os.getpid()))
        self.kw_set('tid', "%x" % threading.current_thread().ident)
        # use instead of getfqdn(), since it does a DNS lookup and can
        # slow things a lot
        self.kw_set('host_name', socket.gethostname())
        self.kw_set('tc_name', self.name)
        # top level testcase name is that of the toplevel testcase,
        # with any subcases removed (anything after ##), so
        #
        # some/test/path/name#param1#param2##subcase/path/subcase
        #
        # becomes
        #
        # some/test/path/name#param1#param2
        self.kw_set('tc_name_toplevel', self.name.split("##", 1)[0])
        self.kw_set('cwd', os.getcwd())
        # This one is left for drivers to do their thing in here
        self.kw_set('tc_name_short', self.name)
        self.origin = origin
        self.kw_set('tc_origin', self.origin)
        # Instantiate a copy of build_only to respect what the derived
        # class has set -- we need type(self) to ensure that we get
        # the type of the derived class -- and here I might not know
        # enough Python, mind you, there might be a better way to do
        # it-- and deepcopy to make sure we have an object that is
        # fully owned by this instance object in case it decides to
        # modify it.
        self.build_only = copy.deepcopy(type(self).build_only)

        #: dictionary of tags this test case has been stamped with
        self._tags = dict(self._tags)
        self._tags['name'] = (self.name, commonl.origin_get())
        self._tags_update()
        #: Group of targets this testcase is being ran on
        self.target_group = None

        if os.path.isabs(tc_file_path):
            srcdir = os.path.relpath(os.path.dirname(
                os.path.abspath(tc_file_path)))
            thisfile = tc_file_path
        else:
            # This makes sure "FILE" resolves to "./FILE"
            srcdir = os.path.relpath(os.path.dirname(
                os.path.abspath(tc_file_path)))
            thisfile = os.path.join(srcdir, os.path.basename(tc_file_path))
        self._kw_set("runid", "" if tc_c.runid == None else tc_c.runid,
                     origin = "cmdline")
        for key, value in self.runid_extra.items():
            self._kw_set(f"runid_extra.{key}", value, origin = "cmdline")
        self._kw_set("srcdir", srcdir)
        self._kw_set("srcdir_abs",
                     os.path.dirname(os.path.abspath(tc_file_path)))
        self._kw_set("thisfile", thisfile)

        #: Expect loop to wait for things to happen
        self.tls.expect_timeout = 60
        #: (DEPRECATED) Ticket ID for this testcase / target group
        self.ticket = None
        #: Testcase identifier including the Run ID (if specified)
        #:
        #: This can be used by reporting drivers/engines to get a
        #: string looking like RUNID:HASHID that uniquely identifies
        #: the testcase amongst many runs.
        self.runid_hashid = None	        # updated by mkticket()
        # The group of targets where the TC is running
        self._target_group = None
        #: :class:`Target objects <tcfl.tc.target_c>`) in which this
        #: testcase is running (keyed by target want name, as given to
        #: decorators :func:`tcfl.tc.target` and
        #: func:`tcfl.tc.interconnect`.
        #: Note this maps to ``self._target_groups_c.targets()`` for
        #: convenience.
        self.targets = {}
        # List of remote targets selected for this run
        self.rt_selected = None
        self.ic_selected = None
        # FIXME
        self.skip_reports = False

        # Prefix used for lines that print reports
        self._report_prefix = ""
        self._prefix_update()	# Update the prefix

        #: Result of the last evaluation run
        #:
        #: When an evaluation is run (setup/start/eval/teardown), this
        #: variable reflexts the evaluation status; it is meant to be
        #: used during the *teardown* phase, so for example, in case
        #: of failure, the teardown phase might decide to gather
        #: information about the current target's state.
        self.result_eval = result_c(0, 0, 0, 0, 0)

        #: Result of the last run of all phases in this testcase
        #:
        #: we might need to look at this in other testcases executed
        #: inmediately after (as added with :meth:`post_tc_append`).
        self.result = result_c(0, 0, 0, 0, 0)

        # Testcases we need to run when we are done running this one
        self._tcs_post = []

        # Initialize prefixes and a few keywords we need -- we might
        # override these later if/when we assign a target group.
        self.mkticket(hashid)
        self.tmpdir = os.path.join(tc_c.tmpdir, self.ticket)
        try:
            os.makedirs(self.tmpdir)
        except OSError:
            if not os.path.isdir(self.tmpdir):
                raise
        self._kw_set("tmpdir", self.tmpdir)
        self._kw_set("tc_hash", self.ticket)

        global log_dir
        if log_dir == None:
            _log_dir = os.getcwd()
        else:
            _log_dir = log_dir
        # note we set this after setting some of the kws
        #: Report file prefix
        #:
        #: When needing to create report file collateral of any kind,
        #: prefix it with this so it always shows in the same location
        #: for all the collateral related to this testcase:
        #:
        #: >>>    target.shell.file_copy_from("remotefile",
        #: >>>                                self.report_file_prefix + "remotefile")
        #:
        #: will produce *LOGDIR/report-RUNID:HASHID.remotefile* if
        #: *--log-dir LOGDIR -i RUNID* was provided as command line.
        #:
        #: >>>    target.capture.get('screen',
        #: >>>                       self.report_file_prefix + "screenshot.png")
        #:
        #: will produce *LOGDIR/report-RUNID:HASHID.screenshot.png*
        #:
        self.report_file_prefix = os.path.join(
            _log_dir, "report-" + self.runid_hashid + ".")

        # Always before we start, run the site hook
        for hook in self.hook_pre:
            hook(self)

        #: list of subcases this test is asked to execute by the test
        #: case runner (see :data:`subtc`)
        #:
        #: Subcases follow the format
        #: *NAME/SUBNAME/SUBSUBNAME/SUBSUBSUBNAME...*
        self.subcases = []

        #: list of subcases this testcase contains
        #:
        #: Note this is different to :data:`subcases` in that this is
        #: the final list the testcase has collected after doing
        #: discovery in the machine and (possibly) examining the
        #: execution logs.
        #:
        #: It is ordered by addition time, so things sub-execute in
        #: addition order.
        self.subtc = collections.OrderedDict()
        #: parent of this testcase (normally used for subcases)
        self.parent = None
        #: do we have to actually acquire any targets?
        #:
        #: in general (default), the testcases need to acquire the
        #: targets where they are going to be executed, but in some
        #: cases, they do not.
        self.do_acquire = True

        # Do actually execute the steps? This is used by the subcase
        # handling system when we are just reporting using @subcase;
        # later on, we don't actually need to execute those subtcs,
        # just report on them.
        self._execute = True


    def __init_shallow__(self, other):
        # Called by clone() to initialize those things that are
        # different in shallow clones (instances that are almost
        # identical except for ... a few things)

        self.lock = threading.Lock()

	#: For use during execution of phases; testcase drivers can
        #: store anything they need during the execution of each phase.
        #: It will be, however, deleted when the phase is completed.
        self.buffers = {}
        if other:
            self.kws = dict(other.kws)
        elif self.kws == None:
            # respect settings from __init__
            self.kws = dict()

        #: Origin of the keyword in self.kws; the values for these are
        #: lists of places where the setting was set, or re-set
        if other:
            self.kws_origin = dict(other.kws_origin)
        elif self.kws_origin == None:
            # respect settings from __init__
            self.kws_origin = dict()

        #: list of files kept as collateral in logdir (to decide if we
        #: remove later or not)
        self.collateral = set()
        self.tls = threading.local()
        self.log = tc_logadapter_c(logger, None)
        # instance specific list of files/paths to wipe at the end
        self._cleanup_files = set()

        if other:
            self.subcases = list(other.subcases)
        else:
            self.subcases = list()
        self.subtc = collections.OrderedDict()
        if other:
            self.parent = other.parent
            for subtc_name, subtc in other.subtc.items():
                subtc_copy = subtc._clone()
                subtc_copy.parent = self
                self.subtc[subtc_name] = subtc_copy
        else:
            self.parent = None

        # sequentially incremented everytime we call expect()
        self._expect_count = 0
        self._expectations_global = []
        self._expectations_global_names = set()

        # update its inception time (from reporter_c.__init__), since
        # calling this means this is being assigned to a target group
        # to run.
        # for reporter_c
        self.testcase = self
        self.ts_start = time.time()

    def __thread_init__(self, tls_parent):
        """
        When we run some methods of this object in a different thread,
        we need to initialize some parts first.

        This is currently quite a dirty hack, but it is what we
        have. We use it when we clone the object to run in a target
        group or when we spawn threads to run methods in parallel.
        """
        # we want to keep the same buffering state we had in the
        # parent thread, for any search context, etc, so we do a full
        # replication.
        if hasattr(tls_parent, 'buffers'):
            self.tls.buffers = copy.deepcopy(tls_parent.buffers)
        else:
            self.tls.buffers = {}
        if hasattr(tls_parent, '_expectations'):
            self.tls._expectations = copy.deepcopy(tls_parent._expectations)
        else:
            self.tls._expectations = []
        if tls_parent:
            self.tls.expect_timeout = tls_parent.expect_timeout
        else:
            # this is a hack
            self.tls.expect_timeout = 60

    def _subcase_get(self, subcase):
        if subcase in self.subtc:
            return self.subtc[subcase]
        # messed up we don't keep tc_file_path as is
        # Convention is we separate subcase names with ##
        subtc = subtc_c(self.name + "##" + subcase,
                        self.kws['thisfile'],
                        self.origin, self)
        # normally this subcases do not need to be executed, the are
        # just used for accounting
        subtc._execute = False
        self.subtc[subcase] = subtc
        return subtc

    @classmethod
    def testcase_name_validate(cls, name):
        """
        Validate *name* is a valid testcase name

        :param str name: name of the testcase; this must be:

          - a simple string (shortish if possible)
          - contain no *##*

        """
        assert isinstance(name, str), \
            f"name: expected str, got {type(name)}"
        assert '##' not in name, \
            f"test case names cannot contain '##', got '{name}'"


    def subcase(self, subcase):
        """
        Start a new subcase context

        This can be called in any method of any phase to start
        reporting as a new subcase of this testcase
        (NAME##SUBCASENAME). Note this can be nested as much as
        wanted, eg:

        >>> class _test(tcfl.tc.tc_c):
        >>>     ...
        >>>     def eval_10_something(self):
        >>>        ....
        >>>        with self.subcase("NAME1"):
        >>>            with self.subcase("NAME2"):
        >>>                self.report_pass("Done")

        Will report a pass on testcase *_test##NAME1##NAME2* which is
        a subcase of *_test##NAME1* which is itself a subcase of
        *_test*.

        :param str subcase: a subcase name
        """
        self.testcase_name_validate(subcase)
        return msgid_c(subcase = subcase)


    def is_static(self):
        """
        Returns *True* if the testcase is *static* (needs to targets to
        execute), *False* otherwise.
        """
        return not self._targets


    @classmethod
    def report_exception_hook_add(cls, fn, origin = None):
        """Add a report exception hook

        When the reporting system catches an exception to
        automatically report on, these hooks can be called to look for
        known issues and specialize the reporting.

        A hook is defined as:

        >>> def _known_issues_report_exception_hook(
        >>>    reporter: tcfl.tc.reporter_c,
        >>>    testcase: tcfl.tc.tc_c,
        >>>    target: tcfl.tc.target_c,
        >>>    e: exception,
        >>>    level: int,
        >>>    alevel: int,
        >>>    msg_tag: str,
        >>>    attachments: dict,
        >>>    subcase: str = None, subcase_base: str = None):
        >>>        # actions
        >>>
        >>> tcfl.tc.tc_c.report_exception_hook_add(
        >>>     _known_issues_report_exception_hook)

        The reporter will be either the testcase or the target; the
        target sometimes can't be found and will be *None*. *reporter*
        is always something that can be used to call *report_fail* or
        *report_pass*, etc

        *e* is the exception that was caught; *level* and *alevel* are
        the levels the handler computed for verbosity; *msg_tag* is
        PASS, FAIL, ERRR, BLCK or SKIP, based on what the handler
        computed from the exception in
        :meth:`result_c.report_from_exception`. *attachments* are any
        extra data passed with the exception. *subcase* and
        *subcase_base* describe the names of the subcases that were
        reported, if any.

        """
        assert callable(fn), \
            f"{fn}: expected callable, got {type(fn)}"
        if origin == None:
            origin = commonl.origin_get(2)
        cls._report_exception_hooks[fn] = origin



    #: Number of characters in the testcase's :term:`hash`
    #:
    #: The testcase's *HASHID* is a unique identifier to identify a
    #: testcase the group of test targets where it ran.
    #:
    #: This defines the lenght of such hash; before it used 4 to be
    #: four but once over 40k testcases are being run, conflicts start
    #: to pop up, where more than one testcase/target combo maps to
    #: the same hash.
    #:
    #:  32 ^ 4 = 1048576 unique combinations
    #:
    #:  32 ^ 6 = 1073741824 unique combinations
    #:
    #: 6 chars offers a keyspace 1024 times larger with base32 than
    #: 4 chars. Base64 increases the amount, but not that much
    #: compared to the ease of confusion between caps and non caps.
    #:
    #: So it has been raised to 6.
    #:
    #: FIXME: add a registry to warn of used ids
    hashid_len = 6

    def relpath_to_abs(self, path):
        """
        Given a path relative to the test script's source, make it absolute.

        .. admonition: example

           If the testscript calling this API (as given by
           ``testcase.kws['srcdir_abs']`` is
           ``/some/path/test_file.py`` and this is given as
           ``subdir/somefile``, then the source will be considered to
           be ``/some/path/subdir/somefile``

        @returns string with the absolutized path if relative, the
          same if already absolute

        """

        if os.path.isabs(path):
            return path
        return os.path.join(self.kws['srcdir_abs'], path)

    def shcmd_local(self, cmd, origin = None, reporter = None,
                    logfile = None, env = None):
        """
        Run a shell command in the local machine, substituting
        %(KEYWORD)[sd] with keywords defined by the testcase.

        :param str origin: (optional) when reporting information about
          this expectation, what origin shall it list, eg:

          - *None* (default) to get the current caller
          - *commonl.origin_get(2)* also to get the current caller
          - *commonl.origin_get(1)* also to get the current function

          or something as:

          >>> "somefilename:43"

        See also :meth:`run_local`.

        :param dict env: (optional) dictionary of environment
          variables to be passed to :func:`subprocess.check_output`
          (same format as such).
        """
        if origin == None:
            origin = commonl.origin_get(2)
        return self._shcmd_local(cmd % self.kws, origin = origin,
                                 reporter = reporter, logfile = logfile,
                                 env = env)

    def run_local(self, command, expect = None, cwd = None):
        """
        Run a command on the local system with an interface similar to
        :meth:`target.shell.run <tcfl.target_ext_shell.shell.run>`.

        This is similar to :meth:`shcmd_local`.

        :param str command: command line to run (will be run a shell command)

        :param str or re._pattern_type expect: (optional) if defined, a
          string or regular expression that shall be found in the output
          of the command, raising :exc:`tcfl.tc.failed_e` otherwise.

        :param str cwd: (optional) change into this directory before
          starting

        :raises: :exc:`subprocess.CalledProcessError` if the command fails

        """
        if cwd == None:
            cwd = self.tmpdir

        output = subprocess.check_output(command, cwd = cwd, shell = True,
                                         text = "utf-8",
                                         stderr = subprocess.STDOUT)
        if expect:
            if isinstance(expect, str):
                if expect not in output:
                    raise failed_e(
                        "can't find '%s' in output" % expect,
                        dict(output = output))
            elif isinstance(expect, typing.Pattern):
                if not expect.search(output):
                    raise failed_e(
                        "can't find '%s' in output" % expect.pattern,
                        dict(output = output))
            else:
                raise blocked_e(
                    "can't handle expect of type %s;"
                    " can do strings or compiled regex" % expect)
        self.report_info("command ran: " + command,
                         dict(output = output), alevel = 1)
        return output


    @classmethod
    def file_ignore_add_regex(cls, regex, origin = None):
        """
        Add a regex to match a file name to ignore when looking
        for testcase files

        :param str regex: Regular expression to match against the
          file name (not path)
        :param str origin: [optional] string describing where this
          regular expression comes from (eg: FILE:LINENO).
        """
        assert isinstance(regex, str)
        if origin != None:
            assert isinstance(origin, str)
        if origin == None:
            o = inspect.stack()[1]
            origin = "%s:%s" % (o[1], o[2])
        cls._ignore_regexs.append((re.compile(regex), origin))

    @classmethod
    def dir_ignore_add_regex(cls, regex, origin = None):
        """
        Add a regex to match a directory name to ignore when looking
        for testcase files

        :param str regex: Regular expression to match against the
          directory name (not path)
        :param str origin: [optional] string describing where this
          regular expression comes from (eg: FILE:LINENO).
        """
        assert isinstance(regex, str)
        if origin != None:
            assert isinstance(origin, str)
        if origin == None:
            o = inspect.stack()[1]
            origin = "%s:%s" % (o[1], o[2])
        cls._ignore_directory_regexs.append((re.compile(regex), origin))

    @classmethod
    def driver_add(cls, _cls, origin = None, *args):
        """
        Add a driver to handle test cases (a subclass of :class:tc_c)

        A testcase driver is a subclass of :class:`tcfl.tc.tc_c` which
        overrides the methods used to locate testcases and implements the
        different testcase configure/build/evaluation functions.

        >>> import tcfl.tc
        >>> class my_tc_driver(tcfl.tc.tc_c)
        >>> tcfl.tc.tc_c.driver_add(my_tc_driver)

        :param tcfl.tc.tc_c _cls: testcase driver
        :param str origin: (optional) origin of this call
        """
        assert issubclass(_cls, tc_c), \
            f"{_cls}: shall be a type, got {type(_cls)}"
        if origin == None:
            o = inspect.stack()[1]
            origin = "%s:%s" % (o[1], o[2])
        setattr(_cls, "origin", origin)
        logger.info("%s: Added test case driver %s", origin, _cls)
        tcd_setup = getattr(_cls, "setup", None)
        if tcd_setup != None:	# base class tc_c has no setup()
            _cls.setup(*args)
        cls._tc_drivers.append(_cls)

    #: (list of callables) a list of functions to call before starting
    #: execution of each test case instance (right before any phases
    #: are run)
    #:
    #: Usable to do final testcase touch up, adding keywords needed
    #: for the site deployment. etc.
    #:
    #: Note these will be called as methods in the order in the list,
    #: so the first argument will be always be the the testcase
    #: instance.
    #:
    #: E.g.: in a TCF configuration file `.tcf/conf_hook.py` you can
    #: set:
    #:
    #: >>> def _my_hook_fn(tc):
    #: >>>     # Classify testcases based on category:
    #: >>>     # - red
    #: >>>     # - green
    #: >>>     # - blue
    #: >>>     #
    #: >>>     # tc_name keyword has the path of the testcase, which
    #: >>>     # we are using for the sake of example to categorize;
    #: >>>     # keywords can be dumped by running `tcf run
    #: >>>     # /usr/share/examples/test_dump_kws*py.
    #: >>>
    #: >>>     name = tc.kws['tc_name']
    #: >>>     categories = set()
    #: >>>     for category in [ 'red', 'green', 'blue' ]:
    #: >>>         # if test's path has CATEGORY, add it
    #: >>>         if category in name:
    #: >>>             categories.add(category)
    #: >>>     if not categories:
    #: >>>         categories.add('uncategorized')
    #: >>>     tc.kw_set('categories', ",".join(categories))
    #: >>>     tc.log.error("DEBUG categories: %s", ",".join(categories))
    #: >>>
    #: >>> tcfl.tc.tc_c.hook_pre.append(_my_hook_fn)
    #: >>>
    #:
    #: .. warning::
    #:
    #:    - this is a global variable for all testcases of all classes
    #:      and instances assigned to run in different targets
    #:
    #:    - these functions will execute on different threads
    #:      and processes, so **do not use shared data or global
    #:      variables**.
    #:
    #:    - only add to this list from configuration files, never from
    #:      testcases or testcase driver code.
    hook_pre = []

    #: (dict) a dictionary to translate target type names, from
    #: *TYPE[:BSP]* to another name to use when reporting as it is
    #: useful/convenient to your application (eg: if what you are
    #: testing prefers other type names); will be only translated if
    #: present. E.g.:
    #:
    #: >>> tcfl.tc_c.type_map = {
    #: >>>     # translate to Zephyr names
    #: >>>     "arduino-101:x86" = "arduino_101",
    #: >>>     "arduino-101:arc" = "arduino_101_ss",
    #: >>> }
    type_map = {}

    #: Map exception types to results
    #:
    #: this allows to automaticall map an exception raised
    #: automatically and be converted to a type. Any testcase can
    #: define their own version of this to decide how to convert
    #: exceptions from the default of them being considered blockage
    #: to skip, fail or pass
    #:
    #: >>> class _test(tcfl.tc.tc_c):
    #: >>>     def configure_exceptions(self):
    #: >>>         self.exception_to_result[OSError] = tcfl.tc.error_e
    exception_to_result = {
        AssertionError: blocked_e,
    }

    #: How many times do we repeat the evaluation (for stress/MTBF)
    eval_repeat = 1

    #: Which evaluation are we currently running (out of :data:`eval_repeat`)
    eval_count = 0

    #: List of callables that will be executed when a testcase is
    #: identified; these can modify as needed the testcase (eg:
    #: scanning for tags)
    testcase_patchers = []

    runid = None
    runid_visible = ""
    #: Extra values set from the commandline for identified this run
    #: (eg: URLs to repositories, repository versions, etc)
    #:
    #: They are usually taken from the command line (--id-extra
    #: KEY=VALUE) or can be set in configuration files:
    #:
    #: tcfl.tc.tc_c.runid_extra[KEY] = VALUE
    #:
    #: Note this shall be limited to simple values; KEYs can have to
    #: be strings (using periods to separate subfields FIXME:
    #: reference), values shall be scalar (str, int, float, bool)
    runid_extra = {}	# FIXME: impl verification run

    #: temporary directory where testcases can drop things; this will
    #: be specific to each testcase instance (testcase and target
    #: group where it runs).
    tmpdir = tempfile.mkdtemp(prefix = "tcf.run" + runid_visible + "-")

    #: temporary directory where to store information (serial console,
    #: whatever) that will be captured on each different evaluation;
    #: on each invocation of the evaluation, a new buffer dir will be
    #: allocated and code that captures things from the target will
    #: store captures in there.
    buffers = None

    #
    # Protected / private APIs
    #
    # For internal use only!

    # Number of testcases we are running (used to calculate how much
    # we wait for targets)
    _tcs_total = 0

    #: Number of testcases running on targets
    jobs = 1

    _hash_salt = ""

    _ignore_regexs = [
        # FIXME: there was a way to get the current file and line to
        # replace builtin
        (re.compile(".*~$"), "builtin"),
        (re.compile(r".*\.txt$"), "builtin"),
        (re.compile(r".*\.[oachS]$"), "builtin"),
        (re.compile(r".*\.asm$"), "builtin"),
        (re.compile(r".*\.so$"), "builtin"),
        (re.compile(r".*\.o\.cmd$"), "builtin"),
        (re.compile(r".*\.cmd$"), "builtin"),
        (re.compile(r".*\.pyc$"), "builtin"),
        (re.compile(r"\.git/"), "builtin"),
    ]
    # List of regular expressions of directory names to ignore
    _ignore_directory_regexs = [
        (re.compile("^outdir(-.*)?$"), "builtin"),
        (re.compile("^.git$"), "builtin"),
    ]
    # In an unlimited test run, we execute on ALL the targets we find; on
    # limited we only run on target of each type
    _mode = False

    # Extra report fields when a test case fails
    #
    # Can be set from configuration (extra_report_format) or with the
    # --extra-report-format command line to tcf. Fields are the same
    # as for the markup rules.
    _extra_report_format = None

    # Phases we are going to run (configure, build, deploy, eval,
    # clean) Note we use a dictionary where the key is the name of the
    # phase and the value is a list of the places where said decission
    # has been made. Likewise with _phases_skip.
    _phases = collections.defaultdict(set)
    _phases_skip = collections.defaultdict(set)

    #: tags that describe this testcase
    _tags = {}
    #: targets on which this testcase wants to run
    # FIXME: rename to _target_wants
    # This needs to be an OrderedDictionary so the same order in which
    # we specified the wanted targets is maintained by default.
    _targets = collections.OrderedDict()
    # Do we run also on targets that have been disabled
    _targets_disabled_too = False
    #: index for target names
    _target_count = 0
    #: interconnects this TC needs to run
    _interconnects = set()
    #: index for interconnect names
    _ic_count = 0

    # Do we actually run or pretend to?
    _dry_run = False

    # Where we cache all the remote target info
    rt_all = None

    #
    # Support to call the testcase methods
    # (configure|build|deploy|eval|clean)*()
    #
    _configure_serially = False
    _configure_serial = []
    _configure_parallel = []

    _build_serially = False
    _build_serial = []
    _build_parallel = []

    _deploy_serially = False
    _deploy_serial = []
    _deploy_parallel = []

    _setup_serial = []
    _start_serial = []
    _eval_serial = []
    _teardown_serial = []
    _class_teardown_serial = []

    _clean_serially = False
    _clean_serial = []
    _clean_parallel = []

    # List of available testcase drivers
    _tc_drivers = []

    # Will targets be released at the end of the testcase
    release = True

    #
    # App Builder entry points; when you define an app_NAME builder in
    # the @target() decorator, _target_app_setup() creates
    # CLASSNAME.(configure|build|deploy|clean)_for_TARGET() calls for
    # you that call these functions.
    #
    # Internally, these are discovered and called by _methods_run /
    # _method_trampoline_call as they have been aliased via a
    # setattr() to a CLASNAME.FUNCTION_for_TARGETNAME() by the
    # @target() decorator via _target_app_setup().
    def _configure_50_for_target(self, _target):
        for bsp in _target.bsps:
            # FIXME: use msgid_c here
            _target.bsp_set(bsp)
            _target.report_info("configuring BSP %s" % bsp, dlevel = 3)
            _app, app_src = _target._app_get_for_bsp("configure")
            app.configure(_app, self, _target, app_src)
        for bsp, (_app, app_src, _) \
            in _target.bsps_stub.items():
            if _app == None:
                raise blocked_e("%s: BSP has to be stubbed, but no "
                                "stubbing information was provided" % bsp)
            # FIXME: use msgid_c here
            _target.bsp_set(bsp)
            _target.report_info("configuring stub-BSP %s" % bsp, dlevel = 3)
            # FIXME: append app_src_options
            app.configure(_app, self, _target, app_src)
        if _target.bsps:
            _target.bsp_set()

    # same as _configure_50_for_target()
    def _build_50_for_target(self, _target):
        for bsp in _target.bsps:
            # FIXME: use msgid_c here
            _target.bsp_set(bsp)
            _target.report_info("building BSP %s" % bsp, dlevel = 3)
            _app, app_src = _target._app_get_for_bsp("build")
            app.build(_app, self, _target, app_src)
        for bsp, (_app, app_src, _) \
            in _target.bsps_stub.items():
            if _app == None:
                raise blocked_e("%s: BSP has to be stubbed, but no "
                                "stubbing information was provided" % bsp)
            # FIXME: use msgid_c here
            _target.bsp_set(bsp)
            _target.report_info("building stub-BSP %s" % bsp, dlevel = 3)
            app.build(_app, self, _target, app_src)
        if _target.bsps:
            _target.bsp_set()

    # same as _configure_50_for_target()
    # Deployment methods can do all the work themselves or append a
    # list of images to the images set to be uploaded to the target
    # using the images interface.
    def _deploy_50_for_target(self, _target):
        images = dict()
        result = result_c(0, 0, 0, 0, 0)
        for bsp in _target.bsps:
            # FIXME: use msgid_c here
            _target.bsp_set(bsp)
            _target.report_info("deploying BSP %s" % bsp, dlevel = 3)
            _app, app_src = _target._app_get_for_bsp("deploy")
            r = app.deploy(images, _app, self, _target, app_src)
            assert isinstance(r, result_c)
            result += r
        for bsp, (_app, app_src, _) \
            in _target.bsps_stub.items():
            if _app == None:
                raise blocked_e("%s: BSP has to be stubbed, but no "
                                "stubbing information was provided" % bsp)
            # FIXME: use msgid_c here
            _target.bsp_set(bsp)
            _target.report_info("deploying stub-BSP %s" % bsp,
                                dlevel = 3)
            r = app.deploy(images, _app, self, _target, app_src)
            assert isinstance(r, result_c)
            result += r
        if _target.bsps:
            _target.bsp_set()

        if images:
            if getattr(_target, "images", None) != None:
                result = _target.images.flash(images)
            else:
                result = result_c(0, 0, 0, 1, 0)
                _target.report_info("Images collected by deploy phase, but "
                                    "no image upload interfaces available",
                                    dlevel = -1)
        return result

    # same as _configure_50_for_target()
    def _setup_50_for_target(self, _target):
        for bsp in _target.bsps:
            # FIXME: use msgid_c here
            _target.bsp_set(bsp)
            _target.report_info("setting up BSP %s" % bsp, dlevel = 3)
            _app, app_src = _target._app_get_for_bsp("setup")
            app.setup(_app, self, _target, app_src)
        # Stubs are assumed to need no setup
        if _target.bsps:
            _target.bsp_set()

    # same as _configure_50_for_target()
    def _start_50_for_target(self, _target):
        for bsp in _target.bsps:
            # FIXME: use msgid_c here
            _target.bsp_set(bsp)
            _target.report_info("starting BSP %s" % bsp, dlevel = 3)
            _app, app_src = _target._app_get_for_bsp("start")
            app.start(_app, self, _target, app_src)
        # Stubs are assumed to need no starting
        if _target.bsps:
            _target.bsp_set()

    # same as _configure_50_for_target()
    def _teardown_50_for_target(self, _target):
        for bsp in _target.bsps:
            # FIXME: use msgid_c here
            _target.bsp_set(bsp)
            _target.report_info("tearing down BSP %s" % bsp, dlevel = 3)
            _app, app_src = _target._app_get_for_bsp("teardown")
            app.teardown(_app, self, _target, app_src)
        # Stubs are assumed to need no teardown
        if _target.bsps:
            _target.bsp_set()

    # same as _configure_50_for_target()
    def _clean_50_for_target(self, _target):
        for bsp in _target.bsps:
            _target.bsp_set(bsp)
            _target.report_info("cleaning BSP %s" % bsp, dlevel = 3)
            _app, app_src = _target._app_get_for_bsp("clean")
            app.clean(_app, self, _target, app_src)
        for bsp, (_app, app_src, _) \
            in _target.bsps_stub.items():
            if _app == None:
                raise blocked_e("%s: BSP has to be stubbed, but no "
                                "stubbing information was provided" % bsp)
            _target.bsp_set(bsp)
            _target.report_info("cleaning stub-BSP %s" % bsp, dlevel = 3)
            app.clean(_app, self, _target, app_src)
        if _target.bsps:
            _target.bsp_set()

    #
    # Linkage into the report API and support for it
    #
    @staticmethod
    def ident():
        """
        Returns the current phase identifier for the testcase

        The phase identifier is accumulated per thread and the user
        can add more to it by running:

        >>> with tcfl.msgid_c("L1"):
        >>>    ...more code...

        Any calls inside the *with* block will be reported as:

          RUNID:HASHIDL1

        If a second with is done (eg: inside another function):

        >>> with tcfl.msgid_c("L1"):
        >>>    ...more code...
        >>>    with tcfl.msgid_c("L2"):
        >>>        ...more code...

        It would be reported as:

          RUNID:HASHIDL2L3

        :returns: a string with the current accumulated phase
          identifier.
        """
        return msgid_c.ident()


    # OLD, deprecated
    def _report_mk_prefix(self):
        """
        Update the prefix we use for the logging/reports when some
        parameter changes.
        """
        if self.target_group == None:
            self._report_prefix = self.name + " @local"
        elif self.target_group.len() == 1:
            self._report_prefix = \
                self.name \
                + " @" \
                + list(self.target_group.targets.values())[0].fullid
        else:
            self._report_prefix = self.name + " @" + self.target_group.name

    def report_mk_prefix(self):
        """
        Update the prefix we use for the logging/reports when some
        parameter changes.
        """
        return self._report_prefix

    #
    # Method handling code
    #
    # these _method* methods are the infrastructure that helps locate
    # and call the methods
    # configure|build|deploy|setup|start|eval|clean* defined by the
    # user or dynamically added by App builders.
    #
    @staticmethod
    def _valid_id(ident):
        # Credit:
        # http://stackoverflow.com/questions/12700893/how-to-check-if-a-string-is-a-valid-python-identifier-including-keyword-check/29586366#29586366
        # Licensing: cc by-sa 3.0
        """Determines, if string is valid Python identifier."""

        # Smoke test - if it's not string, then it's not identifier,
        # but we don't want to just silence exception. It's better to
        # fail fast.
        if not isinstance(ident, str):
            raise TypeError('expected str, but got {!r}'.format(type(ident)))

        # Resulting AST of simple identifier is <Module [<Expr <Name "foo">>]>
        try:
            root = ast.parse(ident)
        except SyntaxError:
            return False

        if not isinstance(root, ast.Module):
            return False

        if len(root.body) != 1:
            return False

        if not isinstance(root.body[0], ast.Expr):
            return False

        if not isinstance(root.body[0].value, ast.Name):
            return False

        if root.body[0].value.id != ident:
            return False

        return True

    def __method_prepare(self, fname, fn, do_serially, serial, parallel):
        """
        Prepare a single method for later execution with __methods_call()

        Collects the information
        """
        # Evaluate a method from the class and decide if it goes in
        # the list of things that have to be run serially or in the
        # ones that can be parallelize.
        # Verifies that methods that declare variables match targets
        # that have beend declared.
        cls = type(self)
        # FIXME: maybe this should dig into the function chain until
        # no more __wrappeds__ are there?
        real_fn = getattr(fn, "__wrapped__", fn)
        argnames = list(real_fn.__code__.co_varnames[:real_fn.__code__.co_argcount])
        if inspect.ismethod(fn):
            if fn.__self__ is cls:
                _type = False
            else:
                _type = True
            try:
                argnames.pop(0)
            except IndexError:
                raise blocked_e(
                    "%s.%s(): not a valid method (missing 'self'?), "
                    "@classmethod or @staticmethod @%s"
                    % (cls.__name__, fname, commonl.origin_get_object(fn)))
        else:
            _type = False

        # Any arguments left MUST be names of targets that have been
        # declared with the @target decorator to the class
        for argname in argnames:
            if not self._valid_id(argname):
                raise blocked_e(
                    "%s.%s() (%s): argument '%s' is not a "
                    "valid Python identifier"
                    % (cls.__name__, fname,
                       commonl.origin_get_object(fn), argname))
            if not argname in list(self._targets.keys()):
                raise blocked_e(
                    "%s @ %s.%s() %s: needs target named '%s', which hasn't "
                    "been declared with the @target class decorator "
                    "(available: %s)"
                    % (self.name, cls.__name__, fname, cls._origin_fn(fn),
                       argname,
                       " ".join([i for i in sorted(self._targets.keys())])))

        # what kind of execution can we do for this method? serial or
        # parallel?
        if len(argnames) == 0:
            # not target specification, so it might use all
            # of them--force serial
            execution_mode = 'serial'
        elif len(argnames) == 1:
            # methods that use only one target can be parallelized
            execution_mode = 'parallel'
        else:
            # methods that use more than one target have to be serial
            execution_mode = 'serial'

        # our guess is overriden by the function's decoration with
        # :func:`tcfl.tc.serially` or :func:`tcfl.tc.concurrently`
        decorated_execution_mode = getattr(fn, "execution_mode", None)
        if decorated_execution_mode:
            # overriden by the user
            execution_mode = decorated_execution_mode

        # the final override comes from...taxaaaan an argument to this
        # function overriding it all?
        if do_serially:
            execution_mode = 'serial'

        # Do not transform into a set, we want to keep the order
        if len(argnames) == 0:
            argnames = None
        if execution_mode == 'serial':
            serial.append((fname, fn, _type, argnames))
        elif execution_mode == 'parallel':
            parallel.append((fname, fn, _type, argnames))
        else:
            raise AssertionError("unknown execution mode '%s'"
                                 % execution_mode)

    @classmethod
    def _origin_fn(cls, fn):
        # Find the origin of a function, being mindful that if it was
        # generated by an App builder, we need to look for the alias
        #
        try:
            return commonl.origin_get_object(fn)
        except IOError as e:
            fname = fn.__name__
            if str(e) == 'source code not available':
                if fname.startswith("__" + cls.__name__) \
                   and any("_%s_for_" % phase in fname \
                           for phase in [ "configure", "build",
                                          "deploy", "clean"]):
                    return "internal app builder"
                return "origin n/a"
            else:
                raise

    def _methods_prepare(self):
        """
        Scan for methods in the testcase that can be used for
        running the diffferent phases.

        Store their information in a way that will be easily
        accessible later by __method_call().
        """
        cls = type(self)
        # Note we need to make sure we create NEW instances of all the
        # class-specific variables that are specific to cls (and not
        # modify the ones we inherit from the base class)
        self._configure_serial = \
            copy.deepcopy(super(cls, cls)._configure_serial)
        self._configure_parallel = \
            copy.deepcopy(super(cls, cls)._configure_parallel)

        self._build_serial = copy.deepcopy(super(cls, cls)._build_serial)
        self._build_parallel = copy.deepcopy(super(cls, cls)._build_parallel)

        self._deploy_serial = copy.deepcopy(super(cls, cls)._deploy_serial)
        self._deploy_parallel = copy.deepcopy(super(cls, cls)._deploy_parallel)

        self._setup_serial = copy.deepcopy(super(cls, cls)._setup_serial)
        self._start_serial = copy.deepcopy(super(cls, cls)._start_serial)
        self._eval_serial = copy.deepcopy(super(cls, cls)._eval_serial)
        self._teardown_serial = copy.deepcopy(super(cls, cls)._teardown_serial)
        # Class teardown inherits the class we inherit from too
        self._class_teardown_serial = copy.deepcopy(
            super(cls, cls)._class_teardown_serial)

        self._clean_serial = copy.deepcopy(super(cls, cls)._clean_serial)
        self._clean_parallel = copy.deepcopy(super(cls, cls)._clean_parallel)

        # We might have added methods later to the
        # instance, not to the class, so inspect the object
        for fname, fn in sorted(inspect.getmembers(self),
                                key = lambda x: x[0]):
            if not callable(fn):
                continue
            if fname.startswith("configure"):
                self.__method_prepare(fname, fn, self._configure_serially,
                                      self._configure_serial,
                                      self._configure_parallel)
            elif fname.startswith("build"):
                self.__method_prepare(fname, fn, self._build_serially,
                                      self._build_serial, self._build_parallel)
            elif fname.startswith("deploy"):
                self.__method_prepare(
                    fname, fn, self._deploy_serially,
                    self._deploy_serial, self._deploy_parallel)
            elif fname.startswith("setup"):
                self.__method_prepare(fname, fn, True,
                                      self._setup_serial, None)
            elif fname.startswith("start"):
                self.__method_prepare(fname, fn, True,
                                      self._start_serial, None)
            elif fname.startswith("eval"):
                self.__method_prepare(fname, fn, True, self._eval_serial, None)
            elif fname.startswith("teardown"):
                self.__method_prepare(fname, fn, True,
                                      self._teardown_serial, None)
            elif fname.startswith("class_teardown"):
                self.__method_prepare(fname, fn, True,
                                      self._class_teardown_serial, None)
            elif fname.startswith("clean"):
                self.__method_prepare(fname, fn, self._clean_serially,
                                      self._clean_serial, self._clean_parallel)
            else:
                continue
        # Now that the (configure|build|deploy|clean)*() methods are
        # classified, we need to check that the ones that are to be
        # run in parallel don't have conflicting arguments (we can't
        # do more than one operation in the same target at the same
        # time). For that we collect all the parallels argument's list
        # and make sets out of that have to be disjuntive
        for method_list in [self._configure_parallel, self._build_parallel,
                            self._deploy_parallel, self._clean_parallel]:
            for fname, fn, _type, argnames in method_list:
                for fname2, fn2, _type2, argnames2 in method_list:
                    if fn == fn2:
                        continue
                    common_argnames = set(argnames) & set(argnames2)
                    decorated_execution_mode1 = getattr(fn, 'execution_mode', None)
                    decorated_execution_mode2 = getattr(fn2, 'execution_mode', None)
                    if decorated_execution_mode1 == 'parallel' \
                       and decorated_execution_mode2 == 'parallel':
                        # do not bother checking, as both has been
                        # decorated as 'have to execute concurrently'
                        # even if they share targets.
                        continue
                    if common_argnames:
                        raise blocked_e(               # FIXME: add origin
                            "%s.%s() (%s) and %s.%s() (%s) conflict in using "
                            "target/s '%s' at the same time (and thus they "
                            "cannot be run in parallel. "
                            "You have to focus actions for a target in "
                            "a single method or run them serially by "
                            "prefixing a @tcfl.tc.serially() decorator."
                            # FIXME: offer fixing methodologies
                            % (cls.__name__, fname,
                               cls._origin_fn(fn),
                               cls.__name__, fname2,
                               cls._origin_fn(fn2),
                               ", ".join(sorted(list(common_argnames)))))
        # And finally, sort the methods alphabetically (the ones that
        # will be run serially and also the parallel in case config
        # says we have to run them serially none-the-less)
        for method_list in [
                self._configure_serial, self._configure_parallel,
                self._build_serial, self._build_parallel,
                self._deploy_serial, self._deploy_parallel,
                self._clean_serial, self._clean_parallel]:
            method_list.sort(key = lambda i: i[0])


    def _mk_target_args_for_fn(self, fn, args):
        """
        Given a list of arguments for a method, translate it into the
        target instantations so it can be called

        When a method declaration requires in its argument list the
        names of the targets it wants (from the testcase's list of
        declared targets), this will pull the actual insances of the
        targets so they can be passed when calling the method.

        :param func fn: function to call
        :param tuple args: list of argument names
        :returns list: list of target_c instances matching each target name
        """
        if args == None:
            return []
        _args = []
        for wanted_target in args:
            if wanted_target in self.target_group.targets:
                _args.append(self.target_group.target(wanted_target))
            else:
                raise blocked_e(
                    "BUG? testcase function `%s.%s` (%s) wants an "
                    "interconnect or target '%s' not present in target "
                    "group `%s` (available: %s)"
                    % (type(self).__name__, fn.__name__,
                       commonl.origin_get_object(fn), wanted_target,
                       self.target_group.name,
                       ", ".join([
                           i for i in list(self.target_group.targets.keys())
                       ])))
        return _args


    def __method_trampoline_call(self, fname, fn, _type, targets):
        # runs a class function and return a return_c
        #
        # convers the return value to return_c from bool, or None, or
        # passes a return_c. Passes exceptions.
        #
        # We use fname instead of fn.__name__ because if we have made
        # an alias out of the method in the class (with setattr(), we
        # want to see the alias name, not the original function name).
        if fname == fn.__name__:
            self.report_info("running %s.%s()"
                             % (type(self).__name__, fn.__name__),
                             dlevel = 3)
        else:
            self.report_info("running %s.%s() [App builder's alias for "
                             "%s() which calls tc_c._*_50_for_target()]"
                             % (type(self).__name__, fname, fn.__name__),
                             dlevel = 3)
        if type(fn) == types.MethodType:	# instance needed
            if fn.__self__ == None:
                # This was added from target_wanted_add(), binding to an
                # specific object; we forced it unbound, so we have to
                # call it with 'self'
                r = getattr(self, fname)(self, *targets)
            else:
                # This was added from the @target() decorator, binding
                # to a class, it doesn't need a self as Python will
                # add it for us, current self.
                r = getattr(self, fname)(*targets)
        else:	# static/classmethod
            r = fn(*targets)
        # Now, let's see what did it return
        if isinstance(r, result_c):
            return r
        return result_c.from_retval(r)


    def __method_trampoline_thread(self, msgid, fname, fn, _type, targets,
                                   tls_parent):
        # runs a function and returns a tuple (result, exception
        # info). If there was no exception, [1] will be None;
        # otherwise it will contain the exception information so it
        # can be re-raised when called from a thread.
        #
        # Because we are in the context of a newly initialized thread,
        # we need to initialize some things
        try:
            with msgid_c(parent = msgid):
                self.__thread_init__(tls_parent)
                return (
                    self.__method_trampoline_call(fname, fn, _type, targets),
                    None)
        except:
            return (None, sys.exc_info())

    @result_c.from_exception
    def _method_run(self, fname, fn, _type, args):
        """
        Implement the guts of _methods_call()

        Runs and aggregates results of serial methods, then launches a
        threadpool for the parallel methods; aggregates their results
        and raise exception if any happened.

        :returns result_c: aggregated result of all the methods.
        """
        targets = self._mk_target_args_for_fn(fn, args)
        return self.__method_trampoline_call(fname, fn, _type, targets)

    def _methods_run(self, do_serially, serial_list, parallel_list):
        """
        Implement the guts of _methods_call()

        Runs and aggregates results of serial methods, then launches a
        threadpool for the parallel methods; aggregates their results
        and raise exception if any happened.

        :returns result_c: aggregated result of all the methods.
        """
        result = result_c(0, 0, 0, 0, 0)
        if do_serially:
            for fname, fn, _type, args in sorted(serial_list + parallel_list):
                retval = self._method_run(fname, fn, _type, args)
                result += retval
                break_on_non_pass = getattr(fn, "break_on_non_pass", True)
                if break_on_non_pass and ( retval.errors or retval.failed
                                           or retval.blocked ):
                    return result
        else:
            for fname, fn, _type, args in serial_list:
                retval = self._method_run(fname, fn, _type, args)
                result += retval
                break_on_non_pass = getattr(fn, "break_on_non_pass", True)
                if break_on_non_pass and ( retval.errors or retval.failed
                                           or retval.blocked ):
                    return result
            # FIXME: set from from config
            # All these are supposedly I/O bound jobs as either they
            # are chitchatting with the network or spawning jobs to do
            # stuff
            thread_pool = _multiprocessing_method_pool_c(processes = 10)
            threads = {}
            count = 0
            for fname, fn, _type, args in parallel_list:
                targets = self._mk_target_args_for_fn(fn, args)
                threads[fname] = thread_pool.apply_async(
                    self.__method_trampoline_thread,
                    (msgid_c(f"{count:02d}"), fname, fn, _type, targets,
                     _simple_namespace(self.tls.__dict__))
                )
                count += 1
                # so we can Ctrl-C right away--the system is designed
                # to cleanup top bottom, with everything being
                # expendable
                threads[fname].daemon = True
            thread_pool.close()
            thread_pool.join()
            for thread in list(threads.values()):
                r = thread.get()
                if r[1] != None:	# re-raise thrown exceptions
                    # Re raise the exception, but with the right traceback
                    # (
                    #     None,
                    #     (
                    #         <class 'subprocess.CalledProcessError'>,
                    #         CalledProcessError(125, ['buildah', 'bud', '-f', '/home/inaky/t/py3-tcf.git/tests/report-hkkcfu.Dockerfile', 'rpyc-test']),
                    #         <traceback object at 0x7f1e0002c080>
                    #     ),
                    # )
                    raise r[1][1].with_traceback(r[1][2])
                result += r[0]
            del thread_pool
        return result


    @result_c.from_exception
    def _methods_call(self, base):
        """
        Call all the methods of a class' instance whose name starts with
        @base.

        Methods who take no arguments (other than classmethods or
        instance method's cls and self) are called serially in
        alphabetical order.

        Methods that take targets as arguments are called in
        parallel (unless the corresponding switch that forces
        serialization is flipped).

        No two methods that take targets might share a target, an
        exception will be raised.

        :returns result_c: aggregated result of all method calls.
        :raises: whatever exception any method raised is passed up
        """
        if base == "configure":
            return self._methods_run(self._configure_serially,
                                     self._configure_serial,
                                     self._configure_parallel)
        elif base == "build":
            return self._methods_run(self._build_serially,
                                     self._build_serial, self._build_parallel)
        elif base == "deploy":
            return self._methods_run(
                self._deploy_serially,
                self._deploy_serial, self._deploy_parallel)
        elif base == "setup":
            return self._methods_run(True, self._setup_serial, [])
        elif base == "start":
            return self._methods_run(True, self._start_serial, [])
        elif base == "eval":
            return self._methods_run(True, self._eval_serial, [])
        elif base == "teardown":
            return self._methods_run(True, self._teardown_serial, [])
        elif base == "class_teardown":
            return self._methods_run(True, self._class_teardown_serial, [])
        elif base == "clean":
            return self._methods_run(self._clean_serially,
                                     self._clean_serial, self._clean_parallel)
        else:
            assert False, "Unknown base '%s'" % base


    #
    # Helpers for the public API
    #
    def _shcmd_local(self, cmd, origin = None, reporter = None,
                     logfile = None, nonzero_e = error_e, env = None):
        """
        Run a single shell command

        :param tc_c _tc: test case
        :param str cmd: shell command to run
        :param str origin: where the shell command was found
        :param dict env: (optional) dictionary of environment
          variables to be passed to :func:`subprocess.check_output`
          (same format as such).
        :return: True if successful, False if failed, None if blocked

        """
        if reporter == None:
            reporter = self
        if origin == None:
            origin = commonl.origin_get(2)
        # This is safe as it runs in the client, as the current user
        # FIXME: we might want to jail it so rogue test cases don't
        # run silly things...
        phase = msgid_c.phase()
        if phase != "":
            output_tag = "%s output" % phase
            ex_trace = "%s trace" % phase
        else:
            output_tag = "output"
            ex_trace = "trace"
        # Make the tag file-name approved -- FIXME: do this in logfiel_open
        if phase:
            _phase = "-" + phase.translate(str.maketrans("/.", "__"))
            phase += " "
        else:
            _phase = ""

        # delete = False, needed so we can open it from other
        # places to report
        if logfile == None:
            logf = commonl.logfile_open(
                tag = _phase + "stdouterr",
                delete = False, directory = self.tmpdir)
        else:
            logf = logfile
        try:
            p = subprocess.Popen([ cmd ], shell = True, close_fds = False,
                                 stdout = logf, stderr = subprocess.STDOUT,
                                 env = env)
            rc = p.wait()
            logf.flush()
            generator_factory = commonl.generator_factory_c(
                commonl.file_iterator, logf.name)
            if rc == 0:
                reporter.report_pass(
                    "%spassed: '%s' @%s" % (phase, cmd, origin),
                    {
                        output_tag: generator_factory
                    })
                logf.seek(0,0)
                return logf.read()
            elif rc == 127:
                raise blocked_e(
                    "exit code %d from '%s' @%s" % (rc, cmd, origin),
                    {
                        output_tag: generator_factory ,
                        "alevel": 0
                    })
            else:
                raise nonzero_e(
                    "exit code %d from '%s' @%s" % (rc, cmd, origin),
                    {
                        output_tag: generator_factory,
                        "alevel": 0
                    })
        except (failed_e, error_e, blocked_e):
            raise
        except Exception as e:
            raise blocked_e(
                "exception ('%s' from %s): %s %s" % (cmd, origin, type(e), e),
                { ex_trace: "".join(traceback.format_tb(sys.exc_info()[2])) })
        finally:
            if logfile == None:
                del logf


    @property
    def target_group(self):
        return self._target_group

    @target_group.setter
    def target_group(self, target_group):
        if target_group != None:
            assert isinstance(target_group, target_group_c)
        self._target_group = target_group
        servers = set()		# We don't care about reps in servers
        if target_group:
            self.targets = target_group.targets
            # Generate a string with the target types to place in
            # the test keywords so it can be reported
            if target_group.targets:
                target_types = []	# Here we want one per target
                # Note this is sorted by target-want-name, the names
                # assigned by the testcase to the targets, so all the
                # types and server lists are sorted by that.
                target_names = []
                for tgname in sorted(target_group.targets.keys()):
                    _target = target_group.targets[tgname]
                    servers.add(_target.rtb.aka)
                    if len(_target.rt.get('bsp_models', {})) > 1:
                        target_type = _target.type + ":" + _target.bsp_model
                    else:
                        target_type = _target.type
                    target_names.append(_target.fullid
                                        + _target.bsp_model_suffix()
                                        + _target.bsp_suffix())
                    target_type = self.type_map.get(target_type, target_type)
                    target_types.append(target_type)
                target_types = ",".join(target_types)
            else:
                target_type = self.type_map.get('static', 'static')
                target_types = target_type
                target_names = [ ]
            target_group_targets = ",".join(target_names)
            target_group_name = target_group.name
            target_group_info = target_group.descr
        else:
            self.targets = {}
            target_group_targets = 'static'
            target_group_name = 'static'
            target_group_info = 'static'
            target_types = "static"

        self.kw_set('target_group_targets', target_group_targets )
        self.kw_set('target_group_info', target_group_info)
        self.kw_set('target_group_name', target_group_name)
        self.kw_set("type", target_types)
        self.kw_set("target_group_types", target_types)
        self.kw_set('target_group_servers', ",".join(servers))
        self._report_mk_prefix()
        return target_group

    def _extra_report(self, kws):
        if self._extra_report_format != None:
            while True:
                try:
                    return " " + self._extra_report_format % kws
                except KeyError as e:
                    # If the key is not available, n/a it -- this can
                    # happen when we have a generic message for a
                    # dynamic test from (let's say) the target or that
                    # is not available in every target; or a static vs
                    # dynamic test, in which there are keys for static
                    # that are not there for dynamic
                    # str(e) is the key that we failed to find
                    kws[str(e)] = "n/a"
                    continue
                except Exception as e:
                    self.log.error("extra-report error: %s", e)
                    return " {bad extra-report format}"
        else:
            return ""

    def _tags_update(self, tags = None):
        if not tags:
            tags = []
        # Tag/s were updated, see if there are any special ones we
        # need to handle
        if not tags:
            tags = list(self._tags.keys())
        for tagname in tags:
            value, origin = self._tags[tagname]
            if tagname == 'build_only' and value == True:
                self.build_only.append('tag:' + origin)

    def tag_set(self, tagname, value = None, origin = None):
        """
        Set a testcase tag.

        :param str tagname: name of the tag (string)
        :param value: (optional) value for the tag; can be a string,
          bool; if none specified, it is set to True
        :param str origin: (optional) origin of the tag; defaults to
          the calling function

        Note that there are a few tags that have speciall conventions:

        - *component/COMPONENTNAME* is a tag with value
          *COMPONENTNAME* and it is used to classify the testcases by
          component. Multiple tags like this might exist if the
          testcase belongs to multiple components. Note it should be a
          single word.

          TCF will create a tag *components* with value *COMPONENTNAME1
          COMPONENTNAME2 ...* (space separated list of components)
          which shall match the *component/COMPONENTx*
          *name* contains the name of the testcase after testcase
          instantiation.
        """

        assert isinstance(tagname, str), (
            "tagname has to be a string, not a %s" % type(tagname).__name__)
        if value == None:
            value = True
        else:
            assert isinstance(value, str) \
                or isinstance(value, bool)
        if origin == None:
            origin = "[builtin default] " + commonl.origin_get(1)
        else:
            assert isinstance(origin, str)
        self._tags[tagname] = (value, origin)
        self._tags_update([ tagname ])

    def tags_set(self, _tags, origin = None, overwrite = True):
        """
        Set multiple testcase tags.

        :param dict tags: dictionary of tags and values
        :param str origin: (optional) origin of the tag; defaults to
          the calling function

        Same notes as for :meth:`tag_set` apply
        """

        for tagname, (value, _origin) in _tags.items():
            assert isinstance(tagname, str), \
                "tagname has to be a string, not a %s" \
                % type(tagname).__name__
            if value == None:
                value = True
            else:
                assert isinstance(value, (str, bool)), \
                    "tag value has to be None (taken as True), bool, " \
                    "string, not a %s" % type(value).__name__
            if origin:
                _origin = origin
            if _origin == None:
                _origin = "[builtin default] " + commonl.origin_get(1)
            if tagname in self._tags and overwrite == False:
                continue
            self._tags[tagname] = (value, _origin)
        self._tags_update(list(_tags.keys()))

    def kw_set(self, key, value, origin = None):
        """
        Set a testcase's keyword and value

        :param str key: keyword name
        :param str value: value
        :param str origin: (optional) where this comes from
        """
        self._kw_set(key, value, origin)

    def kw_unset(self, kw):
        """
        Unset a string keyword for later substitution in commands

        :param str kw: keyword name
        """
        assert isinstance(kw, str)
        if kw in self.kws:
            del self.kws[kw]
        if kw in self.kws_origin:
            del self.kws_origin[kw]

    def kws_set(self, d, origin = None):
        """
        Set a bunch of testcase's keywords and values

        :param dict d: A dictionary of keywords and values
        """
        assert isinstance(d, dict)
        if origin == None:
            o = inspect.stack()[1]
            origin = "%s:%s" % (o[1], o[2])
        else:
            assert isinstance(origin, str)
        for key, value in d.items():
            self._kw_set(key, value, origin)

    class _poll_state_c(object):
        def __init__(self):
            self.lock = threading.Lock()
            self.last_ts = time.time()
            self.buffers = {}

    _poll_state = {}	    # protected by testcase.lock

    def _expect_append(self, run_name, expectations_required, exps,
                       poll_period, exp, name):
        # FIXME: print the origin of this
        assert isinstance(exp, expectation_c), \
            'argument %s is not an instance of expectation_c but %s' \
            % (exp, type(exp).__name__)
        assert name == None or isinstance(name, str)

        # if we forced a name, use it, otherwise use whichever if
        # there; if there is none, create a default
        if name:
            exp.name = name
        elif not exp.name:
            exp.name = '%03d' % (len(exps) - 1)
        # note the poll name does not include the run name. Why?
        # because we want to reuse the poll data from one run to the
        # other insted of re-reading everything; the polling mechanism
        # is supposed to know to append stuff
        exp.poll_name = exp.poll_context()
        poll_period.setdefault(exp.poll_name, 3) # FIXME: default
        if exp.name in [ i.name for i in exps ]:
            raise AssertionError(
                "an expectation named '%s' is already present" % exp.name)
        exps.append(exp)
        if exp.timeout > 0:
            expectations_required.add(exp)
        if exp.poll_period < poll_period[exp.poll_name]:
            self.report_info(
                '%s/%s: reducing poll period from %.2fs to %.2fs for poll '
                'context %s'
                % (run_name, exp.name, poll_period[exp.poll_name],
                   exp.poll_period, exp.poll_name), dlevel = 4)
            poll_period[exp.poll_name] = exp.poll_period
        # Initialize the polling lock for this poll; the polling in
        # most cases is shared by multiple threads in multiple
        # expectations, so we want to do it in a very coordinated
        # way. The poll() method will be called only once and state
        # will be ketp shared.
        with self.lock:
            if exp.poll_name not in self._poll_state:
                self._poll_state[exp.poll_name] = self._poll_state_c()

    def expect_global_append(self, exp, skip_duplicate = False):
        """
        Append an expectation to the testcase global expectation list

        Refer to :meth:`expect` for more information
        """
        assert isinstance(exp, expectation_c), \
            'argument %s is not an instance of expectation_c but %s' \
            % (exp, type(exp).__name__)
        if exp.origin == None:
            exp.origin = commonl.origin_get(2)
        if exp.name in self._expectations_global_names:
            if skip_duplicate:
                return
            else:
                raise blocked_e(
                    "expectation '%s' already exists in testcase "
                    "global list" % exp.name)
        self._expectations_global.append(exp)
        self._expectations_global_names.add(exp.name)

    def expect_global_remove(self, exp):
        """
        Remove an expectation from the testcase global expectation list

        :param str exp: expectation's name
        :param tcfl.tc.expectation_c exp: expectation's instance

        Refer to :meth:`expect` for more information
        """
        assert isinstance(exp, (str, expectation_c)), \
            'argument %s is not an instance of expectation_c or name, but %s' \
            % (exp, type(exp).__name__)
        if isinstance(exp, str):
            # FIXME: this shall be made a dictionary, wth
            for exp_itr in self._expectations_global:
                if exp_itr.name == exp:
                    self._expectations_global_names.remove(exp_itr.name)
                    self._expectations_global.remove(exp_itr)
                    break
        else:
            self._expectations_global_names.remove(exp.name)
            self._expectations_global.remove(exp)

    def expect_tls_append(self, exp):
        """
        Append an expectation to the thread-specific expectation list

        Refer to :meth:`expect` for more information
        """
        assert isinstance(exp, expectation_c), \
            'argument %s is not an instance of expectation_c but %s' \
            % (exp, type(exp).__name__)
        if exp.origin == None:
            exp.origin = commonl.origin_get(2)
        self.tls._expectations.append(exp)

    def expect_tls_remove(self, exp):
        """
        Remove an expectation from the testcase global expectation list

        :param str exp: expectation's name
        :param tcfl.tc.expectation_c exp: expectation's instance

        Refer to :meth:`expect` for more information
        """
        assert isinstance(exp, (str, expectation_c)), \
            'argument %s is not an instance of expectation_c or name, but %s' \
            % (exp, type(exp).__name__)
        if isinstance(exp, str):
            # FIXME: this shall be made a dictionary, wth
            for exp_itr in self.tls._expectations:
                if exp_itr.name == exp:
                    self.tls._expectations.remove(exp_itr)
                    break
        else:
            self.tls._expectations.remove(exp)

    def expect(self, *exps_args, **exps_kws):
        """Wait for a list of things we expect to happen

        This is a generalization of the pattern *expect this string in
        a serial console* where we can wait, in the same loop for many
        things (expectations) from multiple sources such as serial
        console, screen shots, network captures, audio captures, etc...

        Each expectation is an object that implements the
        :class:`expectation_c` interface which indicates how to:

         - poll from a data source

         - detect what we are expecting in the polled data

         - generate collateral for said detected data

        This function will enter into a loop, polling the different
        expectation data sources according to the poll periods they
        establish, then detecting data and reporting the results back
        to the user and raising exceptions if so the user indicates want
        (eg: raise an exception if timeout looking for a shell prompt,
        or raise an exception if a *kernel panic* string is found).

        For example:

        >>> self.expect(
        >>>     name = "waiting for desktop to boot",
        >>>     timeout = 30,
        >>>     text_on_console(target, "Kernel Panic",
        >>>                     name = "kernel panic watchdog",
        >>>                     raise_on_found = tcfl.tc.error_e(
        >>>                         "Kernel Panic found!"),
        >>>     image_on_screenshot(target, 'screen', 'icon-power.png'),
        >>>     config_button = image_on_screenshot(target, 'screen',
        >>>                                         'icon-config.png')
        >>> )

        The first expectation will be called *kernel panic watchdog*
        and will raise an exception if the console print a
        (oversimplified for the sake of the example) kernel panic
        message. If not found, nothing happens.

        The second will default to be called whatever the
        :class:`target.capture.image_on_screenshot
        <tcfl.target_ext_capture.extension.image_on_screenshot>`
        calls it (*icon-power.png*), while the second will have it's
        name overriden to *config_button*. These last two will capture
        an screenshot from the target's screenshot capturer called
        *screen* and the named icons need to be found for the call to
        be succesful. Otherwise, error exceptions due to timeout will
        be raised.

        The list of expectations that will be always scanned is in
        this order:

        - testcase's global list of expectations (add with
          :meth:`expect_global_append`)

        - testcase's thread specific list of expectations (add with
          :meth:`expect_tls_append`)

        - list of expectations in the arguments

        :param expectation_c exps_args: expectation objects which are
          expected to be self-named (their implementations will assign
          names or a default will be given). eg:

          >>> self.expect(tcfl.tc.tc_c.text_on_console(args..etc),
          >>>             tcfl.tc.tc_c.image_on_screenshot(args..etc))

        :param expectation_c exps_kws: expectation objects named after
          the keyword they are assigned to; note the keywords *name*
          and *timeout* are reserved. eg:

          >>> self.expect(
          >>>     shell_prompt = tcfl.tc.tc_c.text_on_console(args..etc),
          >>>     firefox_icon = tcfl.tc.tc_c.image_on_screenshot(args..etc)
          >>> )

        :param int timeout: Maximum time of seconds to wait for all
          non-optional expectations to be met.

          >>> timeout = 4

         if no timeout is given it is taken from a testcase specific
         default stored int he thread specific variable
         ``testcase.tls.expect_timeout`` (60s).

        :param str name: a name for this execution, used for reporting
          and generation of collateral; it defaults to a test-specific
          monotonically increasing number shared amongst all the
          threads running in this testcase. eg:

          >>> name = "shell prompt received"

        :param str origin: (optional) when reporting information about
          this expectation, what origin shall it list, eg:

          - *None* (default) to get the current caller
          - *commonl.origin_get(2)* also to get the current caller
          - *commonl.origin_get(1)* also to get the current function

          or something as:

          >>> "somefilename:43"

        If no expectations are given, then this just waits the given
        timeout. Likewise, if all the individual expectations are
        given with *timeout = 0* (meaning if we don't get it, it is ok
        too), this will wait the given timeout.

        """
        origin = exps_kws.pop('origin', None)
        if origin == None:
            origin = commonl.origin_get(2)
        # where detectors store detection data, keyed by expectation
        buffers = dict()
        # where results are dumped, keyed by expectation
        results = dict()

        if 'name' in exps_kws:
            name = exps_kws['name']
            del exps_kws['name']
            assert isinstance(name, str), \
                "argument 'name' has to be a string, found %s" \
                % type(name).__name__
        else:
            name = ""

        if 'timeout' in exps_kws:
            assert exps_kws['timeout'] > 0
            timeout = exps_kws['timeout']
            del exps_kws['timeout']
        else:
            timeout = self.tls.expect_timeout

        with self.lock:
            run_name = '%02d' % self._expect_count
            self._expect_count += 1
        if name:
            run_name += "." + commonl.name_make_safe(name)

        exps = []	# has to be a list to maintain declaration order
        poll_period = dict()
        # these are the expectations that are required to be detected
        # in some form; it is a subset of them all because there might
        # be some we only want to detect optionally (timeout == 0)
        expectations_required = set()

        # at the end add the thread specific expectations, then the
        # global ones -- we always check first the ones from the
        # arguments
        for exp in self._expectations_global:
            self._expect_append(run_name, expectations_required, exps,
                                poll_period, exp, None)

        for exp in self.tls._expectations:
            self._expect_append(run_name, expectations_required, exps,
                                poll_period, exp, None)

        # read in all the expectations without name from *exps_args, the
        # ones with names in **exps_kws
        for exp in exps_args:
            assert isinstance(exp, expectation_c), \
                "%s: expected type expectation_c, got %s" % (exp, type(exp))
            if exp.origin == None:
                exp.origin = origin
            self._expect_append(run_name, expectations_required, exps,
                                poll_period, exp, None)

        # kws is at this point just a named list of expectations and
        # their implementation
        for exp_name, exp in exps_kws.items():
            assert isinstance(exp, expectation_c), \
                "%s: expected type expectation_c, got %s" % (exp_name, type(exp))
            if exp.origin == None:
                exp.origin = origin
            self._expect_append(run_name, expectations_required, exps,
                                poll_period, exp, exp_name)


        expectations_pending = list(exps)

        time_ts0 = time.time()
        time_ts = time_ts0
        time_out = time_ts0 + timeout
        detect_ts = dict()
        if poll_period:
            min_poll_period = min(poll_period.values())
        else:
            min_poll_period = 0.25	# yeah, in case we get an empty list
        self.report_info('%s: poll_period %.2f, timeout %.2f'
                         % (run_name, min_poll_period, timeout), dlevel = 5)
        try:
            # if we have no expectations or the ones we have have
            # timeout == 0, then we want to make sure the outer
            # timeout rules
            expectations_present = len(expectations_required)
            while time_ts <= time_out:
                # iterate over this copy, we'll remove from the original,
                # so next iteration doesn't take the original; respect
                # the original order
                _expectations_pending = list(expectations_pending)
                for exp in _expectations_pending:
                    poll_context = exp.poll_context()
                    time_ts = time.time()
                    # if it is the first run, set these timestamps so
                    # we kick in a poll
                    detect_ts.setdefault(exp.name, time_ts)
                    buffers.setdefault(exp.name, dict())
                    with self.lock:
                        poll_state = self._poll_state[exp.poll_name]
                    if exp.target:
                        reporter = exp.target
                    else:
                        reporter = self

                    # Poll the expectation if the poll period has
                    # ellapsed; remember we might be sharing this poll
                    # source between multiple expectations in multiple
                    # threads -- hence why the polling is testcase
                    # global and we lock it per poll_name
                    with poll_state.lock:
                        ellapsed = time_ts - time_ts0
                        last_poll_ts = poll_state.last_ts
                        poll_ellapsed = time_ts - last_poll_ts
                        # == 0 -> first time we run, so poll
                        if exp.poll_name not in poll_period:
                            # sometimes this hits and it is still not
                            # clear why; funny thing is nothing really
                            # happens, since it checks later and it
                            # works ok...
                            self.log.error(
                                "BUG: exp %08x name %s context %s"
                                " poll_name %s is MISSING"
                                " from poll_period %s: %s;"
                                " this happens when multiple"
                                " expectation that poll the same"
                                " source had the same name"
                                %  (
                                    id(exp), exp.name,
                                    exp.poll_context(), exp.poll_name,
                                    poll_period,
                                    "".join(traceback.format_stack())
                                )
                            )
                        else:
                            exp_poll_period = poll_period[exp.poll_name]
                            if poll_ellapsed == 0 \
                               or poll_ellapsed >= exp_poll_period:
                                reporter.report_info(
                                    '%s/%s: polling on %s @%.1f/%.1fs '
                                    '(%.1fs ellapsed on a %.1fs period)'
                                    % (run_name, exp.name, poll_context,
                                       ellapsed, timeout, poll_ellapsed,
                                       exp_poll_period), dlevel = 3)
                                exp.poll(self, run_name, poll_state.buffers)
                                poll_state.last_ts = time_ts
                                last_poll_ts = time_ts	# we need this below
                            else:
                                reporter.report_info(
                                    '%s/%s: not polling on %s @%.1f/%.1fs '
                                    '(only %.1fs ellapsed on a %.1fs period)'
                                    % (run_name, exp.name, poll_context,
                                       ellapsed, timeout, poll_ellapsed,
                                       exp_poll_period), dlevel = 3)
                    # Detect this expectation from the poll -- only if
                    # there has been a more recent poll
                    # If it detection is succesful, we are done, remove it
                    # from the list. Check if it times out (each
                    # expectation can timeout shorter than the global
                    # timeout)
                    last_detect_ts = detect_ts[exp.name]
                    last_detect_ellapsed = last_poll_ts - time_ts
                    if last_detect_ellapsed == 0 \
                       or last_detect_ts < last_poll_ts:
                        reporter.report_info(
                            "%s/%s: detecting expectation on '%s' "
                            "@%.1f/%.1fs (last detect %.1fs before "
                            "last poll on %.1fs)"
                            % (run_name, exp.name, poll_context, ellapsed,
                               timeout, last_detect_ts, last_poll_ts),
                            dlevel = 3)
                        with poll_state.lock:
                            r = exp.detect(self, run_name, poll_state.buffers,
                                           buffers[exp.name])
                        if r:
                            assert r == None or isinstance(r, dict), \
                                "%s/%s: expect returned an unexpected" \
                                " type '%s'; expected *None* or dictionary" \
                                % (run_name, exp.name, r)
                            # this is done
                            results[exp.name] = r
                            expectations_pending.remove(exp)
                            if exp in expectations_required:
                                expectations_required.remove(exp)
                            with poll_state.lock:
                                exp.on_found(run_name, poll_context,
                                             poll_state.buffers, buffers,
                                             ellapsed, timeout, r)
                        if exp.timeout > 0 and ellapsed > exp.timeout:
                            # timeout for this specific expectation
                            with poll_state.lock:
                                exp.on_timeout(run_name, poll_context,
                                               poll_state.buffers, buffers,
                                               ellapsed, timeout)
                        detect_ts[exp.name] = time_ts
                    else:
                        reporter.report_info(
                            '%s/%s: not re-detecting on %s (no new poll) '
                            '@%.1f/%.1fs (last detect %.1fs after last poll '
                            'on %.1fs)'
                            % (run_name, exp.name, poll_context, ellapsed,
                               timeout, last_detect_ts, last_poll_ts),
                            dlevel = 3)

                # if all the required expectations are done, get out!
                if expectations_present and not expectations_required:
                    break
                time.sleep(min_poll_period)
                time_ts += min_poll_period
            if time_ts > time_out:
                ellapsed = time_ts - time_ts0
                # timeout exceeded; look at the expectations we
                # required to pass, sort them by timeout and raise the
                # one with the lowest timeout and exception...or do nothing
                for exp in sorted(expectations_required,
                                  key = lambda exp: exp.timeout):
                    if exp.timeout <= 0:
                        continue
                    # timeout for this specific expectation
                    with self.lock:
                        poll_state = self._poll_state[exp.poll_name]
                    with poll_state.lock:
                        exp.on_timeout(run_name, poll_context,
                                       poll_state.buffers,
                                       buffers.get(exp.name, {}),
                                       ellapsed, timeout)

        except Exception as e:
            # Left over here for when fast debug is needed; this will
            # print a lot of extra data in every call and is not
            # always needed.
            # result_c.report_from_exception(self, e)
            raise
        finally:
            # we got them all, return the results dictionary
            time_ts = time.time()
            self.report_info('%s: completed after @%.1fs, found %d matches'
                             % (run_name, time_ts - time_ts0,
                                len(results)), dlevel = 3)

            # flush the expectations
            # this will generate collaterals as needed
            for exp in exps:
                try:
                    with self.lock:
                        poll_state = self._poll_state[exp.poll_name]
                    with poll_state.lock:
                        exp.flush(self, run_name,
                                  poll_state.buffers,
                                  buffers.get(exp.name, {}),
                                  results.get(exp.name, {}))
                except Exception as e:
                    result_c.report_from_exception(
                        self, e, dict(
                            context = "flushing expectation %s/%s" % (
                                run_name, exp.name)
                        )
                    )
                    # do not re-raise this, this can be the raise path
                    # of an exception in our main body
                    # FIXME: detect if we are not in a raise path and
                    # raise then?

        return results

    def threaded(self, fn):
        """
        Decorate a testcase method so it can be used with
        :meth:`run_for_each_target_threaded`.

        Use in a test class as described in
        :meth:`run_for_each_target_threaded`'s examples.
        """
        def _decorated_fn(*args, **kwargs):
            try:
                # the caller has added these extra parameters we use
                # to initialize the TLS for the thread, inheriting
                # values--we remove them from the args list, so the
                # calling later doesn't complain about extra
                # unexpected parameters.
                # args[-1] is always kwargs
                parent_msgid = kwargs.pop('parent_msgid', None)
                parent_tls = kwargs.pop('parent_tls', None)
                with msgid_c(parent = parent_msgid) as msgid:
                    # since we are starting a new thread, lower the
                    # depth once to account for this msgid object we
                    # created; note, however, that if this was called
                    # over standalone code, we might be at zero depth
                    # already, so compensate for that.
                    msgid._depth -= 1
                    if msgid._depth <= 0:
                        msgid._depth = 1
                    self.__thread_init__(parent_tls)
                    result = fn(*args, **kwargs)
                    return result, None
            except:
                return None, sys.exc_info()
        # this sets a flag in the decorator function so we can verify
        # later it has been decorated before trying to use it
        setattr(_decorated_fn, "func_dict", { 'threaded_decorator': True })
        return _decorated_fn

    def run_for_each_target_threaded(
            self, function, args = None, kwargs = None,
            targets = None, processes = None, re_raise_exceptions = True):
        """
        Executes a function in parallel for each listed target waiting
        for them to finish

        By default, all of the testcase target's are used; (eg) to power
        cycle all targets in parallel (note the use of the
        :meth:`self.threaded <threaded>` decorator):

        >>> @tcfl.tc.target(count = 10)
        >>> class _test(tcfl.tc.tc_c):
        >>>
        >>>     def start(self):
        >>>         @self.threaded
        >>>         def _target_start(target):
        >>>             target.power.cycle()
        >>>
        >>>         self.run_for_each_target_threaded(_target_start)

        if you have your own list of targets:

        >>> @tcfl.tc.interconnect()
        >>> @tcfl.tc.target(name = "server")
        >>> @tcfl.tc.target(name = "client1")
        >>> @tcfl.tc.target(name = "client2")
        >>> @tcfl.tc.target(name = "client3")
        >>> class _test(tcfl.tc.tc_c):
        >>>
        >>>     clients = [ "client1", "client2", "client3" ]
        >>>
        >>>     def start(self, ic):
        >>>         ic.power.on()
        >>>         server.power.cycle()
        >>>         # do some server setup...
        >>>         @self.threaded
        >>>         def _client_start(client):
        >>>             client.power.cycle()
        >>>             client.shell.up(user = 'root')
        >>>
        >>>         self.run_for_each_target_threaded(_client_start,
        >>>                                           targets = clients)

        :param function: function to execute; this has to be a
          function decorated with *@:func:`self.threaded
          <tcfl.tc.tc_c.threaded>`*; it must take as first argument a
          :class:`tcfl.tc.target_c` object, which will be the target
          it is running for.

          The rest of the arguments will be the ones supplied with
          *args* and *kwargs*.

        :param args: (optional, default none) extra arguments to
          pass to each function call

        :param dict kwargs: (optional, default none) extra keyword
          arguments to pass to each function call

        :param list(str) targets: (optional, default all testcase's
          targets) list of targets on which to launch the function
          (each on a thread); this is a list of the strings, each
          being a target role (old *target want name*), the names
          assigned to the targets on the *@tcfl.tc.interconnect* or
          *@tcfl.tc.target* decorators.

          # FIXME: this should take the target object too, now it is quite weird

        :param int processes: (optional, default as many as targets)
          how many threads to run in parallel

        :param bool re_raise_exceptions: if any function raises an
          exception, reraise when all are done.

        :returns: dictionary keyed by target role of tuples containing
          the *( result, exception )* function's return value and
          information about exception raised (if it was raised, *None*
          otherwise).

          Exception information is a tuple as returned by
          :func:`sys.exc_info`.

        """
        assert args == None or isinstance(args, tuple)
        assert kwargs == None or isinstance(kwargs, dict)
        assert targets == None or isinstance(targets, list)
        assert processes == None or processes > 0
        assert isinstance(re_raise_exceptions, bool)
        assert hasattr(function, "func_dict") \
            and function.func_dict.get("threaded_decorator", False) == True, \
            "%s: function has to be decorated with @tcfl.tc.tc_c.threaded"

        if targets == None:
            targets = self.target_group.targets.keys()
        if processes == None:
            processes = len(targets)
        threads = {}
        if args == None:
            args = ( )
        if kwargs == None:
            kwargs = {}

        thread_pool = _multiprocessing_method_pool_c(processes = processes)
        for target_role in targets:
            target = self.target_group.targets[target_role]
            kwargs['parent_tls'] = _simple_namespace(self.tls.__dict__)
            kwargs['parent_msgid'] = msgid_c()
            threads[target_role] = \
                thread_pool.apply_async(function, (target, ) + args, kwargs, )

        thread_pool.close()
        thread_pool.join()
        results = {}
        for role, thread in threads.items():
            result, exception = thread.get()
            if exception and re_raise_exceptions:
                # we get `exception` from sys.exc_info(), which returns the
                # old-style representation of the handled exception,
                # looks something like `(type(e), e, e.__traceback__)`.
                # we only care on raising `e` -- the exception value -- because
                # it has all needed info. So we take #1.
                raise exception[1]
            results[role] = ( result, exception )
        return results


    #
    # Helpers for private APIs
    #

    def tag_get(self, tagname, value_default, origin_default = None):
        """
        Return a tuple (value, origin) with the value of the tag and
        where it was defined.
        """

        if origin_default == None:
            origin_default = "[builtin default] " + commonl.origin_get(1)
        return self._tags.get(tagname, (value_default, origin_default))

    def _prefix_update(self):
        """
        Update the prefix we use for the logging/reports when some
        parameter changes.
        """
        self._report_mk_prefix()
        self.log.prefix = " " + self.report_mk_prefix()
        # Replace the logger's format based on the prefix
        self.log.id = id(self)

    def _phase_skip(self, phase):
        """
        Determine if a phase has to be skipped

        :returns bool: True if @phase has to be skipped, False otherwise.
        """
        retval = False
        if not phase in self._phases:
            self.report_info("phase %s not in [%s]"
                             % (phase, ', '.join(list(self._phases.keys()))),
                             dlevel = 5)
            retval = True
        if phase in self._phases_skip:
            self.report_info("phase %s skipped due to [%s]" %
                             (phase, ", ".join(self._phases_skip[phase])),
                             dlevel = 5)
            retval = True
        return retval

    def _kw_set(self, kw, value, origin = None):
        """
        Set a string keyword for later substitution in commands

        :param str kw: keyword name
        :param str value: value for the keyword
        :param str origin: origin of this setting; if none, it will be
          taken from the stack
        """
        assert isinstance(kw, str)
        assert isinstance(value, (str, int)), \
                "value: expected str|int, got %s: %s" % (type(value).__name__, value)
        if origin == None:
            o = inspect.stack()[1]
            origin = "%s:%s" % (o[1], o[2])
        else:
            assert isinstance(origin, str)
        self.kws[kw] = value
        self.kws_origin.setdefault(kw, []).append(origin)


    def _components_fixup(self):
        # Based on tags called component/COMPONENTNAME define a tag
        # called "components" with the list of component names for
        # this testcase
        componentl = []
        for tag_name in self._tags:
            if tag_name.startswith('component/'):
                componentl.append(tag_name[len('component/'):])
        self.tag_set("components", " ".join(componentl), commonl.origin_get(1))

    #
    # Remote target acquisition
    #

    #: Maximum time (seconds) to wait to succesfully acquire a set of targets
    #:
    #: In heavily contented scenarios or large executions, this
    #: becomes oversimplistic and not that useful and shall be
    #: delegated to something like Jenkins timing out after so long
    #: running things.
    #:
    #: In that case set it to over that timeout (eg: 15 hours); it'll
    #: keep trying to assign until killed; in a tcf :ref:`configuration
    #: file <tcf_client_configuration>`, add:
    #:
    #: >>> tcfl.tc.tc_c.assign_timeout = 15 * 60 * 60
    #:
    #: or even in the testcase itself, before it assigns (in build or
    #: config methods)
    #:
    #: >>> self.assign_timeout = 15 * 60 * 60
    assign_timeout = 1000

    # FIXME: add phase "assign"
    @contextlib.contextmanager
    def _targets_assign(self):

        # targets have been already assigned, there is an allocid that
        # can be used
        if self.allocid:
            yield
            return

        timeout = self.assign_timeout
        period = assign_period

        if not self.do_acquire or self.is_static():
            yield
            return

        acquired = []
        pending = []
        for target in list(self.target_group.targets.values()):
            if target.do_acquire:
                pending.append(target)
        if not pending:
            yield
            return

        # to use fullid, need to tweak the refresh code to add the aka part
        rtbs = set()
        for target in pending:
            rtbs.add(target.rtb)

        # FIXME: move to a multiple server model
        if len(rtbs) > 1:
            raise blocked_e(
                "Targets span more than one server",
                dict(
                    targets = [ target.fullid for target in pending ],
                    servers = [ str(rtb) for rtb in rtbs ],
                ))
        rtb = list(rtbs)[0]

        # If any of the targets declare it takes it a long time to
        # power up, consider that.
        target_max_power_up_time = max(
            pending, key = lambda target: target.rt.get('power_up_time', 0))
        timeout += target_max_power_up_time.rt.get('power_up_time', 0) * 5

        # Increase the timeout based on how many testcases are waiting
        # for it from us FIXME this can't take into account other
        # clients or how long tests are expected to last; thus, we
        # need a better fix
        timeout *= 1 + tc_c._tcs_total / 3.0

        # ensure we have an entry for this server in the list of
        # _allocids we have to remove if we Ctrl-C
        _allocids.setdefault(rtb, set())
        # FIXME: use commonl.progress("")
        register_allocids = set()
        allocid, state, targetids = target_ext_alloc._alloc_targets(
            rtb,
            { "group": [ target.id for target in pending ] },
            obo = self.obo,
            preempt = self.preempt,
            priority = self.priority,
            queue_timeout = timeout,
            reason = self.reason % commonl.dict_missing_c(self.kws),
            register_at = register_allocids)
        if state != 'active':
            # FIXME: this need sto carry more data, we've lost a lot
            # on the way here
            raise blocked_e("allocation failed, state %s" % state)
        if register_allocids:
            with _allocids_mutex:
                _allocids[rtb].update(register_allocids)
        self.allocid = allocid
        self.report_info(
            "acquired %s" % allocid,
            dict(
                targets = targetids,
                servers = [ str(rtb) for rtb in rtbs ],
            ),
            dlevel = 5)

        self._prefix_update()
        try:
            # ensure we have a thread running that is, every ten
            # seconds, refreshing all the allocation IDs held by this
            # process and then yield to the executor part (remember
            # this function is a decorator).
            with commonl.thread_periodical_runner_c(_allocids_keepalive_once,
                                                    period = 10):
                yield
        finally:
            self._prefix_update()
            if tc_c.release:
                with _allocids_mutex:
                    if allocid in _allocids[rtb]:
                        # remove from the Ctrl-C list? it might have been
                        # removed already
                        _allocids[rtb].remove(allocid)
                # but just in case do it in the server too
                target_ext_alloc._delete(rtb, allocid)
                self.report_info(
                    "released",
                    dict(
                        targets = targetids,
                        servers = [ str(rtb) for rtb in rtbs ],
                    ),
                    dlevel = 5)
            else:
                self.report_info("WARNING!! not releasing targets",
                                 dlevel = -10)
                self.report_info(
                    "acquired",
                    dict(
                        targets = targetids,
                        servers = [ str(rtb) for rtb in rtbs ],
                    ),
                    dlevel = 5)


    def targets_active(self, *skip_targets):
        """
        Mark each target this testcase use as being used

        This is to be called when operations are being done on the
        background that the daemon can't see and thus consider the
        target active (e.g.: you are copying a big file over SSH)

        >>> class mytest(tcfl.tc.tc_c):
        >>>     ...
        >>>     def eval_some(self):
        >>>         ...
        >>>         self.targets_active()
        >>>         ...

        If any target is to be skipped, they can be passed as arguments:

        >>> @tcfl.tc.interconnect()
        >>> @tcfl.tc.target()
        >>> @tcfl.tc.target()
        >>> class mytest(tcfl.tc.tc_c):
        >>>     ...
        >>>     def eval_some(self, target):
        >>>         ...
        >>>         self.targets_active(target)
        >>>         ...
        """
        # in practical matters, now all targets are in the same
        # server, but in the future that won't be the case
        # Also in the future, the allocation thread will take care of
        # doing this on its own and this function will be a nil
        servers = collections.defaultdict(dict)
        for target in list(self.targets.values()):
            if not target in skip_targets and target.keep_active:
                servers[target.rtb][self.allocid] = "active"
        # FIXME: paralellize
        for rtb, allocids in servers.items():
            rtb.send_request("PUT", "keepalive", json = allocids)

    def _targets_active(self):
        # backwards compat
        self.targets_active()

    #
    # Remote target selection
    #
    # Given all the list of available, this helps _run_on_targets() to
    # generate the minimum set of unique permutations of interconnects
    # and targets where we can / need to run the TC for optimal
    # coverage.
    #
    # _target_wants_find_candidates() finds candidate remote targets
    # _target_want_list_permutations() computes the permutations of
    # wanted targets with suitable candidate remote targets
    #
    # _target_wants_find_candidates()
    #   _target_want_find_candidates()
    #     _target_want_rt_candidates()
    #       _targets_select_by_spec()
    #       _targets_select_by_app()
    #
    def _target_select_by_app(self, target_want_name, target_want,
                              rt, bsp_model, origin):
        rt_full_id = rt['fullid']
        rt_type = rt['type']

        if 'app' not in target_want or not target_want['app']:
            self.report_info(
                "target '%s': %s:%s (type:%s): candidate by App builder; "
                "there are no App builder specifications, so have no say @%s"
                % (target_want_name, rt_full_id, bsp_model, rt_type,
                   origin),
                dlevel = 7)
            return True

        wanted_bsps = set(target_want['app'].keys())
        wanted_bsp_count = len(wanted_bsps)
        if 'bsp_models' in rt:
            bsps = rt['bsp_models'][bsp_model]
        else:
            bsps = []
        if bsps == None:
            bsps = set([bsp_model])
        else:
            bsps = set(bsps)
        bsp_count = len(bsps)
        if bsp_count != wanted_bsp_count:
            self.report_info(
                "target '%s': %s:%s (type:%s): ignoring by App builder; "
                "BSP count %d doesn't match needed %d by app_* constraints "
                "set in @%s"
                % (target_want_name, rt_full_id, bsp_model, rt_type,
                   bsp_count, wanted_bsp_count, origin),
                dlevel = 8)
            return False

        # If we have a list of BSPs we need to match, make sure the
        # remote target has them. The list of wanted BSPs comes from
        # the ones we might have configured by specifying apps
        # to the @target decorator.
        # Note that per definition, @target allows only ONE wildcard
        # BSP, for the case when a single BSP/app is specified
        # and we don't care to specify the BSP.
        if "*"  in wanted_bsps:
            # This could never happen, but be anal
            assert wanted_bsp_count == 1
            # the first check above in the count has made sure we are
            # only asking for a target with as many BSPs as the ones
            # we are asking for in the wanted section. By means of
            # having a "*" BSP, it ensures we only want one BSP, so we
            # know this is a match
            self.report_info(
                "target '%s': %s:%s (type:%s): candidate by App buider; "
                "wildcard BSP in app matches single BSP "
                "target/BSP-model @%s"
                % (target_want_name, rt_full_id, bsp_model, rt_type, origin),
                dlevel = 7)
            return True

        # If we get here, we have a list of wanted BSPs in the
        # app specification of the target/want. We also have a
        # list of BSPs the remote target/bsp_model combination
        # offers. We should have a one-on-one mapping
        orphan_bsps = bsps - wanted_bsps
        if orphan_bsps:
            self.report_info(
                "target '%s': %s:%s (type:%s): ignoring by App builder; "
                "BSPS %s are left with no build information with "
                "the current app_* information "
                "(you might want to specify 'app_manual' if those "
                "are going to be built manually)"
                " @%s"
                % (target_want_name, rt_full_id, bsp_model, rt_type,
                   ",".join(list(orphan_bsps)), origin),
                dlevel = 8)
            return False
        orphan_wanted_bsps = wanted_bsps - bsps
        if orphan_wanted_bsps:
            self.report_info(
                "target '%s': %s:%s (type:%s): ignoring by App builder; "
                "BSPS %s specified to build in @%s are not exposed "
                "by the target"
                % (target_want_name, rt_full_id, bsp_model, rt_type,
                   origin, ",".join(list(orphan_bsps))),
                dlevel = 8)
            return False
        # I guess there is a match
        self.report_info(
            "target '%s': %s:%s (type:%s):  candidate by App builder; "
            "selected by app_* specifications on BSP count match "
            "(%d) and serviced BSPs @%s"
            % (target_want_name, rt_full_id, bsp_model, rt_type,
               bsp_count, origin),
            dlevel = 7)
        return True

    def _targets_select_by_spec(self, target_want_name,
                                rt, bsp_model, spec, origin,
                                _kws = None):
        if not _kws:
            _kws = {}
        # We are going to modify the _kws dict, so make a copy!
        kws = dict(_kws)
        # Given a remote target and BSP-model and a filtering spec,
        # iterate over the BSPs that apply to said BSP-model and see
        # where the spec matches -- if so, consider that rt/bsp-model
        # as candidate for the target need specified by the testcase in
        # target_want.
        #
        # For each spec that matches, then we are good to consider
        # that rt/bsp-model combination as candidate, we don't need to
        # keep iterating over BSPs
        #
        # Note it does not matter if we select the same remote target
        # in two different target_wants -- the creator of the target
        # groups will discard permutations that are not valid.
        if bsp_model:
            kws['bsp_model'] = bsp_model
            if 'bsp_models' in rt:
                bsps = rt['bsp_models'][bsp_model]
                if bsps == None:
                    bsps = [ bsp_model ]
            else:
                bsps = []
        else:
            bsps = []
            kws['bsp_model'] = ""
        kws['bsp_count'] = len(bsps)
        kws_bsp = dict()
        # we use to evaluate the spec both the flattened rt and the nested one
        commonl.kws_update_from_rt(kws, rt)
        for key, val in commonl.dict_to_flat(rt,
                                             sort = False, empty_dict = True):
            kws[key] = val
        rt_full_id = rt['fullid']
        rt_type = rt['type']

        for bsp in bsps:
            kws_bsp.clear()
            kws_bsp.update(kws)
            kws_bsp['bsp'] = bsp
            if 'bsps' in rt:
                commonl.kws_update_type_string(kws_bsp, rt['bsps'][bsp])
            self.report_info(
                "target '%s': %s:%s/%s (type:%s): considering by spec"
                % (target_want_name, rt_full_id, bsp_model, bsp, rt_type),
                dlevel = 9)
            if commonl.conditional_eval("target selection", kws_bsp,
                                        spec, origin, kind = "specification"):
                # This remote target matches the specification for
                # this target want
                self.report_info(
                    "target '%s': %s:%s/%s (type:%s): candidate by spec"
                    % (target_want_name, rt_full_id, bsp_model, bsp, rt_type),
                    dlevel = 7)
                return True
            else:
                self.report_info(
                    "target '%s': %s:%s/%s (type:%s): ignoring by spec; "
                    "didn't match '%s'"
                    % (target_want_name, rt_full_id, bsp_model, bsp,
                       rt_type, spec),
                    dlevel = 8)
        if bsps == []:
        # If there are no BSPs, just match on the core keywords
            # No match on BSPs? try without a BSP
            if commonl.conditional_eval("target selection", kws,
                                        spec, origin, kind = "specification"):
                # This remote target matches the specification for
                # this target want
                self.report_info(
                    "target '%s': %s:%s (type:%s): candidate by spec w/o BSP"
                    % (target_want_name, rt_full_id, bsp_model, rt_type),
                    dlevel = 7)
                return True
            else:
                self.report_info(
                    "target '%s': %s:%s (type:%s): ignoring by spec w/o BSP; "
                    "didn't match '%s'"
                    % (target_want_name, rt_full_id, bsp_model,
                       rt_type, spec),
                    dlevel = 8)
                return False

    def _target_want_rt_candidate(self, target_want_name, target_want,
                                  rt_full_id, bsp_model, kws = None):
        if not kws:
            kws = {}
        # Decide if a remote target and BSP model shall be considered
        # by a testcase's want of a target
        rt = dict(self.rt_all[rt_full_id])
        # update the flat dictionaries FIXME: this is now a quick hack
        # which we are doing in the most non-efficient way--ideally we
        # shall have only one place where we do this at target
        # discovery time.
        for key, val in commonl.dict_to_flat(
                rt, sort = False, empty_dict = True):
            if key not in rt:
                # only update if we would not be overriding an
                # existing key (eg: flat and non flat names match, or
                # this is a dictionary in the non-flat equivalents)
                rt[key] = val
        rt_type = rt['type']
        spec = target_want.get('spec', None)
        if spec == None:
            wanted_by_specs = True
        elif isinstance(spec, str):
            wanted_by_specs = self._targets_select_by_spec(
                target_want_name, rt, bsp_model,
                spec, target_want['origin'], _kws = kws)
        elif callable(spec):
            wanted_by_specs = spec(
                target_want_name, target_want,
                rt, bsp_model,
                spec, target_want['origin'])
        else:
            raise blocked_e("Unknown type %s for target_want['spec'] @%s"
                            % (type(spec).__name__, target_want['origin']))
        if not wanted_by_specs:
            return False
        wanted_by_app = self._target_select_by_app(
            target_want_name, target_want,
            rt, bsp_model, target_want['origin'])

        # If the user specified a spec (filter or function)
        # and it conflicts with what the App builder says,
        # that's something is not ok.
        if spec != None and not wanted_by_app:
            wanted_bsps = set(target_want['app'].keys())
            if 'bsp_models' in rt:
                bsps = rt['bsp_models'][bsp_model]
            else:
                bsps = []
            if bsps == None:
                bsps = set([bsp_model])
            else:
                bsps = set(bsps)
            orphan_bsps = bsps - wanted_bsps
            if orphan_bsps:
                orphan_bsps_str = \
                    ". Missing app assignment to " \
                    "BSPs " + " ".join(list(orphan_bsps)) + \
                    "; use `app_manual` for those who will " \
                    "be manually configured/built/deployed... " \
                    "in configure/build/... methods"
            else:
                orphan_bsps_str = ""
            self.report_info(
                "target '%s': %s:%s (type:%s): can't use; "
                "conditions imposed by the spec @%s conflict "
                "with the App builder's"
                % (target_want_name, rt_full_id, bsp_model,
                   rt_type, target_want['origin'])
                +  orphan_bsps_str, dlevel = 3)
            return False

        if not wanted_by_app:
            return False

        return True

    def _target_want_find_candidates(self, target_want_name,
                                     target_want, rts_bsp_models,
                                     kws):
        candidates = set()
        # Note the shuffling -- we want to access in random order
        # so we spread access around all the selected targets in
        # case we only choose one of each type
        rts_bsp_models = random.sample(rts_bsp_models, len(rts_bsp_models))
        for rt_full_id, bsp_model in rts_bsp_models:
            if self._target_want_rt_candidate(target_want_name, target_want,
                                              rt_full_id, bsp_model,
                                              kws = kws):
                candidates.add((rt_full_id, bsp_model))

        if not candidates:
            self.report_skip(
                "target '%s': No targets can be used (all %d "
                "selected from %d available eliminated by "
                "testcase filtering)" %
                (target_want_name, len(rts_bsp_models),
                 len(self.rt_all)),
                { "candidates": [ i[0] for i in rts_bsp_models ] },
                dlevel = 1)
        return candidates

    def _rt_types(self, rt_bsp_model_set):
        """
        Given a list/set of remote targets in (fullid,BSP-MODEL), return a set
        of the remote target types / BSPmodels represented in said list
        """
        rt_types = set()
        for rt_fullid, bsp_model in rt_bsp_model_set:
            rt_types.add((self.rt_all[rt_fullid]['type'], bsp_model))
        return rt_types

    def _target_wants_find_candidates(self, target_want_name_list,
                                      rt_selected, kws = None):
        """
        For the given wanted targets, find which of the remote targets given
        are candidates to be considered for assignment.

        :param list target_want_name_list: list of wanted target names (in
          self._targets), targets/interconnects which will be passed to
          testcase methods.

        :param dict rt_selected: dict of available remote targets to use, keyed
          by remote target full ID and set of available :term:`BSP
          models`s for each.

        :returns: dictionary of candidate assignment keyed by the wanted target
          name; each contains a set of suitable remote target (in a tuple
          (FULLID,BSP-MODEL).

        """
        if not kws:
            kws = {}
        rts_bsp_models = []
        candidates = {}
        # Pregenerate a list of <rt_fullid, bspmodel>
        for rt_full_id, bsp_models in list(rt_selected.items()):
            if bsp_models:
                for bsp_model in bsp_models:
                    rts_bsp_models.append((rt_full_id, bsp_model))
            else:
                rts_bsp_models.append((rt_full_id, None))
        for target_want_name in target_want_name_list:
            candidates[target_want_name] = \
                self._target_want_find_candidates(
                    target_want_name, self._targets[target_want_name],
                    rts_bsp_models, kws)
        return candidates

    # FIXME: use this everywhere where we are trying to print rt_selected
    @staticmethod
    def _selected_str(rt_selected):
        return " ".join([fullid + ":" + ",".join(bsp_models)
                         for fullid, bsp_models in rt_selected.items()])

    @staticmethod
    def _tg_str(icg):
        s=""
        for twn, (rt_full_id, bsp_model) in sorted(iter(icg.items()),
                                                   key = lambda x: x[0]):
            if bsp_model == None:
                s += "%s=%s " % (twn, rt_full_id)
            else:
                s += "%s=%s:%s " % (twn, rt_full_id, bsp_model)
        return s[:-1]

    def _target_wants_list_permutations(self, target_want_candidates, n,
                                        tag = "target", name_prefix = ""):

        # Compute how many unique candidates we have
        candidates = set()
        for c in list(target_want_candidates.values()):
            candidates |= c

        # max_repeats is how many repetitions we will allow before we
        # call it quits
        #
        # A repetition is when we have to reject a candidate target
        # group because:
        #
        #  - we have already considered that type
        #
        #  - it involves an invalid combination of targets
        #
        # Now, this algorithm is anything but efficient now and this
        # is a hack that will be rid of when we make this algorithm
        # proper.
        #
        # As we have more candidates, we need more chances to find it
        max_repeats = max(100, 5 * len(candidates))

        # Limited evaluation: this means to only do one target of each
        # type. This can be general (set on the command line, applies to all
        # targets) or per per target (declared by the target
        # decorator. _types_seen decides which is applied to each.
        _types_seen = {}
        for twn in target_want_candidates:
            target_want = self._targets[twn]
            mode = target_want['kws'].get('mode', None)
            if mode == None:
                mode = self._mode
            if mode == "one-per-type":
                _types_seen[twn] = 'one-per-type'
            elif mode == "any":
                _types_seen[twn] = 'any'
            elif mode == "all":
                _types_seen[twn] = 'all'
                # If the user is asking to run EVERYWHERE *and* we
                # have only one target,  increase the max number of
                # iterations to the number of available remote
                # targets and BSP model combinations, to try to pick'em all
                # We don't do this when more than one target, the
                # permutations could go crazy really quick -- use the
                # command line to control that.
                if len(target_want_candidates) == 1:
                    _n = 0
                    for rt_fullid, rt in self.rt_all.items():
                        # Yes, if no BSP model, count as 1
                        bsp_model_count = len(rt.get('bsp_models', [None ]))
                        _n += bsp_model_count
                    if _n > n:
                        n = _n
                        max_repeats = 3 * _n
            else:
                assert True, "Unknown mode in self._mode (%s) "\
                    "or target_want['kws']['mode'] (%s)" \
                    % (self._mode, target_want['kws']['mode'])


        # We are going to generate permutations of assigning candidate remote
        # targets (A*, B* in the example below) to each target want declared by
        # the testcsae (T* in the example); if we have:
        #
        # T1 candidates A1, A2, B1
        # T2 candidates A1, A2, B1
        #
        # Then the possible groups would be:
        #
        #       1  2  3   4  5  6   7  8  9
        #  T1  A1 A1 A1  A2 A2 A2  B1 B1 B1
        #  T2  A1 A2 B1  A1 A2 B1  A1 A2 B1
        #
        # Removing dups (as two target wants can't use the same remote target)
        #
        #       2  3   4  6   7  8
        #  T1  A1 A1  A2 A2  B1 B1
        #  T2  A2 B1  A1 B1  A1 A2
        #
        # Now, that's  six groups; but A1 and A2 are type A and B1 is type B,
        # so we don  have to redo targets that have the same types in limited
        # mode (that'd be 4--repeats 2, 6-3, 8-7):
        #
        #      2  3   7
        #  T1  A1 A1  B1
        #  T2  A2 B1  A1
        #
        # We are left with three unique type / target assignment permutations
        #
        # Note that if now we mark T2 as 'don't care' (as in any target type
        # works)
        #
        #       2  3   4  6   7  8
        #  T1  A1 A1  A2 A2  B1 B1
        #  T2  N  N   N  N   N  N
        #
        # Then
        #
        #       2  7
        #  T1  A1 B1
        #  T2  N  N
        #
        # We are left with just two groups to test
        #
        # For tracking limited mode: for each permutation we generate,
        # we have a string [TARGET-WANT-NAME1:TARGET-TYPE1
        # TARGET-WANT-NAME2:TARGET-TYPE2 ...]; if the string is in the
        # @types_seen set, then it means we already have that type
        # covered, so we ignore it.
        #
        # If we generate this many groups that are repeated, we are maybe out
        # of permutations to generate. LAME, but quick to implement
        types_seen = set()

        permutations = {}
        itr = 0
        target_counter = 0
        while itr < n:
            if max_repeats == 0:
                # Lame way to avoid getting stuck in not being able to generate
                # good permutations
                self.report_info("Exiting after generating %d permutations "
                                 "(tried to generate more, but discarded "
                                 "too many)" % len(permutations), dlevel = 6)
                break

            # Instead of brute forcing a random number, converting it to our
            # base and discarding the invalids, we are going to go for each
            # target want at random order in the list and pick up a candidate
            # at random, making sure it is not taken already

            # This keeps track of which physical remote targets have already
            # been taken, so we don't try to pick it again
            rts_taken = set()
            _types = dict()
            perm = {}
            cnt = 0
            for target_want_name in random.sample(list(target_want_candidates.keys()), len(target_want_candidates)):
                target_want = self._targets[target_want_name]
                # Of the want's candidates, get which aren't taken already
                rts_not_taken = set()
                for rt_full_id, bsp_model in target_want_candidates[target_want_name]:
                    if rt_full_id in rts_taken:
                        continue
                    rts_not_taken.add((rt_full_id, bsp_model))
                if len(rts_not_taken) == 0:
                    spec = target_want.get('spec', "")
                    self.report_skip(
                        "%s group %s: no remote targets (or group of) "
                        "can satisfy the conditions [%s] for wanted target '%s'" %
                        (tag, name_prefix, spec, target_want_name),
                        attachments = { "length_perm": len(perm), "perm": perm },
                        alevel = 1)
                    perm.clear()
                    type(self).class_result += result_c(0, 0, 0, 0, 1)
                    break
                # From available candidates, pick one at random and
                # mark it taken
                # need the sorted since in newer pythons,
                # random.sample() requires it.
                (rt_fullid, bsp_model) = random.sample(sorted(rts_not_taken), 1)[0]
                rts_taken.add(rt_fullid)
                perm[target_want_name] = (rt_fullid, bsp_model)
                rt_type = self.rt_all[rt_fullid]['type']
                if _types_seen[target_want_name] == 'one-per-type':
                    _types[target_want_name] = (rt_type, bsp_model)
                elif _types_seen[target_want_name] == 'any':
                    # Use always the same string, so it is simplified
                    # by the set
                    _types[target_want_name] = ('any', None)
                else: 	#  run in 'ALL' targets: use the sequential
                        #  counter as a "type" so they are all
                        #  different and thus are unique so it is not
                        #  eliminated by the set reduction
                    _types[target_want_name] = ("%d" % target_counter, None)
                self.report_info(
                    "%s group %s: target '%s': candidate %s:%s"
                    % (tag, name_prefix, target_want_name,
                       rt_fullid, bsp_model),
                    dlevel = 8)
                cnt += 1
                target_counter += 1

            if perm == {}:
                # This meant we don't have enough targets to assign
                break
            perm_id = commonl.mkid(self._tg_str(perm), l = 4)
            # Generate a unique string that lists the target want
            # names and the type of remote target they got assigned
            perm_types_id = " ".join([
                twn + "=" + rt_type + \
                  ((":" + bsp_model) if bsp_model else "")\
                for twn, (rt_type, bsp_model) in sorted(iter(_types.items()),
                                                        key = lambda x: x[0])])
            # Have we seen that permutations of target wants / remote
            # target types?
            if perm_types_id in types_seen:
                max_repeats -=1
                self.report_info("%s group %s: "
                                 "ignoring (types already considered) %s"
                                 % (tag, perm_id, perm_types_id), dlevel = 7)
                continue
            elif perm_id in permutations:
                # We have one like this already
                _name_prefix = "-" + name_prefix
                self.report_info("%s group %s%s: "
                                 "ignoring (repeated)"
                                 % (tag, _name_prefix, perm_id), dlevel = 7)
                max_repeats -=1
                continue

            itr += 1
            types_seen.add(perm_types_id)
            permutations[perm_id] = perm
        return permutations


    #
    # The top level testcase running interface
    #
    # _run_on_targets() is given all the available remote targets,
    # figures out the permutations of remote targets to wanted targets
    # and spawns on one thread per TC/targetgroup _run(), which calls
    # _run_on_target_group()

    def _do_the_eval(self):
        # on each eval repetition, we want self.result_eval to only
        # have the eval data for that repetition; for all the
        # repetitions, we accumulate it in result_all_evals.
        result_all_evals = result_c(0, 0, 0, 0, 0)
        for self.eval_count in range(0, self.eval_repeat):
            self.kw_set("eval_count", self.eval_count)
            try:
                # Repeat the evaluation as many times as we were asked
                # too, even if it fails.
                #
                # Note we run setup/start/eval/teardown for all the eval*
                # methods; however, for the test* methods, we do for each
                # setup/start/testX/teardown
                with msgid_c("E#%d" % (self.eval_count + 1), phase = 'eval'):
                    self.result_eval = result_c(0, 0, 0, 0, 0)
                    if self._eval_serial:
                        retval = self._methods_call("setup")
                        self.result_eval += retval
                        if retval.errors or retval.failed or retval.blocked:
                            self._methods_call("teardown")
                            continue
                        retval = self._methods_call("start")
                        self.result_eval += retval
                        if retval.errors or retval.failed or retval.blocked:
                            self._methods_call("teardown")
                            continue
                        self.result_eval += self._methods_call("eval")
                        self._methods_call("teardown")
            finally:
                result_all_evals += self.result_eval
                # However, we always update self.result_eval() at the end with
                # the totals in case we exit and are not taking the
                # loop again
                self.result_eval = result_all_evals
        return self.result_eval.summary()


    def _run_on_target_group(self, msgid):
        """
        Run the five phases of evaluating a test case and collect the
        result of each.

        Note that for deployment and evolution we need the targets assigned.
        """
        # FIXME: this needs to differentiate skipping a whole TC
        #        if we raise from the main TC, skip and stop executing
        #        if we skip from a subtc, keep going

        # Note dlevel_passed; we don't want the PASS messages for
        # level-0 verbosity, only when things FAIL or are BLOCKed
        result = result_c(0, 0, 0, 0, 0)
        while True:	# Fugly but beats not having goto
            # This we ran during __init__, but then because this
            # *self* was a copy of that self that ran __init__
            # (FIXME: ugly hack that has to be fixed), we need to
            # run it again so it generates the right keywords for
            # the targets where it is assigned to run.
            for hook in self.hook_pre:
                hook(self)
            # specific, we can take it out to the TC level
            if not self._phase_skip("configure"):
                if self.subtc:
                    self.report_info(
                        "NOTE: this testcase unfolds %d subcases: %s" %
                        (len(self.subtc), " ".join(self.subtc.keys())),
                        dlevel = 1)
                else:
                    self.report_info(
                        "NOTE: this testcase does not unfold subcases"
                        " (it may later)",
                        dlevel = 3)
                with msgid_c("C", phase = 'config'):
                    retval = self._methods_call("configure")
                result += retval
                # This testcase might have no build, so ignore if nothing
                # is ran in the build
                if self.report_tweet(
                        "configure", retval, ignore_nothing = True,
                        extra_report = self._extra_report(self.kws),
                        dlevel = -1, dlevel_skipped = 2, dlevel_passed = 2) \
                        == False:
                    break
            # We don't want to count build success as a separate
            # test-case when we have evaluation--hence why we reassign
            # `result` insted of adding to it.
            if not self._phase_skip("build"):
                with msgid_c("B", phase = 'build'):
                    result = self._methods_call("build")
                # This testcase might have no build, so ignore if nothing
                # is ran in the build
                if self.report_tweet(
                        "build", result, ignore_nothing = True,
                        extra_report = self._extra_report(self.kws),
                        dlevel = -1, dlevel_skipped = 2, dlevel_passed = 1) \
                        == False:
                    break

            # are the test target or testcase declaring as build only?
            if self.build_only:
                self.report_info("testcase is build only: "
                                 "skipping deploy and eval (%s)"
                                 % ", ".join(self.build_only), dlevel = 4)
                break

            # We don't want to count build success as a separate
            # test-case when we have evaluation--hence why we reassign
            # `result` insted of adding to it.

            deploy_skip = self._phase_skip("deploy")
            eval_skip = self._phase_skip("eval")
            if deploy_skip and eval_skip:	# Skip assigning targets
                break				# if we know we won't run

            # We use the current message id as the ticket for target
            # assignment
            try:
                with self._targets_assign():
                    if self.allocid:
                        self.report_info(f"allocation ID: {self.allocid}",
                                         # report this verbose on top levels
                                         dlevel = 2 if self.parent else 0)
                    if not deploy_skip:
                        with msgid_c("D", phase = "deploy"):
                            retval = self._methods_call("deploy")
                        result += retval
                        # We don't report deploy as much as build/eval
                        if self.report_tweet(
                                "deploy", retval, ignore_nothing = True,
                                extra_report = self._extra_report(self.kws),
                                dlevel_passed = 1) == False:
                            break
                    if not eval_skip:
                        retval = self._do_the_eval()
                        result += retval
                        if self.report_tweet(
                                "evaluation", retval,
                                extra_report = self._extra_report(self.kws),
                                dlevel = -1, dlevel_skipped = 1,
                                dlevel_passed = 1) \
                                == False:
                            break
            # Catch stuff in _targets_assign()
            except Exception as e:
                self.report_blck("exception: %s %s" % (type(e).__name__, e),
                                 { 'exception info': traceback.format_exc() },
                                 alevel = 1)
                result += result_c(0, 0, 0, 1, 0)

            break # we are in a while loop that has to happen only once

        # Run clean rules
        if not self._phase_skip("clean"):
            with msgid_c("L", phase = 'clean'):
                retval = self._methods_call("clean")
            self.report_tweet("clean", retval,
                              extra_report = self._extra_report(self.kws),
                              ignore_nothing = True,
                              dlevel_passed = 1)
            # cleanup success does not count, only if it fails
            # or blocks, so it is noticed and can be addressed
            result += result_c(0, retval.errors, retval.failed,
                               retval.blocked, 0)

        return result.summary()

    def _cleanup(self):
        # Remove and wipe files left behind
        # FIXME: move this to set all the files in a tempdir specific
        # to each TC instantation and just wipe that dir at the end / __del__
        for pathname in self._cleanup_files:
            try:
                os.unlink(pathname)
            except OSError as e:
                if e.errno != errno.ENOENT:
                    raise

    def finalize(self, result):
        assert isinstance(result, result_c)
        self.ts_end = time.time()
        self.report_tweet(
            "COMPLETION", result,
            extra_report = self._extra_report(self.kws),
            # Trick: won't see this, report driver will
            level = 1000,
            ignore_nothing = False
        )
        self._cleanup()

    def mkticket(self, hashid = None):
        # FIXME: rename to internal API, _mkid
        #        rename ticket to hashid
        #        change to consider also tc axes
        #          (test running on same TC group with different axes)
        #        remove passing of ticket to server? we don't really
        #        use it anymore
        #
        # Note we use this msgid's string as tc_hash for subsitution,
        # it is a unique name based on target name and BSP model, test
        # case name (which might be more than just the tescase path if
        # a single file yields multiple test cases).
        if hashid == None:
            target_group_name = self.target_group.name \
                                if self.target_group else 'n/a'
            self.ticket = msgid_c.encode(
                self._hash_salt + self.runid_visible + self.name + target_group_name,
                self.hashid_len).decode('UTF-8')
        else:
            self.ticket = hashid
        self._kw_set("tc_hash", self.ticket)
        if self.runid == None:
            self.runid_hashid = self.ticket
        else:
            self.runid_hashid = f"{self.runid_visible}{report_runid_hashid_separator}{self.ticket}"

    def _run(self, msgid, tls_parent):
        """
        High level executor for the five phases of the testcase

        Note that self.target_group contains the names of the targets
        and their description

        Note seems the msgid is overriden, but context is obtaiend via
        TLS.
        """
        try:
            self.__thread_init__(tls_parent)
            result = result_c(0, 0, 0, 0, 0)
            self.mkticket()
            # temporary directory specialization for this TC
            self.tmpdir = os.path.join(tc_c.tmpdir, self.ticket)
            if not os.path.isdir(self.tmpdir):
                # And fail if exists and it is not a directory
                os.makedirs(self.tmpdir)

            self._kw_set("tmpdir", self.tmpdir)

            # Calculate the report file prefix
            global log_dir
            if self.runid:
                # see the doc in report_runid_hashid_file_separator
                # for why this.
                file_prefix = f"{self.runid}{report_runid_hashid_file_separator}{self.ticket}"
            else:
                file_prefix = self.runid_hashid
            self.report_file_prefix = os.path.join(log_dir, f"report-{file_prefix}.")
            self.kws['report_file_prefix'] = self.report_file_prefix

            with msgid_c(depth = 1, testcase = self) as msgid:
                try:
                    self.report_info(
                        # report this for the reports to databases and
                        # files, so triagers can know which version
                        # generated it; normally we don't need this on
                        # the console, so we verbose level it one up
                        f"client version {version}",
                        dlevel = 3 if self.parent else 1)
                    if self.parent:
                        self.report_info(
                            f"NOTE: this is a subtestcase of"
                            f" {self.parent.name} ({self.parent.runid_hashid});"
                            " refer to it for full information", dlevel = 1)
                    for _target in list(self.target_group.targets.values()):
                        # We need to update all the target's KWS, as we
                        # have added KWS to the tescase (tc_hash and
                        # tg_hash!) , which we could only add once we had
                        # the target group created -- FIXME: this is
                        # currently a hack and we need a better way to do
                        # it--unifying with the target group creation and
                        # setting in run_on_targets()
                        _target.ticket = self.ticket
                        if _target.bsp_model == None:
                            bsp_model = ""
                        else:
                            bsp_model = _target.bsp_model
                        _target.kw_set('tg_hash', msgid_c.encode(
                            self.ticket + _target.fullid + bsp_model, 4))
                    if self._execute:
                        self.report_info(
                            "will run on target group '%s' (PID %d / TID %x)"
                            % (
                                self.target_group.descr, os.getpid(),
                                threading.current_thread().ident
                            ),
                            # be less verbose for subcases,
                            # since we know this info already
                            dlevel = 2 if self.parent else 0 )
                        # this is a subtestcase created just to collect
                        # reports, so we just need to finalize it
                        result += self._run_on_target_group(msgid)
                    else:
                        self.report_info(
                            "ran on target group '%s' (PID %d / TID %x)"
                            % (
                                self.target_group.descr, os.getpid(),
                                threading.current_thread().ident
                            ),
                            # be less verbose for subcases,
                            # since we know this info already
                            dlevel = 2 if self.parent else 0 )
                        r = self.result.normalized()
                        self.report_tweet(
                            # hardcode eval phase here
                            "subcase", r, ignore_nothing = False,
                            extra_report = self._extra_report(self.kws),
                            # report first level on non-pass
                            level = 0,
                            dlevel_passed = 1,
                            dlevel_skipped = 2)
                        result += r
                except Exception as e:
                    self.log.error(
                        "exception: %s %s\n%s" %
                        (type(e).__name__, e, traceback.format_exc()))
                    self.report_blck(
                        "exception: %s %s" % (type(e).__name__, e),
                        { 'exception info': traceback.format_exc() },
                        alevel = 1)
                    result = result_c(0, 0, 0, 1, 0)
                finally:
                    self.finalize(result)
        except Exception as e:
            # This msgid_c context is a hack so that this exception
            # report has a proper RUNID and HASH prefix
            with msgid_c(depth = 0) as msgid:
                self.report_blck(
                    "BUG exception: %s %s" % (type(e).__name__, e),
                    { 'exception info': traceback.format_exc() },
                    alevel = 1)
            result = result_c(0, 0, 0, 1, 0)

        self.result = result
        results = [ (type(self), result.summary()) ]

        # run the list of sub testcases and the post tcs added
        for tc in list(self.subtc.values()) + self._tcs_post:
            # Well, this is quite a hack -- for reporting to work ok,
            # rebind each's target's testcase pointer to this
            # subtestcase.
            for _target in list(self.targets.values()):
                _target.testcase = tc
            tc.target_group = self.target_group
            tc._targets = self._targets
            tc._kw_set("type", self.kws['type'])
            tc._methods_prepare()	# setup phase running methods
            tc.__init_shallow__(tc)
            tc._prefix_update()
            # remember this returns a list, so we have to concatenate them
            results += tc._run(msgid_c.parent(),
                               _simple_namespace(self.tls.__dict__))
            del tc	# not needed any more

        # Undo the hack from before, as we might need these values to
        # be proper for reporting later on
        for _target in list(self.targets.values()):
            _target.testcase = self

        return results

    def post_tc_append(self, tc):
        """
        Append a testcase that shall be executed inmediately after
        this testcase is done executing in the same target group.

        This is a construct that can be used for:

        - execute other testcases that have been detected as needed
          only during runtime

        - reporting subtestcases of a main testcase (relying only on
          the output of the main testcase execution, such as in
          :class:`tcfl.tc_zephyr_sanity.tc_zephyr_subsanity_c`.

        :param tc_c tc: [instance of a] testcase to append; note this
          testcase will be executed in the same target group as this
          testcase is being executed. So the testcase has to declare
          at the same targets (with the same names) or a subset of
          them. Example:

          >>> @tcfl.tc.target("target1")
          >>> @tcfl.tc.target("target2")
          >>> @tcfl.tc.target("target3")
          >>> class some_tc(tcfl.tc.tc_c):
          >>>     ...
          >>>     def eval_something(self, target2):
          >>>         new_tc = another_tc(SOMEPARAMS)
          >>>         self.post_tc_append(new_tc)
          >>>
          >>>
          >>> @tcfl.tc.target("target2")
          >>> class another_tc(tcfl.tc.tc_c):
          >>>     ...
          >>>     def eval_something(self, target2):
          >>>         self.report_info("I'm running on target2")
          >>>
          >>> @tcfl.tc.target("target1")
          >>> @tcfl.tc.target("target3")
          >>> class yet_another_tc(tcfl.tc.tc_c):
          >>>     ...
          >>>     def eval_something(self, target1, target3):
          >>>         self.report_info("I'm running on target1 and target3")

        """
        self._tcs_post.append(tc)

    def _clone(self):
        """Return a hybrid shallow/deep copy of this object

        This for now a dirty workaround around the fact that we have to
        split a testcase object between the information for it and what
        has to be specific to each thread.

        Anything that is not going to be modified, we don't need to deep
        copy, the rest, yes.

        # FIXME: it makes little sense to try to load upon TCPY writers
        to split deep vs shallow, shareable vs not.
        So let's have this clone function default to do a deep copy,
        except of the parts we know are ok to be shallow and those we
        share.

        - this is really bad in general becuase it means the
        constructor is not called, so we are shortcircuting a lot of
        implementation details for inheriting classes?. We have to
        stop doing this, and just do a new object and copying?

        """
        c = copy.copy(self)
        c.__init_shallow__(self)
        c._prefix_update()
        return c

    @result_c.from_exception
    def _permutations_make(self, rt_all, rt_selected, ic_selected):
        # FIXME: maybe move this to __init__
        self._methods_prepare()	# setup phase running methods
        self.rt_all = rt_all
        if self.is_static():
            if self._dry_run:
                self.report_info("will run")
            rt_selected = { 'local': list(rt_all['local']['bsp_models'].keys()) }
            ic_selected = { }

        # This will be now testcase-specific, so make a deep copy
        # so it can be modified by the testcase -- note we might alter
        # FIXME: note we might not need this deep coy --
        # verify once the reorg is done
        self.rt_selected = copy.deepcopy(rt_selected)
        self.ic_selected = copy.deepcopy(ic_selected)

        # list candidates to interconnects
        #
        # We requested R interconnects [len(self._interconnects)] and we
        # have ic_selected as candidates for those interconnects. Each
        # interconnect requested might put restrictons to which
        # interconnects can be used. So in ic_candidates, return which of
        # ic_selected can be used for each interconnect.
        self.report_info("interconnect groups: finding remote ic targets",
                         dlevel = 6)
        ic_candidates = self._target_wants_find_candidates(
            self._interconnects, ic_selected)
        self.report_info("interconnect groups: available remote ic "
                         "targets: %s" % self._selected_str(ic_selected),
                         dlevel = 5)
        # The list of candidates to interconnects could be arranged in
        # different ways. Permutations will make it grow quick. If IC0 =
        # {AB}, IC1={ABC} and IC2={ABC}, there is an upper ceiling of
        # N!/(N-R)!, where R is the number of interconnects and N is the
        # max number of available candidates (len(ic_selected) (the real
        # ceiling will be lower as this is not a pure permutation
        # problem--some of the sets of candidates don't include all the
        # available N candidates in ic_selected and I don't really know if
        # there is a math way to compute it -- internet got too dense for
        # my limited set theory knowledge quite quick).
        #
        # So we are just going to generate a few at random; if we only have
        # one interconnect, we'll try to get one for each type of
        # interconnect. Otherwise we use the command line to limit the
        # amount of IC groups we create.
        if len(self._interconnects) == 1:
            ic_want_name = list(self._interconnects)[0]
            max_permutations = \
                len(self._rt_types(ic_candidates[ic_want_name]))
        else:
            # FIXME: get from command line and defaults
            max_permutations = 10
        self.report_info("interconnect groups: generating %d by "
                         "permuting remote ic targets" %
                         max_permutations, dlevel = 6)
        ic_permutations = self._target_wants_list_permutations(
            ic_candidates, max_permutations, tag = "interconnect")
        if len(self._interconnects):
            if len(ic_permutations) == 0:
                type(self).class_result += result_c(0, 0, 0, 0, 1)
                raise skip_e("WARNING! No interconnects available")
        elif len(self._interconnects) == 0:
            # Cheat, when we don't use interconnects, just define an empty
            # one so we enter into the loop below
            ic_permutations = { "all": { } }
        for icgid, icg in ic_permutations.items():
            self.report_info("interconnect group %s: %s"
                             % (icgid, self._tg_str(icg)), dlevel = 5)


        # FIXME: now filter them

        # Now that we have a set of interconnect permutations, we need
        # to select, for each, which targets fit in the permutation
        # Basically, this is a simple OR, if the target is in any of
        # the interconnects, it's good to go.
        self.report_info("interconnect groups: finding remote "
                         "targets available for each",
                         dlevel = 6)
        icg_selected = collections.defaultdict(dict)
        for icgid, icg in ic_permutations.items():
            for icwn, (rtic, _) in icg.items():
                rtic_id = self.rt_all[rtic]['id']
                for rt_full_id, bsp_models in rt_selected.items():
                    rt = self.rt_all[rt_full_id]
                    if rtic_id in rt.get('interconnects', {}):
                        icg_selected[icgid][rt_full_id] = bsp_models
            if icgid == "all":	# No interconnects, take'em all
                icg_selected[icgid] = dict(rt_selected)

            if not icg_selected[icgid]:
                type(self).class_result += result_c(0, 0, 0, 0, 1)
                self.report_skip(
                    "interconnect group %s (%s): no targets can be used!!"
                    % (icgid, self._tg_str(icg)), dlevel = 3)
            else:
                self.report_info(
                    "interconnect group %s (%s): candidate targets "
                    "available: %s" %
                    (icgid, self._tg_str(icg),
                     self._selected_str(icg_selected[icgid])),
                    dlevel = 4)

        # So now icg_selected is a dict, keyed by interconnet
        # group name, which contains the remote targets that are
        # valid candidates to be used for that interconnect group
        # (so they satisfy the condition of being in any of the
        # interconnects that are required for the group).
        #
        # So now, for each of those interconnect groups we need to
        # generate a list of target groups.
        self.report_info("interconnect groups: generating target "
                         "groups for each", dlevel = 6)
        rt_candidates = {}
        rt_permutations = {}
        target_want_list = set(self._targets.keys()) - self._interconnects
        for icgid, rt_selected in icg_selected.items():
            # Generate keywords that describe the current
            # interconnects
            kws_ics = dict()
            for icwn, ic in ic_permutations[icgid].items():
                commonl.kws_update_from_rt(kws_ics, self.rt_all[ic[0]],
                                           prefix = icwn + ".")
            # list candidates to targets in this interconnect group
            self.report_info("interconnect group %s: "
                             "finding remote target candidates" %
                             icgid, dlevel = 8)
            rt_candidates = self._target_wants_find_candidates(
                target_want_list, rt_selected, kws_ics)
            self.report_info("interconnect group %s: "
                             "remote target candidates: %s"
                             % (icgid, pprint.pformat(rt_candidates)),
                             dlevel = 7)

            # Generate permutations of targets wants vs available
            # targets for this interconnect group
            if len(target_want_list) == 1:
                twn = list(target_want_list)[0]
                max_permutations = len(self._rt_types(rt_candidates[twn]))
            else:
                max_permutations = tc_c.max_permutations

            self.report_info("interconnect group %s: generating "
                             "target groups" % icgid, dlevel = 6)
            tgs = self._target_wants_list_permutations(
                rt_candidates, max_permutations, name_prefix = icgid)
            if tgs:
                for tgid, tg in tgs.items():
                    rt_permutations[(icgid, tgid)] = tg
            elif len(self._targets) == 0: # static TC
                ic_permutations["localic"] = {}
                rt_permutations = { ("localic", "localtg") : {} }
            elif len(target_want_list) == 0: # only ICs?
                self.report_info("interconnect group %s: "
                                 "no targets needed" % (icgid),
                                 dlevel = 4)
                # FIXME: fix so if tgid == None, nothjing is printed
                rt_permutations[(icgid, None)] = {}
            else:
                self.report_skip("interconnect group %s: "
                                 "not enough targets" % (icgid),
                                 dlevel = 1, alevel = 1,
                                 attachments = { "icg" : icg})
        return ic_permutations, rt_permutations


    @result_c.from_exception
    def _run_on_targets(self, tp, rt_all, rt_selected, ic_selected):
        """
        Launch the test case execution, in one or more background
        threads

        :param dict targets: Dictionary of targets where the test
          would have to be run. Can be empty (to indicate
          auto-selection of targets or when unneeded--eg for static
          test cases).

        :param dict rt_selected: Dictionary keyed by
          fullid listing the BSP models that have to be run for
          each target. This comes after all the filtering, and more
          filtering can happen here, target specific.

        :param dict ic_selected: Dictionary keyed by fullid listing
          the interconnects available, also listing (if it provides)
          the BSP models that have to be run for each target. This
          comes after all the filtering, and more filtering can happen
          here, target specific.

        :returns: dict of thread descriptors as returned by
          ThreadPool.apply_async(); if {}, it means that no targets
          could be located and this TC is blocked.

        :exceptions: any, treat them as a blocker

        """
        try:
            threads = []

            ic_permutations, rt_permutations = self._permutations_make(
                rt_all, rt_selected, ic_selected)

            # So now we are going to iterate over all the groups; as a
            # hack to tie us up until the orchestrator is fix, respect
            # the limit on how many runs of each single testcase will
            # be run
            permutations = rt_permutations.items()
            if self.max_runs_per_tc > 0:
                permutation = rt_permutations.items()
                max_runs_per_tc = min(len(permutations), self.max_runs_per_tc)
                permutations = random.sample(rt_permutations.items(),
                                             max_runs_per_tc)

            for (icgid, tgid), tg in permutations:
                if tgid == None:
                    # no targets, but interconnects
                    tg_name = icgid
                else:
                    if icgid == "all":	# No interconnects?
                        tg_name = tgid
                    else:
                        tg_name = "%s-%s" % (icgid, tgid)
                # create a representation of the name that indicates
                # the twn/role, such as
                #
                # ic=SERVER1/nwa target=SERVER2/target2 target2=SERVER1/target3
                icg = ic_permutations[icgid]
                strs = []
                icg_str = self._tg_str(icg)
                if len(icg_str):
                    strs.append(icg_str)
                tg_str = self._tg_str(tg)
                if len(tg_str) > 0:
                    strs.append(tg_str)
                if not strs:
                    strs = [ tg_name ]
                group_str = " ".join(strs)
                del strs

                tc_for_tg = self._clone()
                if self._dry_run:
                    self.report_info("will run on target group '%s'"
                                     % group_str, dlevel = 1)
                    continue

                # FIXME: this order could be better
                target_group = target_group_c(group_str)
                # Ids of the interconnect targets we'll be using
                icg_ids = set()
                for target_want_name in reversed(list(self._targets.keys())):
                    # iterate over the list of wanted targets to add
                    # them in the right order to the
                    # target_group.
                    #
                    # FIXME: reversed for decorator workaround
                    if target_want_name in icg:
                        rt_full_id, bsp_model = icg[target_want_name]
                        icg_ids.add(rt_all[rt_full_id]['id'])
                    elif target_want_name in tg:
                        rt_full_id, bsp_model = tg[target_want_name]
                    else:
                        raise RuntimeError("BUG? target want %s neither "
                                           "in icf or tg?" % target_want_name)
                    target_group.target_add(
                        target_want_name,
                        target_c(rt_all[rt_full_id], tc_for_tg,
                                 bsp_model, target_want_name))
                # this should be the unique ID
                target_group.name_set(tg_name)
                tc_for_tg.target_group = target_group
                tc_for_tg.targets = target_group.targets
                for target in list(target_group.targets.values()):
                    target._kws_update_interconnect_addrs(icg_ids)
                    # this just updates the core keys, but later calls
                    # to kw_set() and company will refresh the main
                    # target.kws dict.

                #
                # Set the new tmpdir
                #
                # FIXME: this is bad to make these two here
                tc_for_tg.mkticket()
                tc_for_tg.tmpdir = os.path.join(tc_c.tmpdir, tc_for_tg.ticket)
                commonl.makedirs_p(tc_for_tg.tmpdir)

                for target_want_name, target in tc_for_tg.targets.items():
                    target.tmpdir = os.path.join(
                        tc_for_tg.tmpdir, "targets", target_want_name)
                    commonl.makedirs_p(target.tmpdir)

                tc_for_tg.report_info("queuing for execution", dlevel = 3)
                thread = tp.apply_async(
                    tc_for_tg._run,
                    (
                        msgid_c.current(),
                        _simple_namespace(self.tls.__dict__)
                    )
                )
                # so we can Ctrl-C easily; we don't care for the
                # cleanup, we consider all expendable resournces
                # and the toplevel tempdirs will be cleaned below
                # Targets acquired will be released as idle by the
                # daemon
                thread.daemon = True
                threads.append(thread)
            self.log.info("%d jobs launched" % len(threads))
            return threads
        finally:
            self._cleanup()

    #
    # Testcase driver internal interface
    #

    @classmethod
    def _classes_enumerate(cls, obj):
        # Recursively enumerate classes in the object that are a base
        # of tc_c -- this allows us to pick up classes that are
        # defined inside other classes, which is quite useful for unit
        # testcases and other odd ends
        l = []
        for x in inspect.getmembers(obj):
            if x == ("__class__", type):
                pass
            elif inspect.isclass(x[1]):
                # Well, FIXME: this is not ok -- we are trying to
                # ignore here any old style unit class from the
                # ./tests/ tree as we move to the new style of unit
                # test cases.
                # But because we have a mess with how we import files
                # now, some times they are found as
                # commonl.testing.test_ttbd_mixin, some as
                # commonl.testing.test_ttbd_mixin and we can't match
                # on that, so for the time being, we match on name.
                base_classes = [ i.__name__ for i in inspect.getmro(x[1]) ]
                for base_class in base_classes:
                    if base_class in  [ "test_ttbd_mixin", "test_tcf_mixin" ]:
                        logging.info("ignoring unit test case %s",
                                     x[1].__name__)
                        break
                else:
                    l += cls._classes_enumerate(x[1])
                    if issubclass(x[1], tc_c):
                        if x[1].__name__.endswith("_base"):
                            logger.info(
                                "%s: %s: considering a base class "
                                "due to name starting with `_base`, "
                                "ignoring", obj.__file__, x[1].__name__)
                        else:
                            l.append(x[1])
        return l

    # Testcases are any file that start with `test` and ends with `.py`
    file_regex = re.compile(r'^test[^/]*\.py$')

    # Default driver loader; most test case drivers would over load
    # this to determine if a file is a testcase or not.
    @classmethod
    def is_testcase(cls, path, from_path, tc_name, subcases_cmdline):
        """Determine if a given file describes one or more testcases and
        crete them

        TCF's test case discovery engine calls this method for each
        file that could describe one or more testcases. It will
        iterate over all the files and paths passed on the command
        line files and directories, find files and call this function
        to enquire on each.

        This function's responsibility is then to look at the contents
        of the file and create one or more objects of type
        :class:`tcfl.tc.tc_c` which represent the testcases to be
        executed, returning them in a list.

        When creating :term:`testcase driver`, the driver has to
        create its own version of this function. The default
        implementation recognizes python files called *test_\*.py* that
        contain one or more classes that subclass :class:`tcfl.tc.tc_c`.

        See examples of drivers in:

        - :meth:`tcfl.tc_clear_bbt.tc_clear_bbt_c.is_testcase`
        - :meth:`tcfl.tc_zephyr_sanity.tc_zephyr_sanity_c.is_testcase`
        - :meth:`examples.test_ptest_runner` (:term:`impromptu
          testcase driver`)

        note drivers need to be registered with
        :meth:`tcfl.tc.tc_c.driver_add`; on the other hand, a Python
        :term:`impromptu testcase driver` needs no registration, but
        the test class has to be called *_driver*.

        :param str path: path and filename of the file that has to be
          examined; this is always a regular file (or symlink to it).

        :param str from_path: source command line argument this file
          was found on; e.g.: if *path* is *dir1/subdir/file*, and the
          user ran::

            $ tcf run somefile dir1/ dir2/

          *tcf run* found this under the second argument and thus:

          >>> from_path = "dir1"

        :param str tc_name: testcase name the core has determine based
          on the path and subcases specified on the command line; the
          driver can override it, but it is recommended it is kept.

        :param list(str) subcases_cmdline: list of subcases the user
          has specified in the command line; e.g.: for::

            $ tcf run test_something.py#sub1#sub2

          this would be:

          >>> subcases_cmdline = [ 'sub1', 'sub2']

        :returns: list of testcases found in *path*, empty if none
          found or file not recognized / supported.

        """
        if not cls.file_regex.search(os.path.basename(path)):
            return []
        try:
            # Include where path is in the sys.path, so modules that
            # are in the same directory are loaded
            sys.path.insert(0, os.path.dirname(path))
            name = path.translate(str.maketrans("/.", "__"))
            module = imp.load_source(name, path)
        except exception as e:
            raise
        except (ImportError, Exception) as e:
            if 'base.util.tag' in e.args \
               or 'util.const' in e.args \
               or 'util.tag' in e.args :
                raise skip_e(
                    "unsupported: %s" % e,
                    { "ex_trace": traceback.format_exc(), "dlevel": 4 })
            else:
                raise blocked_e(
                    "Cannot import: %s (%s)" % (e, type(e).__name__),
                    { "ex_trace": traceback.format_exc() })
        finally:
            sys.path.pop(0)
        l = cls._classes_enumerate(module)
        # Get instances of subclassess of tc_c class as testcases
        if l == []:
            logger.warning("%s: no suitable classes found in %s",
                           path, module)
            # Nothing found, so let's try to fully unload & delete the
            # module and garbage collect it
            del sys.modules[module.__name__]
            del module
        tcs = []
        for _cls in l:
            path = inspect.getsourcefile(_cls)
            subcase_name = _cls.__name__
            if subcase_name != "_driver" \
               and subcase_name != "_test" \
               and subcases_cmdline and subcase_name not in subcases_cmdline:
                logging.info("%s: subcase ignored since it wasn't "
                             "given in the command line list: %s",
                             subcase_name, " ".join(subcases_cmdline))
                continue
            if subcase_name not in ( "_test", "_driver" ):
                # if the test class name is just "_test", we don't
                # even list it as as test, we assume per convention it
                # is the only test in the file
                name = tc_name + "#" + subcase_name
            else:
                name = tc_name
            # testcase name, file where it came from, origin
            try:
                # some forms of defining classes (like using type)
                # might not find it funny and make it easy to find the
                # line number
                source_line = str(inspect.getsourcelines(_cls)[1])
            except IOError as e:
                source_line = "n/a"
            tc = _cls(name, path, path + ":" + source_line)
            if subcase_name in ( "_test", "_driver" ):
                # make a copy, as we might modify during execution
                tc.subcases = list(subcases_cmdline)
            tcs.append(tc)
        return tcs

    @classmethod
    def _create_from_file_name(cls, tcis, file_name, from_path,
                               subcases_cmdline):
        """
        Given a filename that contains a possible test case, create one or
        more TC structures from it and return them in a list

        :param list tcis: list where to append found test case instances
        :param str file_name: path to file to consider
        :param str from_path: original path from which this file was
          scanned (this will be a parent path of this file)
        :param list subcases: list of subcase names the testcase should
          consider
        :returns: result_c with counts of tests passed/failed (zero,
          as at this stage we cannot know), blocked (due to error
          importing) or skipped(due to whichever condition).
        """
        # FIXME: not working well to ignore .git
        result = result_c(0, 0, 0, 0, 0)
        for ignore_regex, origin in cls._ignore_regexs:
            if ignore_regex.match(file_name):
                logger.log(6, "%s: ignored by regex %s [%s]",
                           file_name, ignore_regex.pattern, origin)
                return result

        def _style_get(_tc_driver):
            signature = inspect.signature(_tc_driver.is_testcase)
            if len(signature.parameters) == 1:
                return 2
            # v2: added from_path
            elif len(signature.parameters) == 2:
                return 3
            # v3; added tc_name, subcases
            elif len(signature.parameters) == 4:
                return 4
            else:
                raise AssertionError(
                    "%s: unknown # of arguments %d to is_testcase()"
                    % (_tc_driver, len(signature.parameters)))

        def _is_testcase_call(tc_driver, tc_name, file_name,
                              from_path, subcases_cmdline):
            style = _style_get(tc_driver)
            # hack to support multiple versions of the interface
            if style == 2:
                return tc_driver.is_testcase(file_name)
            elif style == 3:
                return tc_driver.is_testcase(file_name, from_path)
            elif style == 4:
                return tc_driver.is_testcase(file_name, from_path,
                                             tc_name, subcases_cmdline)
            raise AssertionError("bad style %d" % style)

        if subcases_cmdline:
            tc_name = file_name + "#" + "#".join(subcases_cmdline)
        else:
            tc_name = file_name
        for _tc_driver in cls._tc_drivers:
            tc_instances = []
            # new one all the time, in case we use it and close it
            tc_fake = tc_c(tc_name, file_name, "builtin")
            tc_fake.mkticket()
            with msgid_c(depth = 1) as _msgid:
                cwd_original = os.getcwd()
                try:
                    tc_instances += _is_testcase_call(_tc_driver, tc_name,
                                                      file_name, from_path,
                                                      subcases_cmdline)
                    for _tc in tc_instances:
                        logger.info("testcase found @ %s by %s",
                                    _tc.origin, _tc_driver)

                # this is so ugly, need to merge better with result_c's handling
                except subprocess.CalledProcessError as e:
                    retval = result_c.from_exception_cpe(tc_fake, e)
                    tc_fake.finalize(retval)
                    result += retval
                    continue
                except OSError as e:
                    attachments = dict(
                        errno = e.errno,
                        strerror = e.strerror
                    )
                    if e.filename:
                        attachments['filename'] = e.filename
                    retval = result_c.report_from_exception(tc_fake, e,
                                                            attachments)
                    tc_fake.finalize(retval)
                    result += retval
                    continue
                except Exception as e:
                    retval = result_c.report_from_exception(tc_fake, e)
                    tc_fake.finalize(retval)
                    result += retval
                    continue
                finally:
                    cwd = os.getcwd()
                    if cwd != cwd_original:
                        logging.error(
                            "%s: driver changed working directory from "
                            "%s to %s; this is a BUG in the driver!"
                            % (_tc_driver.origin, cwd_original, cwd))
                        os.chdir(cwd_original)
            if not tc_instances:
                continue

            for _tc in tc_instances:
                for testcase_patcher in cls.testcase_patchers:
                    testcase_patcher(_tc)
                _tc._components_fixup()
            tcis += tc_instances
            break
        else:
            logger.log(7, "%s: no testcase driver got it", file_name)

        return result

    @classmethod
    def find_in_path(cls, tcs, path, subcases_cmdline):
        """
        Given a path, scan for test cases and put them in the
        dictionary @tcs based on filename where found.
        list of zero or more paths, scan them for files that
        contain testcase tc information and report them.
        :param dict tcs: dictionary where to add the test cases found

        :param str path: path where to scan for test cases

        :param list subcases: list of subcase names the testcase should
          consider

        :returns: result_c with counts of tests passed/failed (zero,
          as at this stage we cannot know), blocked (due to error
          importing) or skipped(due to whichever condition).
        """
        assert isinstance(tcs, dict)
        # FIXME this should cache stuff so we don't have to rescan all the
        # times--if the ctime hasn't changed since our cache entry, then
        # we use the cached value
        result = result_c(0, 0, 0, 0, 0)
        logger.info("%s: scanning argument", path)
        tc_global.report_info("%s: scanning directory from arguments" % path,
                              dlevel = 4)
        if os.path.isdir(path):
            for tc_path, _dirnames, _filenames in os.walk(path):
                logger.log(5, "%s: scanning directory", tc_path)
                tc_global.report_info("%s: scanning directory" % tc_path,
                                      dlevel = 5)
                # Remove directories we don't care about
                # FIXME: very o(n)
                keep_dirs = []
                for dirname in list(_dirnames):
                    for dir_ignore_regex, _origin in tc_c._ignore_directory_regexs:
                        if dir_ignore_regex.match(dirname):
                            break
                    else:
                        keep_dirs.append(dirname)
                del _dirnames[:]
                _dirnames.extend(keep_dirs)
                for filename in sorted(_filenames):
                    tc_instances = []
                    file_name = os.path.join(tc_path, filename)
                    result += cls._create_from_file_name(
                        tc_instances, file_name, path, subcases_cmdline)
                    for _tc in tc_instances:
                        tcs[_tc.name] = _tc
        elif os.path.isfile(path):
            tc_instances = []
            result += cls._create_from_file_name(
                tc_instances, path, os.path.dirname(path), subcases_cmdline)
            for _tc in tc_instances:
                tcs[_tc.name] = _tc
        return result

    def _class_teardowns_run(self):
        """
        Runs all the class teardown methods for this testcase class
        """
        return self._methods_call("class_teardown")

tc_c.driver_add(tc_c)

class subtc_c(tc_c):
    """Helper for reporting sub testcases

    This is used to implement a pattern where a testcase reports, when
    executed, multiple subcases that are always executed. Then the
    output is parsed and reported as individual testcases.

    A simple way is using:

    >>> target.report_info("some message", subcase = "case1")
    >>> target.report_fail("error X", subcase = "case4")

    when reporting, NAME#case1 and NAME#case4 will be reported as subtestcases

    As well as the parameters in :class:`tcfl.tc.tc_c`, the
    following parameter is needed:

    :param tcfl.tc.tc_c parent: testcase which is the parent of this
      testcase.

    Refer to this :ref:`simplified example <example_subcases>` for a
    usage example.

    Note these subcases are just an artifact to report the subcases
    results individually, so they do not actually need to acquire or
    physically use the targets.

    """
    def __init__(self, name, tc_file_path, origin, parent):
        assert isinstance(name, str)
        assert isinstance(tc_file_path, str)
        assert isinstance(origin, str)
        assert isinstance(parent, tc_c)

        tc_c.__init__(self, name, tc_file_path, origin)
        self.parent = parent
        self.summary = None
        # we don't need to acquire our targets, won't use them
        self.target_group = parent.target_group
        self.do_acquire = False
        self.attachments = None
        self.mkticket()	# ensure data w/ target_group is updated!

    def update(self, result, summary, output = None, **attachments):
        """
        Update the results this subcase will report

        :param tcfl.tc.result_c result: result to be reported
        :param str summary: one liner summary of the execution report
        :param str output: (optional) string describing output from
          the testcase (general)
        :param attachments: any combination of `KEY=VALUE` which will
          be reported using the reporting API attachment system
        """
        assert isinstance(result, result_c)
        assert isinstance(summary, str)
        assert isinstance(output, str)
        self.result = result
        self.summary = summary
        self.attachments = dict(attachments)
        if output:
            self.attachments['output'] = output

    def eval_50(self):		# pylint: disable = missing-docstring
        if self.result == None:
            self.result = self.parent.result
            self.result.report(
                self, "subcase didn't run; parent didn't complete execution?",
                dlevel = 2, attachments = self.attachments)
        else:
            self.result.report(
                self, "subcase run summary: %s"
                % (self.summary if self.summary else "<not provided>"),
                dlevel = 2, attachments = self.attachments)
        # yeah f-strings and %() fields. bite me
        self.result.report(self, "NOTE: this is a subtestcase of %(tc_name)s "
                           f"(%(runid)s{report_runid_hashid_file_separator}%(tc_hash)s); refer to it for full "
                           "information" % self.parent.kws, dlevel = 1)
        return self.result

    @staticmethod
    def clean():		# pylint: disable = missing-docstring
        # Nothing to do, but do it anyway so the accounting doesn't
        # complain that nothing was found to run
        pass


#: Global testcase reporter
#:
#: Used to report top-level progress and messages beyond the actual
#: testcases we have to execute
tc_global = tc_c("toplevel", "", "builtin")
tc_global.skip_reports = True


def _testcase_match_tags(tc, tags_spec, origin = None):
    """
    :param str tags: string describing tag selection expression
    for :py:mod:`commonl.expr_parser`
    :param bool not_found_mistmatch: if a tag filter specifies a
      tag that is not found in the test case, treat it as a mismatch
      versus ignoring it.
    """
    if tags_spec == None:
        tc.report_info("selected by no-tag specification", dlevel = 4)
        return
    else:
        assert isinstance(tags_spec, str)
    if origin == None:
        origin = "[builtin]"
    kws = dict()
    for name, (value, _vorigin) in tc._tags.items():
        kws[name] = value

    if not commonl.conditional_eval("testcase tag match", kws, tags_spec,
                                    origin, kind = "specification"):
        raise skip_e("because of tag specification '%s' @ %s" %
                     (tags_spec, origin), dict(dlevel = 4))
    tc.report_info("selected by tag specification '%s' @ %s" %
                   (tags_spec, origin), dlevel = 4)


def testcases_discover(tcs_filtered, args):
    result = result_c(0, 0, 0, 0, 0)

    # discover test cases
    tcs_filtered.clear()
    if len(args.testcase) == 0 and len(args.manifest) == 0:
        logger.warning("No testcases specified, searching in "
                       "current directory, %s", os.getcwd())
        args.testcase = [ '.' ]
    tcs = {}
    tc_global.report_info("scanning for test cases", dlevel = 2)

    ignore_r = re.compile(r"^(\s*#.*|\s*)$")
    for manifest_file in args.manifest:
        try:
            with open(os.path.expanduser(manifest_file)) as manifest_fp:
                for tc_path_line in manifest_fp:
                    if not ignore_r.match(tc_path_line):
                        args.testcase.append(
                            os.path.expanduser(tc_path_line.strip()))
        except OSError:
            file_error = sys.exc_info()[1]
            logger.error("Error reading file: " + str(file_error))
            result.blocked += 1

    for tc_path in args.testcase:
        if ',' in tc_path:
            parts = tc_path.split(',')
            tc_path = parts[0]
            subcases_cmdline = parts[1:]
            logger.info("commandline '%s' requests subcases: %s",
                        tc_path, " ".join(subcases_cmdline))
        elif '#' in tc_path:
            parts = tc_path.split('#')
            tc_path = parts[0]
            subcases_cmdline = parts[1:]
            logger.info("commandline '%s' requests subcases: %s",
                        tc_path, " ".join(subcases_cmdline))
        else:
            subcases_cmdline = []
            logger.info("commandline '%s' requests no subcases", tc_path)
        if not os.path.exists(tc_path):
            logger.error("%s: does not exist; ignoring", tc_path)
            continue
        result += tc_c.find_in_path(tcs, tc_path, subcases_cmdline)
        for tcd in tc_c._tc_drivers:
            # If a driver has a different find function, use it to
            # find more
            tcd_find_in_path = getattr(tcd, "find_in_path", None)
            if tcd_find_in_path is not None and\
               id(getattr(tcd_find_in_path, "__func__", tcd_find_in_path)) \
               != id(tc_c.find_in_path.__func__):
                result += tcd.find_in_path(tcs, tc_path, subcases_cmdline)
    if len(tcs) == 0:
        logger.error("WARNING! No testcases found")
        return result

    # Now that we have them testcases, filter them based on the
    # tag filters specified in the command line with '-s'. Multiple's
    # are ORed together. Then for each testcase, apply the filter see
    # if it selects it or not.
    if args.tags_spec == []:
        tags_spec = None
    else:
        tags_spec = "(" + ") or (".join(args.tags_spec) +  ")"

    for tc_path, tc in tcs.items():
        try:
            # This is a TC unit testcase aid
            if args.testcase_name and tc.name != args.testcase_name:
                # Note this is only use for unit testing, so we don't
                # account it in the list of skipped TCs
                tc_global.report_info(
                    "ignoring because of testcase name '%s' not "
                    "matching args.testcase_name %s'"
                    % (tc.name, args.testcase_name),
                    dict(dlevel = 1))
                continue
            _testcase_match_tags(tc, tags_spec, "command line")
            # Anything with a skip tag shall be skipped
            skip_value, skip_origin = tc.tag_get('skip', False)
            if skip_value != False:
                if isinstance(skip_value, str):
                    raise skip_e("because of 'skip' tag @%s: %s"
                                 % (skip_origin, skip_value))
                else:
                    raise skip_e("because of 'skip' tag @%s" % skip_origin)
            # Fill in any arguments from the command line
            # We will consider this testcase
            tcs_filtered[tc_path] = tc
        except exception as e:
            tc.mkticket()
            with msgid_c() as _msgid:
                result += result_c.report_from_exception(tc, e)

    if not tcs_filtered:
        logger.error("All testcases skipped or filtered out by command "
                     "line -s options")
        return result
    logger.warning("Testcases filtered by command line to: %s",
                   ", ".join(list(tcs_filtered.keys())))
    return result

def _targets_discover_local(targets_all):
    """
    Discover the 'local' target, used for running static tests
    """
    bsp = platform.machine()
    targets_all['local'] = {
        # FIXME: obtain from local system
        'bsp_models': { bsp: None },
        'bsps': {
            bsp: {
            },
        },
        'type': platform.system(),
        'id': 'local',
        'fullid': 'local',
        'url' : 'file://' + commonl.origin_get_object_path(_targets_discover_local)
    }


def _targets_discover(args, rt_all, rt_selected, ic_selected):
    """
    Do initial discovery of targets based on the command line options
    """
    rt_selected.clear()
    ic_selected.clear()
    rt_all.clear()
    _targets_discover_local(rt_all)

    rt_selected_all = {}
    ic_selected_all = {}
    # List all the targets available
    import tcfl.targets
    tcfl.targets.setup_by_spec(
        args.target, verbosity = args.verbosity - args.quietosity,
        targets_all = args.all)

    # HACK to remove old tcfl.ttb_client and plug tcfl.targets; all
    # this is going to be abandoned with the improved orchestrator
    rt_all_list = tcfl.targets.discovery_agent.rts.values()
    if not rt_all_list:
        logger.error("WARNING! No targets available")
    for rt in rt_all_list:
        rt_all[rt['fullid']] = rt
        if rt.get('disabled', None) != None \
           and tc_c._targets_disabled_too == False:
            continue
        if 'interconnect_c' in rt.get('interfaces', {}):
            ic_selected_all[rt['fullid']] = set(rt.get('bsp_models', {}).keys())
        else:
            rt_selected_all[rt['fullid']] = set(rt.get('bsp_models', {}).keys())
    del rt_all_list

    tc_global.report_info(
        "targets available: %s"
        % tc_global._selected_str(rt_selected_all), dlevel = 4)
    tc_global.report_info(
        "interconnects available: %s"
        % tc_global._selected_str(ic_selected_all), dlevel = 4)

    # Now filter based on the -t specs given in the command line
    # later each test case might filter more--note we support two
    # modes for the time being if -I was given (othrwise, we go to old
    # methodology)
    if args.target:

        if args.ic_spec:
            raise RuntimeError("ERROR! can't specify -t and -I")
        if args.rt_spec:
            raise RuntimeError("ERROR! can't specify -t and -T")

        if args.target:
            rt_spec = "(" + ") or (".join(args.target) +  ")"
            ic_spec = rt_spec
        else:
            rt_spec = None
            ic_spec = None

    else:

        if args.ic_spec:
            ic_spec = "(" + ") or (".join(args.ic_spec) +  ")"
        else:
            ic_spec = None

        if args.rt_spec:
            rt_spec = "(" + ") or (".join(args.rt_spec) +  ")"
        else:
            rt_spec = None

    if ic_spec != None:
        # Select interconnects
        for rt_fullid, bsp_models in ic_selected_all.items():
            rt = rt_all[rt_fullid]
            # Introduce two symbols after the ID and fullid, so "-t
            # TARGETNAME" works
            rt[rt_fullid] = True
            rt[rt['id']] = True
            for bsp_model in bsp_models:
                if tc_global._targets_select_by_spec(
                        "command line", rt, bsp_model, ic_spec, "command line"):
                    ic_selected.setdefault(rt_fullid, set()).add(bsp_model)
                    break
            else:
                if tc_global._targets_select_by_spec(
                        "command line", rt, None, ic_spec, "command line"):
                    ic_selected.setdefault(rt_fullid, set())
    else:
        ic_selected.update(ic_selected_all)

    # Now filter based on the -t specs given in the command line
    # later each test case might filter more
    if rt_spec != None:

        # Select targets
        for rt_fullid, bsp_models in rt_selected_all.items():
            rt = rt_all[rt_fullid]
            # Introduce two symbols after the ID and fullid, so "-t
            # TARGETNAME" works
            rt[rt_fullid] = True
            rt[rt['id']] = True
            for bsp_model in bsp_models:
                if tc_global._targets_select_by_spec(
                        "command line", rt, bsp_model, rt_spec, "command line"):
                    rt_selected.setdefault(rt_fullid, set()).add(bsp_model)
            if not bsp_models:
                if tc_global._targets_select_by_spec(
                        "command line", rt, None, rt_spec, "command line"):
                    rt_selected.setdefault(rt_fullid, set())
    else:
        rt_selected.update(rt_selected_all)


    tc_global.report_info(
        "targets selected by command line: %s"
        % tc_c._selected_str(rt_selected), dlevel = 4)

    tc_global.report_info(
        "interconnects selected by command line: %s"
        % tc_c._selected_str(ic_selected), dlevel = 4)


# This need to be imported here since they rely on definitions
from . import report_console
from . import report_jinja2
from . import report_taps
from . import report_data_json

# list of allocation IDs we have currently reserved; this list is
# local to each thread so when we interrupt it with a signal, we can
# ask the server to drop those reservations
_allocids = {}
_allocids_mutex = threading.Lock()

def _allocid_rtb_keepalive(rtb, allocids):
    # send a keepalive to a single server for a list of ALLOCIds
    logging.info(f"keepalive: {rtb.aka}: allocids {' '.join(allocids)}")
    data = {}
    for allocid in allocids:
        data[allocid] = "active"
    try:
        _r = rtb.send_request("PUT", "keepalive", json = data)
    except requests.HTTPError as e:
        logging.error(f"keepalive: {rtb.aka}: error (ignoring): {e}")


def _allocids_keepalive_once():
    # send keepalives for all current ALLOCIds
    # paralellizes per server
    #
    # This is used by _targets_assign to start a keepalive thread that
    # keeps the allocationa live. It's a hack.
    with _allocids_mutex:	# make a local copy of the current list
        allocids_local = dict(_allocids)

    with concurrent.futures.ThreadPoolExecutor(len(allocids_local)) as executor:
        _rs = executor.map(lambda i: _allocid_rtb_keepalive(i[0], i[1]),
                          allocids_local.items())
        # with waits for all the executors to finish


def _run(args):
    """
    Runs one ore more test cases
    """

    # FIXME: move this to a more formal orchestrator specific config
    import tcfl.tc_clear_bbt
    tcfl.tc.tc_c.driver_add(tcfl.tc_clear_bbt.tc_clear_bbt_c)

    import tcfl.tc_jtreg
    tcfl.tc.tc_c.driver_add(tcfl.tc_jtreg.driver)

    # Zephyr-specific drivers
    import tcfl.app_zephyr
    import tcfl.tc_zephyr_sanity
    tcfl.app.driver_add(tcfl.app_zephyr.app_zephyr)
    tcfl.tc.target_c.extension_register(tcfl.app_zephyr.zephyr)
    tcfl.tc.tc_c.driver_add(tcfl.tc_zephyr_sanity.tc_zephyr_sanity_c)
    tcfl.tc.tc_c.dir_ignore_add_regex("^doc$")
    tcfl.tc.tc_c.dir_ignore_add_regex("^outdir.*$")


    _globals_init()
    if args.shard:
        try:
            if '/' in args.shard:
                _shard, _shards = args.shard.split("/", 1)
            elif ':' in args.shard:
                _shard, _shards = args.shard.split(":", 1)
            elif '-' in args.shard:
                _shard, _shards = args.shard.split("-", 1)
            else:
                raise ValueError
            shard = int(_shard)
            shards = int(_shards)
            if shard < 1 or shard > shards:
                raise ValueError
        except:
            raise ValueError("%s: bad shard spec; expecting N/M, N:M or N-M "
                             "with 1 <= N <= M" % args.shard)
        shards_log = "-" + args.shard.replace("/", "-")
    else:
        shards_log = ""
        shards = 0

    tc_c._hash_salt = args.hash_salt

    # Is there a Run ID specified? or we asked to generate one? or none?
    global runid
    if args.id != None:
        if args.id == "":
            tc_c.runid_visible = ""
            tc_c.runid = msgid_c.generate()
        else:
            tc_c.runid_visible = args.id
            tc_c.runid = args.id
        if args.log_file == None:
            args.log_file = tc_c.runid + shards_log + ".log"
    else:
        tc_c.runid_visible = ""
    # tc_global, when reporting, doesn't print the HASHID
    tc_global.runid_hashid = tc_c.runid_visible

    for id_extra in args.id_extra:
        if '=' in id_extra:
            key, value = id_extra.split('=', 1)
            value = commonl.cmdline_str_to_value(value)
        else:
            key = id_extra
            value = True
        tc_c.runid_extra[key] = value

    tc_c.max_permutations = args.max_permutations
    tc_c.max_runs_per_tc = args.max_runs_per_tc

    # Establish what is our log directory
    global log_dir
    if args.log_dir != None:
        if args.log_dir == "":
            if tc_c.runid == None:
                logger.error("No log directory or RunID specified "
                             "(use -i or --log-dir)")
                return 1
            log_dir = os.path.abspath(tc_c.runid)
        else:
            log_dir = os.path.abspath(args.log_dir)
        try:
            os.makedirs(log_dir)
        except OSError:
            # Lame check, as long as we have a dir, I don't care much
            if not os.path.isdir(log_dir):
                raise
    else:
        log_dir = os.getcwd()


    # Create a tempdirectory for testcases to go wild
    # Why don't we change current directory to tc_c.tmpdir?
    #
    # - each testcase runs in a separate thread, and the change
    #   directory is per-process, so we can't have tstcases magically
    #   writing to CWD and not collide with each other
    #
    # - we want trash being written in a noticeable place, so it is
    #   noticed and fixed.
    #
    if args.tmpdir:
        commonl.check_dir_writeable(args.tmpdir,
                                    "testcases' run temporary directory")
        # We don't check if the tempdir is empty -- we might want to
        # reuse build stuff from a prev build and such -- it's up to
        # the user to use the right tempdir
        tc_c.tmpdir = os.path.abspath(args.tmpdir)
    else:
        # use defaul tc_c.tmpdir set in class tc_c
        if args.remove_tmpdir:
            atexit.register(shutil.rmtree, tc_c.tmpdir, True)
        else:
            atexit.register(
                sys.stderr.write,
                "I: %s: not removing temporary directory\n" % tc_c.tmpdir)

    # Now settle if we need a log file where to place it
    if args.log_file != None:
        # If no dir path, add one and anyway make it absolute
        if os.path.dirname(args.log_file) == '':
            log_file = os.path.join(log_dir, args.log_file)
        else:
            log_file = args.log_file
        log_file = os.path.abspath(log_file)
    else:
        log_file = None

    # if there is no log file given by the user *but* it did --tmpdir
    # or --no-remove-tmpdir, well, we are doing one by default
    if log_file == None and (args.tmpdir or not args.remove_tmpdir):
        log_file = os.path.join(tc_c.tmpdir, "run.log")

    # Init report drivers FIXME: need a hook to add more
    global report_console_impl
    global report_file_impl
    if log_file:
        if os.path.dirname(log_file) == '':
            log_file = os.path.join(log_dir, log_file)
        else:
            log_file = os.path.abspath(log_file)
    try:
        # do we need to add a *console* driver? if a config file has added
        # one, then do not try to add one. Note _globals_init() might
        # read it if it is not None too
        report_console_impl = report_driver_c.get_by_name("console")
    except ValueError:
        if args.taps:
            report_console_impl = report_taps.driver()
        else:
            report_console_impl = report_console.driver(
                args.verbosity - args.quietosity,
                log_file, verbosity_logf = args.log_file_verbosity)
        report_driver_c.add(report_console_impl, name = "console")

    report_file_impl = report_jinja2.driver(log_dir)
    report_driver_c.add(report_file_impl)
    report_driver_c.add(report_data_json.driver())

    # Setup defaults in the base testcase class
    tc_c.preempt = args.preempt
    tc_c.priority = args.priority
    tc_c.reason = args.reason
    tc_c.obo = args.obo

    if args.allocid:			# Meaning I have them allocated ...
        tc_c.allocid = args.allocid	# ... use this allocid
    tc_c.release = args.release
    tc_c._dry_run = args.dry_run
    tc_c.eval_repeat = args.repeat_evaluation
    tc_c._targets_disabled_too = args.all
    tc_c._mode = args.mode
    if args.extra_report != None:
        tc_c._extra_report_format = args.extra_report

    # Which phases are we running?
    # Note these are class defaults--but there are test cases
    # or targets that might declare themselves build only (and thus
    # cancel deploy and eval, for example). So on __init__ we'll just
    # clone these two arrays, wasteful in memory as it is.
    tc_c._phases['build'] = set(['builtin-config'])
    tc_c._phases['deploy'] = set(['builtin-config'])
    tc_c._phases['eval'] = set(['builtin-config'])
    tc_c._phases['configure'] = set(['builtin-config'])
    tc_c._phases_skip['clean'] = set(['builtin-config'])
    if 'configure' in args.phases:
        tc_c._phases["configure"].add('command line')
        tc_c._phases_skip.pop('configure', None)
    if 'build' in args.phases:
        tc_c._phases['build'].add('command line')
    if 'deploy' in args.phases:
        tc_c._phases['build'].add('command line')
        tc_c._phases['deploy'].add('command line')
    if 'eval' in args.phases:
        tc_c._phases['build'].add('command line')
        tc_c._phases['deploy'].add('command line')
        tc_c._phases['eval'].add('command line')
    if 'clean' in args.phases:
        tc_c._phases['clean'].add('command line')
        tc_c._phases_skip.pop('clean', None)

    if 'configure' in args.phases_skip:
        tc_c._phases.pop('configure', None)
        tc_c._phases_skip["configure"].add('command line')
    if 'build' in args.phases_skip:
        tc_c._phases.pop('build', None)
        tc_c._phases_skip["build"].add('command line')
    if 'deploy' in args.phases_skip:
        tc_c._phases.pop('deploy', None)
        tc_c._phases_skip["deploy"].add('command line')
    if 'eval' in args.phases_skip:
        tc_c._phases.pop('eval', None)
        tc_c._phases_skip["eval"].add('command line')
    if 'clean' in args.phases_skip:
        tc_c._phases.pop('clean', None)
        tc_c._phases_skip["clean"].add('command line')

    with msgid_c():
        tc_global.report_info("version %s" % version, dlevel = 3)

        tcs_filtered = {}
        result = testcases_discover(tcs_filtered, args)
        if shards:
            shard -= 1 # everything is easier zero based
            tcs_sorted = sorted(tcs_filtered.keys())
            tcs_shard = set(tcs_sorted[shard::shards])
            # Yes, there is a more efficient way to do this, but this
            # is cleaner
            tc_global.report_info(
                "shard %s: selecting %s TCs of %d available" % (
                    args.shard, len(tcs_shard), len(tcs_sorted)),
                dlevel = 2)
            for tc in set(tcs_sorted) - tcs_shard:
                del tcs_filtered[tc]

        rt_all = {}
        rt_selected = {}
        ic_selected = {}
        _targets_discover(args, rt_all, rt_selected, ic_selected)
        if not rt_selected:
            logger.error("WARNING! No targets selected")

        tc_c._tcs_total = len(tcs_filtered)
        threads_no = int(args.threads)
        tp = None

        def _sigint_handler(signum, frame):
            # FIXME: move to a sigaction and don't exit, in case we are called from a
            # library
            # FIXME: move one level up
            logging.error("brute force termination; cleaning up..couple secs")
            if tc_c.release:
                # if we are to release targets, do so on cancellation;
                # if the user has given --no-release, then we leave
                # them in whatever state they are, since we might have
                # an user just wanting to cancel to take manual control
                with _allocids_mutex:
                    allocids_local = dict(_allocids)
                for rtb in list(allocids_local.keys()):
                    # FIXME: this has to be done so we can submit a list
                    #        of allocids to remove IN parallel
                    for allocid in allocids_local[rtb]:
                        logging.error("removing allocation %s on %s",
                                      allocid, rtb)
                        target_ext_alloc._delete(rtb, allocid)
                    with _allocids_mutex:
                        # another thread might have cleared it, so
                        if rtb in _allocids:
                            _allocids[rtb].clear()
            if tp:
                tp.terminate()
            time.sleep(1)
            os.kill(0, 9)
            if args.remove_tmpdir:
                shutil.rmtree(tc_c.tmpdir, True)
            sys.exit(1)

        signal.signal(signal.SIGINT, _sigint_handler)
        if sys.platform != "win32":
            signal.signal(signal.SIGQUIT, _sigint_handler)

        # we do oly two executions per process--it doesn't matter in
        # terms of the cost of bringing up the new process, since the
        # interaction with the target takes way longer; but we want
        # the resources to be cleaned up, otherwise memory consumption
        # / leaks which we can't control accumulate.
        tp = _multiprocessing_tc_pool_c(processes = threads_no)

        # So now run as many testcases as possible
        threads = []
        time_start = time.time()
        tc_c.jobs = len(tcs_filtered)
        for tc in list(tcs_filtered.values()):
            tc.mkticket()
            tc.report_info("queuing for pairing", dlevel = 3)
            with msgid_c() as _msgid:
                _threads = tc._run_on_targets(tp, rt_all,
                                              rt_selected, ic_selected)
                del tc	# we don't need it anymore
            if isinstance(_threads, result_c):
                result += _threads
            elif _threads == []:
                # No targets could be found, SKIPPED
                result += result_c(0, 0, 0, 0, 1)
            else:
                threads += _threads
        tp.close()
        # FIXME: maybe we can use tp.imap_iter stuff
        tp.join()
        for thread in threads:
            for cls, retval in thread.get():
                # Note we do this here vs. in tc_c._run() because that
                # will be running in a different process.
                cls.class_result += retval
                result += retval
        time_end = time.time()
        # Run the class' teardown processes, if any -- mostly used for
        # self-testing. Note this is run in the top level process.
        seen_classes = set()
        for tc in list(tcs_filtered.values()):
            cls = type(tc)
            if cls in seen_classes:
                continue
            else:
                seen_classes.add(cls)
                with msgid_c(depth = 0,
                             phase = "class_teardown") as _msgid:
                    result += tc._class_teardowns_run()
        # If something failed or blocked, report totals verbosely
        tc_global.report_tweet(
            "%d tests (%d passed, %d error, %d failed, "
            "%d blocked, %d skipped, in %s) -"
            % (result.total(), result.passed, result.errors, result.failed,
               result.blocked, result.skipped,
               (datetime.timedelta(0, time_end - time_start))),
            result)
        tc_global.finalize(result)

        if result.errors or result.failed:
            return 1
        elif result.blocked > 0:
            return 127
        return 0

def argp_setup(arg_subparsers):

    ap = arg_subparsers.add_parser("run",
                                   help = "Run testcases")
    ap.add_argument(
        "-v", dest = "verbosity", action = "count", default = 0,
        help = "Increase information to display about the tests")
    ap.add_argument(
        "-q", dest = "quietosity", action = "count", default = 0,
        help = "Decrease information to display about the tests")
    ap.add_argument(
        "--no-remove-tmpdir", dest = "remove_tmpdir",
        action = "store_false", default = True,
        help = "Do not remove temporary directory upon exit")
    ap.add_argument(
        "--tmpdir", dest = "tmpdir", action = "store", type = str,
        default = None,
        help = "Directory where to place temporary files "
        "(they will not be removed upon exit; defaults to be "
        "autogenerated and removed upon exit")
    ap.add_argument(
        "--single-thread", action = "store_const", const = "1",
        dest = "threads",
        help = "Run things in a single thread of execution")
    ap.add_argument(
        "--threads", action = "store", type = int, dest = "threads",
        help = "Use this many threads of execution [%(default)s] for "
        "concurrent test case execution--note this can be way more than "
        "the number of CPUs in the system as it might be waiting for a "
        "targets to be available and that should not block buildin other "
        "test cases")
    ap.add_argument(
        "--make-j", action = "store", type = int,
        help = "Use this many threads of execution [%(default)s] "
        "for make (-jN)")
    ap.add_argument(
        "--log-file", action = "store", default = None,
        help = "Log full output to file")
    ap.add_argument(
        "--log-file-verbosity", action = "store", type = int, default = 999,
        metavar = "LOGLEVEL",
        help = "Unsigned integer describing the maximum verbosity of log "
        "file messages (0 is less, greater is more, defaults to 999, all)")
    ap.add_argument(
        "--log-dir", metavar = "DIRECTORY", action = "store", nargs = "?",
        type = str, const = "", default = None,
        help = "Write all log files and reports to said directory"
        "(defaults to autogenerated RunID)")
    ap.add_argument(
        "-u", action = "store_const", const = "all", dest = "mode",
        help = "Do an unlimited test run (on all targets of each type)")
    ap.add_argument(
        "-U", action = "store_const", const = "one-per-type", dest = "mode",
        help = "Do a limited test run (only one target of each type "
        "[default])")
    ap.add_argument(
        "-y", action = "store_const", const = "any", dest = "mode",
        help = "Do a limited test run on any target of any type")
    ap.add_argument(
        "-n", "--dry-run", action = "store_true", default = False,
        help = "Describe only what'd be run and where")
    ap.add_argument(
        "-i", "--id", action = "store", nargs = "?", metavar = "ID",
        type = str,
        const = "",	# Means auto-generate one
        default = None,
        help = "Store a run ID, accesible to the reporting engine (this "
        "also will create a log file ID.log)")
    ap.add_argument(
        "--id-extra", action = "append", nargs = "?", metavar = "KEY=VALUE",
        type = str,
        default = [],
        help = "Store extra ID specific variables; these are usually that"
        " that are associated to a full blown execution, like a URL,"
        " information about a build, etc")
    ap.add_argument("-t", "--target", metavar = "SPEC",
                    action = "append", default = [],
                    help =
                    "Filter which targets and interconncets can be used. "
                    "This is a boolean expression where any inventory"
                    " value can be tested for with"
                    " and, or, not, ==, !=, <, >, >=, <=, in , : (regex) and"
                    " parenthesis to group tests."
                    " Multiple -t SPEC specifications are ORed together"
                    " (can't be used with -T or -I; use those to filter"
                    " separatedly)"
                    )
    ap.add_argument("-T", metavar = "SPEC",
                    action = "append", dest = "rt_spec", default = [],
                    help = "Filter which targets can be used. "
                    "This is a boolean expression where any inventory"
                    " value can be tested for with"
                    " and, or, not, ==, !=, <, >, >=, <=, in , : (regex) and"
                    " parenthesis to group tests."
                    " Multiple -T SPEC specifications are ORed together"
                    " (to be used with -I, can't be used with -t)"
                    )
    ap.add_argument("-I", "--interconnect", metavar = "TARGETSPECs",
                    action = "append", dest = "ic_spec", default = [],
                    help = "Filter which interconnects can be used"
                    " Multiple -T SPEC specifications are ORed together"
                    " (to be used with -I, can't be used with -t)"
                    )
    ap.add_argument(
        "--configure", "-c", action = "append_const",
        dest = 'phases', const = "configure",
        help = "Tell the testcases to run configure phase "
        " (off by default)")
    ap.add_argument(
        "--build", "-b", action = "append_const",
        dest = 'phases', const = "build",
        help = "Tell the testcases to run the build phase"
        " (default)")
    ap.add_argument(
        "--deploy", "-d", action = "append_const",
        dest = 'phases', const = "deploy",
        help = "Tell the testcases to run until the deploy phase"
        " (default)")
    ap.add_argument(
        "--eval", "-e", action = "append_const",
        dest = 'phases', const = "eval",
        help = "Tell the testcases to run until the eval phase"
        " (default)")
    ap.add_argument(
        "--skip-configure", "-C", action = "append_const",
        dest = 'phases_skip', const = "configure",
        help = "Tell the testcases to skip the configure phase")
    ap.add_argument(
        "--skip-build", "-B", action = "append_const",
        dest = 'phases_skip', const = "build",
        help = "Tell the testcases to skip the build phase")
    ap.add_argument(
        "--skip-deploy", "-D", action = "append_const",
        dest = 'phases_skip', const = "deploy",
        help = "Tell the testcases to skip the deploy phase")
    ap.add_argument(
        "--skip-eval", "-E", action = "append_const",
        dest = 'phases_skip', const = "eval",
        help = "Tell the testcases to skip the eval phase")
    ap.add_argument(
        "--clean", "-l", action = "append_const",
        dest = 'phases', const = "clean",
        help = "Tell the testcases to clean after themselves"
        " (off by default)")
    ap.add_argument(
        "-s", action = "append", dest = 'tags_spec', default = [],
        help = "Specify testcase filters, based on the testcase tags; "
        "multiple tags specifications are ORed together")
    ap.add_argument(
        "-r", "--repeat-evaluation",
        action = "store", type = int, default = 1,
        help = "How many times to repeat the evaluation phase "
        " (%(default)d by default); this is used for stress or "
        "MTBF testing without having to re-deploy the target")
    ap.add_argument(
        "--no-release", dest = "release", action = "store_false",
        default = True,
        help = "(internal) do not release targets once complete")
    ap.add_argument(
        "-P", "--max-permutations",
        action = "store", type = int, default = 10,
        help = "Maximum number of permutuations of targets for a "
        "single test that shall be considered")
    ap.add_argument(
        "--max-runs-per-tc",
        action = "store", type = int, default = 0,
        help = "Maximum number of runs for each testcase")
    ap.add_argument(
        "--extra-report", action = "store", default = None,
        help = "Specify extra format information that is to be printed for "
        "failed testcases; fields are the same as for the rules "
        "%%(FIELDNAME)s (or d, or whichever type the field is)")
    ap.add_argument(
        "-a", "--all", action = "store_true", default = False,
        help = "Consider also disabled targets")
    ap.add_argument(
        "--reason", action = "store",
        default = "%(runid)s %(tc_name)s @ %(host_name)s:%(pid)s/%(tid)s",
        help = "Reason template. This is a string that be sent to"
        " the server when allocating the targets to display as a"
        " reason for allocation so other users can see what the"
        " target is being used for; any keyword in the testcase"
        " can be put in here; defaults to '%(default)s'"
    )
    ap.add_argument("-m", "--manifest", metavar = "MANIFESTFILEs",
                    action = "append", default = [],
                    help = "Specify one or more manifest files containing "
                    "test case paths (one per line) or '#' prefixed comment "
                    "lines. Test case paths are acumulated, keeping other "
                    "specified paths")
    ap.add_argument(
        "--shard", metavar = "N/M", action = "store", type = str,
        default = None,
        help = "Divide the sorted list of discovered testcases in "
        "M pieces and consider on the Nth")
    ap.add_argument(
        "--hash-salt", metavar = "SALT", action = "store", type = str,
        default = "",
        help = "Add SALT to the string used to compute the "
        "testcase's hash ID; useful when multiple runs of the same "
        "cases and targets are going to be run with different external "
        "variations, such as different compiler or library versions")
    ap.add_argument(
        "-p", "--priority", action = "store", type = int, default = 500,
        help = "Priority (0 highest, 999 lowest, %(default)s);"
        " policy might restrict the highest priority")
    ap.add_argument(
        "-o", "--obo", action = "store", default = None,
        help = "User to allocate on behalf of; policy might disallow")
    ap.add_argument(
        "--preempt", action = "store_true", default = False,
        help = "Enable allocation preemption (disabled by default);"
        " server policy might restrict if allowed or not.")
    ap.add_argument(
        "--taps", action = "store_true", default = False,
        help = "Report in TAPS format")
    ap.add_argument("testcase", metavar = "TEST-CASE",
                    nargs = "*",
                    help = "Files describing testcases")
    ap.set_defaults(func = _run,
                    testcase_name = None,
                    phases_skip = [], phases = [], tc_filter_specs = [],
                    threads = 4 * _multiprocessing.cpu_count(),
                    make_j = 2 * _multiprocessing.cpu_count(),
                    mode = 'one-per-type')

# Get drivers registered

# FIXME: move to config if sketch package is installed
from . import app_sketch
app.driver_add(app_sketch.app_sketch)

from . import app_manual
app.driver_add(app_manual.app_manual)

# Target API extensions
from . import target_ext_alloc

from . import target_ext_buttons
target_c.extension_register(target_ext_buttons.extension, "button")

from . import target_ext_capture
target_c.extension_register(target_ext_capture.extension, "capture")

from . import target_ext_certs
target_c.extension_register(target_ext_certs.extension, "certs")

from . import target_ext_console
target_c.extension_register(target_ext_console.extension, "console")

from . import target_ext_debug
target_c.extension_register(target_ext_debug.extension, "debug")

from . import target_ext_fastboot
target_c.extension_register(target_ext_fastboot.extension, "fastboot")

from . import target_ext_images
target_c.extension_register(target_ext_images.extension, "images")

from . import target_ext_input
target_c.extension_register(target_ext_input.extension, "input")

from . import target_ext_shell
target_c.extension_register(target_ext_shell.shell)

from . import target_ext_ssh
target_c.extension_register(target_ext_ssh.ssh)

from . import target_ext_store
target_c.extension_register(target_ext_store.extension, "store")

from . import target_ext_power
target_c.extension_register(target_ext_power.extension, "power")

from . import target_ext_things
target_c.extension_register(target_ext_things.extension, "things")

from . import target_ext_tunnel
target_c.extension_register(target_ext_tunnel.tunnel)

from . import pos
target_c.extension_register(pos.extension, "pos")
